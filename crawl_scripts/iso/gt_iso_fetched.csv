,SoW,title,Standards,introduction,scope,id
0,"The premise of the NGCI Apex program is to discover, develop, and transition new prototype technologies, designed to protect the overall critical infrastructure of the financial sector. _New technologies will be deployed to identify, protect, detect, respond, and recover, from a wide range of evolving cyber threats across five identified asset classes: devices, applications, networks, data, and people. _These technologies will be monitored to ensure they remain effective and relevant in the face of evolving threats. _The CART has developed a matrixed approach to help identify the areas of focus for the NGCI Apex program (see Figure 8 below).",Information technology — Security techniques — Information security risk management,https://www.iso.org/obp/ui/#!iso:std:56742:en,"IntroductionThis International Standard provides guidelines for information security risk management in an organization, supporting in particular the requirements of an information security management (ISMS) according to ISO/IEC 27001. However, this International Standard does not provide any specific method for information security risk management. It is up to the organization to define their approach to risk management, depending for example on the scope of the ISMS, context of risk management, or industry sector. A number of existing methodologies can be used under the framework described in this International Standard to implement the requirements of an ISMS.This International Standard is relevant to managers and staff concerned with information security risk management within an organization and, where appropriate, external parties supporting such activities.","1   ScopeThis International Standard provides guidelines for information security risk management.This International Standard supports the general concepts specified in ISO/IEC 27001 and is designed to assist the satisfactory implementation of information security based on a risk management approach.Knowledge of the concepts, models, processes and terminologies described in ISO/IEC 27001 and ISO/IEC 27002 is important for a complete understanding of this International Standard.This International Standard is applicable to all types of organizations (e.g. commercial enterprises, government agencies, non-profit organizations) which intend to manage risks that could compromise the organization’s information security.",ISO/IEC 27005:2011(en)
1,"The premise of the NGCI Apex program is to discover, develop, and transition new prototype technologies, designed to protect the overall critical infrastructure of the financial sector. _New technologies will be deployed to identify, protect, detect, respond, and recover, from a wide range of evolving cyber threats across five identified asset classes: devices, applications, networks, data, and people. _These technologies will be monitored to ensure they remain effective and relevant in the face of evolving threats. _The CART has developed a matrixed approach to help identify the areas of focus for the NGCI Apex program (see Figure 8 below).",Information technology — Service management — Part 1: Service management system requirements,https://www.iso.org/obp/ui/#!iso:std:51986:en,"IntroductionThe requirements in this part of ISO/IEC 20000 include the design, transition, delivery and improvement of services that fulfil service requirements and provide value for both the customer and the service provider. This part of ISO/IEC 20000 requires an integrated process approach when the service provider plans, establishes, implements, operates, monitors, reviews, maintains and improves a service management system (SMS).Co-ordinated integration and implementation of an SMS provides ongoing control and opportunities for continual improvement, greater effectiveness and efficiency. The operation of processes as specified in this part of ISO/IEC 20000 requires personnel to be well organized and co-ordinated. Appropriate tools can be used to enable the processes to be effective and efficient.The most effective service providers consider the impact on the SMS through all stages of the service lifecycle, from strategy through design, transition and operation, including continual improvement.This part of ISO/IEC 20000 requires the application of the methodology known as “Plan-Do-Check-Act” (PDCA) to all parts of the SMS and the services. The PDCA methodology, as applied in this part of ISO/IEC 20000, can be briefly described as follows.Plan: establishing, documenting and agreeing the SMS. The SMS includes the policies, objectives, plans and processes to fulfil the service requirements.Do: implementing and operating the SMS for the design, transition, delivery and improvement of the services.Check: monitoring, measuring and reviewing the SMS and the services against the policies, objectives, plans and service requirements and reporting the results.Act: taking actions to continually improve performance of the SMS and the services.When used within an SMS, the following are the most important aspects of an integrated process approach and the PDCA methodology:a) understanding and fulfilling the service requirements to achieve customer satisfaction;b) establishing the policy and objectives for service management;c) designing and delivering services based on the SMS that add value for the customer;d) monitoring, measuring and reviewing performance of the SMS and the services;e) continually improving the SMS and the services based on objective measurements.Figure 1 illustrates how the PDCA methodology can be applied to the SMS, including the service management processes specified in Clauses 5 to 9, and the services. Each element of the PDCA methodology is a vital part of a successful implementation of an SMS. The improvement process used in this part of ISO/IEC 20000 is based on the PDCA methodology.Figure 1
—
PDCA methodology applied to service managementThis part of ISO/IEC 20000 enables a service provider to integrate its SMS with other management systems in the service provider's organization. The adoption of an integrated process approach and the PDCA methodology enables the service provider to align or fully integrate multiple management system standards. For example, an SMS can be integrated with a quality management system based on ISO 9001 or an information security management system based on ISO/IEC 27001.ISO/IEC 20000 is intentionally independent of specific guidance. The service provider can use a combination of generally accepted guidance and its own experience.Users of an International Standard are responsible for its correct application. An International Standard does not purport to include all necessary statutory and regulatory requirements and contractual obligations of the service provider. Conformity to an International Standard does not of itself confer immunity from statutory and regulatory requirements.For the purposes of research on service management standards, users are encouraged to share their views on ISO/IEC 20000-1 and their priorities for changes to the rest of the ISO/IEC 20000 series. Click on the link below to take part in the online survey.ISO/IEC 20000-1 online survey","1   Scope1.1   GeneralThis part of ISO/IEC 20000 is a service management system (SMS) standard. It specifies requirements for the service provider to plan, establish, implement, operate, monitor, review, maintain and improve an SMS. The requirements include the design, transition, delivery and improvement of services to fulfil service requirements. This part of ISO/IEC 20000 can be used by:a) an organization seeking services from service providers and requiring assurance that their service requirements will be fulfilled;b) an organization that requires a consistent approach by all its service providers, including those in a supply chain;c) a service provider that intends to demonstrate its capability for the design, transition, delivery and improvement of services that fulfil service requirements;d) a service provider to monitor, measure and review its service management processes and services;e) a service provider to improve the design, transition and delivery of services through effective implementation and operation of an SMS;f) an assessor or auditor as the criteria for a conformity assessment of a service provider's SMS to the requirements in this part of ISO/IEC 20000.Figure 2 illustrates an SMS, including the service management processes. The service management processes and the relationships between the processes can be implemented in different ways by different service providers. The nature of the relationship between a service provider and the customer will influence how the service management processes are implemented.Figure 2
—
Service management system1.2   ApplicationAll requirements in this part of ISO/IEC 20000 are generic and are intended to be applicable to all service providers, regardless of type, size and the nature of the services delivered. Exclusion of any of the requirements in Clauses 4 to 9 is not acceptable when a service provider claims conformity to this part of ISO/IEC 20000, irrespective of the nature of the service provider's organization.Conformity to the requirements in Clause 4 can only be demonstrated by a service provider showing evidence of fulfilling all of the requirements in Clause 4. A service provider cannot rely on evidence of the governance of processes operated by other parties for the requirements in Clause 4.Conformity to the requirements in Clauses 5 to 9 can be demonstrated by the service provider showing evidence of fulfilling all requirements. Alternatively, the service provider can show evidence of fulfilling the majority of the requirements themselves and evidence of the governance of processes operated by other parties for those processes, or parts of processes, that the service provider does not operate directly.The scope of this part of ISO/IEC 20000 excludes the specification for a product or tool. However, organizations can use this part of ISO/IEC 20000 to help them develop products or tools that support the operation of an SMS.NOTE ISO/IEC TR 20000-3 provides guidance on scope definition and applicability of this part of ISO/IEC 20000. This includes further explanation about the governance of processes operated by other parties.",ISO/IEC 20000-1:2011(en)
2,"The premise of the NGCI Apex program is to discover, develop, and transition new prototype technologies, designed to protect the overall critical infrastructure of the financial sector. _New technologies will be deployed to identify, protect, detect, respond, and recover, from a wide range of evolving cyber threats across five identified asset classes: devices, applications, networks, data, and people. _These technologies will be monitored to ensure they remain effective and relevant in the face of evolving threats. _The CART has developed a matrixed approach to help identify the areas of focus for the NGCI Apex program (see Figure 8 below).",Information technology — Security techniques — Information security controls for the energy utility industry,https://www.iso.org/obp/ui/#!iso:std:68091:en,"0	Introduction0.1	Background and contextThis document provides guiding principles based on ISO/IEC 27002:2013 “Code of practice for information security controls” for information security management applied to process control systems as used in the energy utility industry. The aim of this document is to extend the contents of ISO/IEC 27002:2013 to the domain of process control systems and automation technology, thus allowing the energy utility industry to implement a standardized and specific information security management system (ISMS) that is in accordance with ISO/IEC 27001:2013 and extends from the business to the process control level.In addition to the security objectives and measures that are set forth in ISO/IEC 27002:2013, the process control systems used by energy utilities and energy suppliers are subject to further special requirements. In comparison with conventional ICT environments (e.g. office IT, energy trading systems), there are fundamental and significant differences with respect to the development, operation, repair, maintenance and operating environment of process control systems. Furthermore, the process technology referred to in this document can represent integral components of critical infrastructures. This means they are therefore essential for the secure and reliable operation of such infrastructures. These distinctions and characteristics need to be taken into due consideration by the management processes for process control systems and justify separate consideration within the ISO/IEC 27000 family of standards.From the viewpoint of design and function, process control systems used by the energy utility sector are in fact information processing systems. They collect process data and monitor the status of the physical processes using sensors. The systems then process this data and generate control outputs that regulate actions using actuators. The control and regulation is automatic but manual intervention by operating personnel is also possible. Information and information processing systems are therefore an essential part of operational processes within energy utilities. This means that it is important that appropriate protection measures be applied in the same manner as for other organizational units.Software and hardware (e.g. programmable logic) components based on standard ICT technology are increasingly utilized in process control environments and are also covered in this document. Furthermore, process control systems in the energy utility sector are increasingly interconnected to form complex systems. Risks arising from this trend need to be considered in a risk assessment.The information and information processing systems in process control environments are also exposed to an increasing number of threats and vulnerabilities. It is therefore essential that, in the process control domain of the energy utility industry, adequate information security is achieved through the implementation and continuous improvement of an ISMS in accordance with ISO/IEC 27001:2013.Effective information security in the process control domain of the energy utility sector can be achieved by establishing, implementing, monitoring, reviewing and, if necessary, improving the applicable measures set forth in this document, in order to attain the specific security and business objectives of the organization. It is important to give particular consideration here to the special role of the energy utilities in society and to the economic necessity of a secure and reliable energy supply. Ultimately, the overall success of the cybersecurity of energy industries is based on collaborative efforts by all stakeholders (vendors, suppliers, customers, etc.).0.2	Security considerations for process control systems used by the energy utilitiesThe requirement for a general and overall information security framework for the process control domain of the energy utility industry is based on several basic requirements:a) Customers expect a secure and reliable energy supply.b) Legal and regulatory requirements demand safe, reliable and secure operation of energy supply systems.c) Energy providers require information security in order to safeguard their business interests, meet customers’ needs and comply with the legal regulations.0.3	Information security requirementsIt is essential that energy utility organizations identify their security requirements. There are three main sources of security requirements:a) The results of an organization’s risk assessment, taking into account the organization’s general business strategies and objectives. Through a risk assessment, risk sources and events are identified; potential consequences and likelihood of the occurrence of the risks are assessed.b) The requirements which result from legislation and bye-laws, regulations and contracts which have to be fulfilled by an organization, and sociocultural requirements. Particular examples include safeguarding a reliable, effective and secure energy supply as well as the reliable fulfilment of the requirements of a deregulated energy market, in particular the reliable and secure transfer of data with external parties.c) The specific principles, objectives and business requirements placed on information processing, which were developed by the organization for supporting its business operations.NOTE It is important that the energy utility organization ensure that security requirements of process control systems are analysed and adequately covered in policies for information security. The analysis of the information security requirements and objectives include the consideration of all relevant criteria for a secure energy supply and delivery, e.g.— Impairment of the security of energy supply;— Restriction of energy flow;— Affected share of population;— Danger of physical injury;— Effects on other critical infrastructures;— Effects on information privacy;— Financial impacts.The necessary security measures or controls are determined by the methodical assessment of security risks. It is necessary that the cost of controls be balanced against the economic losses that can be incurred due to security issues. The results of the risk assessment facilitate:— the definition of adequate management actions and priorities for the management of information security risks; and— the implementation of the controls chosen to protect against these risks.The risk assessment should be repeated periodically in order to take all changes into account, which can affect the results assessed.Requirements for the risk assessment and control selection are given in ISO/IEC 27001:2013.0.4	Selecting controlsOnce the security requirements and risks have been identified and decisions taken on how to deal with the risks, appropriate controls are then selected and implemented in order to ensure that the risks are reduced to an acceptable level.In addition to the controls provided by a comprehensive information security management system, this document provides additional assistance and sector-specific measures for the process control systems used by the energy utility sector, taking into consideration the special requirements in these environments. If necessary, further measures can be developed to fulfil particular requirements. The selection of security measures depends upon the decisions taken by the organization on the basis of its own risk acceptance criteria, the options for dealing with the risk and the general risk management approach of the organization. The selection of measures should also take relevant national and international law, legal ordinances and regulations into consideration.0.5	AudienceThis document is targeted at the persons responsible for the operation of process control systems used by energy utilities, information security managers, vendors, system integrators and auditors. For this target group, it details the fundamental measures in accordance with the objectives of ISO/IEC 27002:2013 and defines specific measures for process control systems of the energy utility industry, their supporting systems and the associated infrastructure.","1   ScopeThis document provides guidance based on ISO/IEC 27002:2013 applied to process control systems used by the energy utility industry for controlling and monitoring the production or generation, transmission, storage and distribution of electric power, gas, oil and heat, and for the control of associated supporting processes. This includes in particular the following:— central and distributed process control, monitoring and automation technology as well as information systems used for their operation, such as programming and parameterization devices;— digital controllers and automation components such as control and field devices or Programmable Logic Controllers (PLCs), including digital sensor and actuator elements;— all further supporting information systems used in the process control domain, e.g. for supplementary data visualization tasks and for controlling, monitoring, data archiving, historian logging, reporting and documentation purposes;— communication technology used in the process control domain, e.g. networks, telemetry, telecontrol applications and remote control technology;— Advanced Metering Infrastructure (AMI) components, e.g. smart meters;— measurement devices, e.g. for emission values;— digital protection and safety systems, e.g. protection relays, safety PLCs, emergency governor mechanisms;— energy management systems, e.g. of Distributed Energy Resources (DER), electric charging infrastructures, in private households, residential buildings or industrial customer installations;— distributed components of smart grid environments, e.g. in energy grids, in private households, residential buildings or industrial customer installations;— all software, firmware and applications installed on above-mentioned systems, e.g. DMS (Distribution Management System) applications or OMS (Outage Management System);— any premises housing the above-mentioned equipment and systems;— remote maintenance systems for above-mentioned systems.This document does not apply to the process control domain of nuclear facilities. This domain is covered by IEC 62645.This document also includes a requirement to adapt the risk assessment and treatment processes described in ISO/IEC 27001:2013 to the energy utility industry-sector–specific guidance provided in this document.",ISO/IEC 27019:2017(en)
3,"The premise of the NGCI Apex program is to discover, develop, and transition new prototype technologies, designed to protect the overall critical infrastructure of the financial sector. _New technologies will be deployed to identify, protect, detect, respond, and recover, from a wide range of evolving cyber threats across five identified asset classes: devices, applications, networks, data, and people. _These technologies will be monitored to ensure they remain effective and relevant in the face of evolving threats. _The CART has developed a matrixed approach to help identify the areas of focus for the NGCI Apex program (see Figure 8 below).",Information technology — Security techniques — Code of practice for information security controls based on ISO/IEC 27002 for cloud services,https://www.iso.org/obp/ui/#!iso:std:43757:en,"IntroductionThe guidelines contained within this Recommendation | International Standard are in addition to and complement the guidelines given in ISO/IEC 27002.Specifically, this Recommendation | International Standard provides guidelines supporting the implementation of information security controls for cloud service customers and cloud service providers. Some guidelines are for cloud service customers who implement the controls, and others are for cloud service providers to support the implementation of those controls. The selection of appropriate information security controls and the application of the implementation guidance provided, will depend on a risk assessment and any legal, contractual, regulatory or other cloud-sector specific information security requirements.",1   ScopeThis Recommendation | International Standard gives guidelines for information security controls applicable to the provision and use of cloud services by providing:– additional implementation guidance for relevant controls specified in ISO/IEC 27002;– additional controls with implementation guidance that specifically relate to cloud services.This Recommendation | International Standard provides controls and implementation guidance for both cloud service providers and cloud service customers.,ISO/IEC 27017:2015(en)
4,"The premise of the NGCI Apex program is to discover, develop, and transition new prototype technologies, designed to protect the overall critical infrastructure of the financial sector. _New technologies will be deployed to identify, protect, detect, respond, and recover, from a wide range of evolving cyber threats across five identified asset classes: devices, applications, networks, data, and people. _These technologies will be monitored to ensure they remain effective and relevant in the face of evolving threats. _The CART has developed a matrixed approach to help identify the areas of focus for the NGCI Apex program (see Figure 8 below).",Information technology — Security techniques — Code of practice for Information security controls based on ISO/IEC 27002 for telecommunications organizations,https://www.iso.org/obp/ui/#!iso:std:64143:en,,,ISO/IEC 27011:2016(en)
5,"The premise of the NGCI Apex program is to discover, develop, and transition new prototype technologies, designed to protect the overall critical infrastructure of the financial sector. _New technologies will be deployed to identify, protect, detect, respond, and recover, from a wide range of evolving cyber threats across five identified asset classes: devices, applications, networks, data, and people. _These technologies will be monitored to ensure they remain effective and relevant in the face of evolving threats. _The CART has developed a matrixed approach to help identify the areas of focus for the NGCI Apex program (see Figure 8 below).",Information technology — Security techniques — Sector-specific application of ISO/IEC 27001 — Requirements,https://www.iso.org/obp/ui/#!iso:std:42508:en,,"1   ScopeThis International Standard defines the requirements for the use of ISO/IEC 27001 in any specific sector (field, application area or market sector). It explains how to include requirements additional to those in ISO/IEC 27001, how to refine any of the ISO/IEC 27001 requirements, and how to include controls or control sets in addition to ISO/IEC 27001:2013, Annex A.This International Standard ensures that additional or refined requirements are not in conflict with the requirements in ISO/IEC 27001.This International Standard is applicable to those involved in producing sector-specific standards that relate to ISO/IEC 27001.",ISO/IEC 27009:2016(en)
6,"The premise of the NGCI Apex program is to discover, develop, and transition new prototype technologies, designed to protect the overall critical infrastructure of the financial sector. _New technologies will be deployed to identify, protect, detect, respond, and recover, from a wide range of evolving cyber threats across five identified asset classes: devices, applications, networks, data, and people. _These technologies will be monitored to ensure they remain effective and relevant in the face of evolving threats. _The CART has developed a matrixed approach to help identify the areas of focus for the NGCI Apex program (see Figure 8 below).",Health informatics — Information security management in health using ISO/IEC 27002,https://www.iso.org/obp/ui/#!iso:std:62777:en,"IntroductionThis International Standard provides guidance to healthcare organizations and other custodians of personal health information on how best to protect the confidentiality, integrity and availability of such information. It is based upon and extends the general guidance provided by ISO/IEC 27002:2013 and addresses the special information security management needs of the health sector and its unique operating environments. While the protection and security of personal information is important to all individuals, corporations, institutions and governments, there are special requirements in the health  sector that need to be met to ensure the confidentiality, integrity, auditability and availability of personal health information. This type of information is regarded by many as being among the most confidential of all types of personal information. Protecting this confidentiality is essential if the privacy of subjects of care is to be maintained. The integrity of health information is to be protected to ensure patient safety, and an important component of that protection is ensuring that the information’s entire life cycle be fully auditable. The availability of health information is also critical to effective healthcare delivery. Health informatics systems is to meet unique demands to remain operational in the face of natural disasters, system failures and denial-of-service attacks. Protecting the confidentiality, integrity and availability of health information therefore requires health sector specific expertise.Regardless of size, location and model of service delivery, all healthcare organizations need to have stringent controls in place to protect the health information entrusted to them. Yet many health professionals work as solo health providers or in small clinics that lack the dedicated IT resources to manage information security. Healthcare organizations therefore need clear, concise, and health-care-specific guidance on the selection and implementation of such controls. This International Standard is to be adaptable to the wide range of sizes, locations, and models of service delivery found in healthcare. Finally, with increasing electronic exchange of personal health information between health professionals (including use of wireless and Internet services), there is a clear benefit in adopting a common reference for information security management in healthcare.ISO/IEC 27002 is already being used extensively for health informatics IT security management through the agency of national or regional guidelines in Australia, Canada, France, the Netherlands, New Zealand, South Africa, the United Kingdom and elsewhere. ISO 27799 draws upon the experience gained in these national endeavours in dealing with the security of personal health information and is intended as a companion document to ISO/IEC 27002. It is not intended to supplant the ISO/IEC 27000- series of standards. Rather, it is a complement to these more generic standards.ISO 27799 applies ISO/IEC 27002 to the healthcare domain in a way that carefully considers the appropriate application of security controls for the purposes of protecting personal health information. These considerations have, in some cases, led the authors to conclude that application of certain ISO/IEC 27002 control objectives is essential if personal health information is to be adequately protected. ISO 27799 therefore places constraints upon the application of certain security controls specified in ISO/IEC 27002.All of the security control objectives described in ISO/IEC 27002 are relevant to health informatics, but some controls require additional explanation in regard to how they can best be used to protect the confidentiality, integrity and availability of health information. There are also additional health sector specific requirements. This International Standard provides additional guidance in a format that persons responsible for health information security can readily understand and adopt.In the health domain, it is possible for an organization (a hospital, say) to be certified using ISO/IEC 27001 without requiring certification against or even acknowledgement of ISO 27799. It is to be hoped, however, that as healthcare organizations strive to improve the security of personal health information, conformance with ISO 27799 as a stricter standard for healthcare will also become widespread.ObjectivesMaintaining information confidentiality, availability, and integrity (including authenticity, accountability and auditability) are the overarching goals of information security. In healthcare, privacy of subjects of care depends upon maintaining the confidentiality of personal health information. To maintain confidentiality, measures is also be taken to maintain the integrity of data, if for no other reason than that it is possible to corrupt the integrity of access control data, audit trails, and other system data in ways that allow breaches in confidentiality to take place or to go unnoticed. In addition, patient safety depends upon maintaining the integrity of personal health information, failure to do this can also result in illness, injury or even death. Likewise, a high level of availability is an especially important attribute of health systems, where treatment is often time-critical. Indeed, disasters that could lead to outages in other, non-health related, IT systems may be the very times when the information contained in health systems is most critically needed. Moreover, denial of service attacks against networked systems are increasingly common.The controls discussed in this International Standard are those identified as appropriate in healthcare to protect confidentiality, integrity and availability of personal health information and to ensure that access to such information can be audited and accounted for. These controls help to prevent errors in medical practice that might ensue from failure to maintain the integrity of health information. In addition, they help to ensure that the continuity of medical services is maintained.There are additional considerations that shape the goals of health information security. These includes the following:a) honouring legislative obligations as expressed in applicable data protection laws and regulations protecting a subject of care is right to privacy;1b) maintaining established privacy and security best practices in health informatics;c) maintaining individual and organizational accountability among health organizations and health professionals;d) supporting the implementation of systematic risk management within health organizations;e) meeting the security needs identified in common healthcare situations;f) reducing operating costs by facilitating the increased use of technology in a safe, secure, and well managed manner that supports, but does not constrain current health activities;g) maintaining public trust in health organizations and the information systems these organizations rely upon;h) maintaining professional standards and ethics as established by health-related professional organizations (insofar as information security maintains the confidentiality and integrity of health information);i) operating electronic health information systems in an environment appropriately secured against threats;j) facilitating interoperability among health systems, since health information increasingly flows among organizations and across jurisdictional boundaries (especially as such interoperability enhances the proper handling of health information to ensure its continued confidentiality, integrity and availability).Relation to information governance,2 corporate governance and clinical governanceWhile health organizations may differ in their positions on clinical governance and corporate governance, the importance of integrating and attending to information governance ought to be beyond debate as a vital support to both. As health organizations have become ever more critically dependent on information systems to support care delivery (e.g. by exploiting decision support technologies and trends towards “evidence based” rather than “experience based” healthcare), it has become evident that events in which losses of integrity, availability and confidentiality occur may have a significant clinical impact and that problems arising from such impacts will be seen to represent failures in the ethical and legal obligations inherent in a “duty of care”.All countries and jurisdictions will undoubtedly have case studies where such breaches have led to misdiagnoses, deaths, or protracted recoveries. Clinical governance frameworks need therefore to treat effective information security risk management as equal in importance to care treatment plans, infection management strategies and other “core” clinical management matters. This International Standard will assist those responsible for clinical governance in understanding the contribution made by effective information security strategies.Health information to be protectedThere are several types of information whose confidentiality, integrity and availability3 needs to be protected bya) personal health information,b) pseudonymized data derived from personal health information through some methodology for pseudonymous identification,c) statistical and research data, including anonymized data derived from personal health information by removal of personally identifying data,d) clinical/medical knowledge not related to any specific subjects of care, including clinical decision support data (e.g. data on adverse drug reactions),e) data on health professionals, staff and volunteers,f) information related to public health surveillance,g) audit trail data, produced by health information systems, that contain personal health information or pseudonymous data derived from personal health information, or that contain data about the actions of users in regard to personal health information, andh) system security data for health information systems, including access control data and other security related system configuration data, for health information systems.The extent to which confidentiality, integrity and availability need to be protected depends upon the nature of the information, the uses to which it is put, and the risks to which it is exposed. For example, statistical data [item c) above] may not be confidential, but protecting its integrity may be very important. Likewise, audit trail data [item g) above] might not require high availability (frequent archiving with a retrieval time measured in hours rather than seconds might suffice in a given application) but its content might be highly confidential. Risk assessment can properly determine the level of effort needed to protect confidentiality, integrity and availability (see B.4.4). The results of regular risk assessment need to be fitted to the priorities and resources of the implementing organization.Threats and vulnerabilities in health information securityTypes of information security threats and vulnerabilities vary widely, as do their descriptions. While none are truly unique to healthcare, what is unique in healthcare is the array of factors to be considered when assessing threats and vulnerabilities.By their nature, health organizations operate in an environment where visitors and the public at large can never be totally excluded. In large health organizations, the sheer volume of people moving through operational areas is significant. These factors increase the vulnerability of systems to physical threats. The likelihood that such threats will occur may increase when emotional or mentally ill subjects of care or relatives are present.The critical importance of correctly identifying subjects of care and correctly matching them to their health records leads health organizations to collect detailed identifying information. Regional or jurisdictional patient registries (i.e. registries of subjects of care) are sometimes the most comprehensive and up-to-date repositories of identifying information available in a jurisdiction. This identifying information is of great potential value to those who would use it to commit identity theft and so should be rigorously protected.Many health organizations are chronically under-funded and their staff members are sometimes obliged to work under significant stress and with systems kept in service long after they ought to have been retired. These factors can increase the potential for certain types of threat and can exacerbate vulnerabilities. On the other hand, clinical care involves a range of professional, technical, administrative, ancillary and voluntary staff, many of whom see their work as a vocation. Their dedication and diversity of experience can often usefully reduce exposure to vulnerabilities. The high level of professional training received by many health professionals also sets healthcare apart from many other industrial sectors in reducing the incidence of insider threats.The health environment, with its unique threats and vulnerabilities should therefore be considered with special care. Annex A contains an informative list of the types of threat that need to be considered by health organizations when they assess risks to the confidentiality, integrity and availability of health information and to the integrity and availability of related information systems.Who should read this International Standard?This International Standard is intended for those responsible for overseeing health information security and for healthcare organizations and other custodians of health information seeking guidance on this topic, together with their security advisors, consultants, auditors, vendors and third-party service providers.This International Standards authors do not intend to write a primer on computer security, nor to restate what has already been written in ISO/IEC 27002 or in ISO/IEC 27001. There are many security requirements that are common to all computer-related systems, whether used in financial services, manufacturing, industrial control, or indeed in any other organized endeavour. A concerted effort has been made to focus on security requirements necessitated by the unique challenges of delivering electronic health information that supports the provision of care.Benefits of using this International StandardISO/IEC 27002 is a broad and complex International Standard and its advice is not tailored specifically to healthcare. ISO 27799 allows for the implementation of ISO/IEC 27002 within health environments in a consistent fashion and with particular attention to the unique challenges that the health sector poses. By following it, healthcare organizations help to ensure that the confidentiality and integrity of data in their care is maintained, that critical health information systems remain available and that accountability for health information is upheld.The adoption of this International Standard by healthcare organizations both within and among jurisdictions will assist interoperation and enable the safe adoption of new collaborative technologies in the delivery of healthcare. Secure and privacy-protective information sharing can significantly improve healthcare outcomes.As a result of implementing this International Standard, healthcare organizations can expect to see the number and severity of their security incidents reduced, allowing resources to be redeployed to productive activities. IT security will thereby allow health resources to be deployed in a cost effective and productive manner. Indeed, research by the respected Information Security Forum and by market analysts has shown that good all-round security can have as much as a 2 % positive effect upon organizations’ results.Finally, a consistent approach to IT security, understandable by all involved in healthcare, will improve staff morale and increase the trust of the public in the systems that maintain personal health information.How to use this International StandardReaders not already familiar with ISO/IEC 27002 are urged to read the introductory clauses of that standard before continuing. The implementers of ISO 27799 is to read first thoroughly ISO/IEC 27002, as the text below will frequently refer the reader to the relevant clauses of that standard. The present International Standard cannot be fully understood without access to the full text of ISO/IEC 27002.Readers seeking guidance on how to implement ISO/IEC 27002 in a health environment will find a practical action plan described in Annex B. No mandatory requirements are contained in this clause. Instead, general advice and guidance are given on how best to proceed with implementation of ISO/IEC 27002 in healthcare. The clause is organized around a cycle of activities (plan/do/check/act) that are described in ISO/IEC 27001 and that, when followed, will lead to a robust implementation of an information security management system.Readers seeking specific advice on the security control security control categories and clauses described in ISO/IEC 27002 will find it in the clauses of this International Standard with the same clause number and title as is found in ISO/IEC 27002. This clause leads the reader through each of the eleven security control clauses of the ISO/IEC 27002. Minimum requirements are stated where appropriate and, in some cases, normative guidelines are set out on the proper application of certain ISO/IEC 27002 security controls to the protection of health information.Once ISO/IEC 27002 has been put into place, the ongoing management is considered essential if the benefits of the International Standard are to be maintained. Clause 18 discusses compliance assessment and the requirements for ongoing information security management. Annex C contains a self-assessment matrix with regard to compliance.This International Standard concludes with four informative appendices.Annex A describes the general threats to health information. Annex B briefly describes a practical action plan for implementing complementary information security related International Standards. Annex C provides a checklist for compliance to ISO 27799. Clause 2 lists the standards that are cited in a normative way; the Bibliography lists other related standards in health information security.","1   ScopeThis International Standard gives guidelines for organizational information security standards and information security management practices including the selection, implementation and management of controls taking into consideration the organization’s information security risk environment(s).This International Standard defines guidelines to support the interpretation and implementation in health informatics of ISO/IEC 27002 and is a companion to that International Standard.4This International Standard provides implementation guidance for the controls described in ISO/IEC 27002 and supplements them where necessary, so that they can be effectively used for managing health information security. By implementing this International Standard, healthcare organizations and other custodians of health information will be able to ensure a minimum requisite level of security that is appropriate to their organization’s circumstances and that will maintain the confidentiality, integrity and availability of personal health information in their care.This International Standard applies to health information in all its aspects, whatever form the information takes (words and numbers, sound recordings, drawings, video, and medical images), whatever means are used to store it (printing or writing on paper or storage electronically), and whatever means are used to transmit it (by hand, through fax, over computer networks, or by post), as the information is always be appropriately protected.This International Standard and ISO/IEC 27002 taken together define what is required in terms of information security in healthcare, they do not define how these requirements are to be met. That is to say, to the fullest extent possible, this International Standard is technology-neutral. Neutrality with respect to implementing technologies is an important feature. Security technology is still undergoing rapid development and the pace of that change is now measured in months rather than years. By contrast, while subject to periodic review, International Standards are expected on the whole to remain valid for years. Just as importantly, technological neutrality leaves vendors and service providers free to suggest new or developing technologies that meet the necessary requirements that this International Standard describes.As noted in the introduction, familiarity with ISO/IEC 27002 is indispensable to an understanding of this International Standard.The following areas of information security are outside the scope of this International Standard:a) methodologies and statistical tests for effective anonymization of personal health information;b) methodologies for pseudonymization of personal health information (see Bibliography for a brief description of a Technical Specification that deals specifically with this topic);c) network quality of service and methods for measuring availability of networks used for health informatics;d) data quality (as distinct from data integrity).",ISO 27799:2016(en)
7,"When the Cyber Apex Consortium contractor begins representational (open-source) architecture testing and operational architecture testing, using institution-specific parameters, (see Figure 10), different risk factors will apply. Poorly managed software integration for testing purposes has the potential to generate setbacks. Improper testing and evaluation methods could result in a cyber technology product not fully ready for deployment. The same is true of inaccurate threat modeling. _Not having deployable-ready technologies available to the customer can lead to schedule delays. _Sub-standard software performance may have the same effect. Since the NGCI Apex program will be broken up into distinct project efforts, some risks may be project dependent. ",Risk management — Principles and guidelines,https://www.iso.org/obp/ui/#!iso:std:43170:en,"IntroductionOrganizations of all types and sizes face internal and external factors and influences that make it uncertain whether and when they will achieve their objectives. The effect this uncertainty has on an organization's objectives is “risk”.All activities of an organization involve risk. Organizations manage risk by identifying it, analysing it and then evaluating whether the risk should be modified by risk treatment in order to satisfy their risk criteria. Throughout this process, they communicate and consult with stakeholders and monitor and review the risk and the controls that are modifying the risk in order to ensure that no further risk treatment is required. This International Standard describes this systematic and logical process in detail.While all organizations manage risk to some degree, this International Standard establishes a number of principles that need to be satisfied to make risk management effective. This International Standard recommends that organizations develop, implement and continuously improve a framework whose purpose is to integrate the process for managing risk into the organization's overall governance, strategy and planning, management, reporting processes, policies, values and culture.Risk management can be applied to an entire organization, at its many areas and levels, at any time, as well as to specific functions, projects and activities.Although the practice of risk management has been developed over time and within many sectors in order to meet diverse needs, the adoption of consistent processes within a comprehensive framework can help to ensure that risk is managed effectively, efficiently and coherently across an organization. The generic approach described in this International Standard provides the principles and guidelines for managing any form of risk in a systematic, transparent and credible manner and within any scope and context.Each specific sector or application of risk management brings with it individual needs, audiences, perceptions and criteria. Therefore, a key feature of this International Standard is the inclusion of “establishing the context” as an activity at the start of this generic risk management process. Establishing the context will capture the objectives of the organization, the environment in which it pursues those objectives, its stakeholders and the diversity of risk criteria – all of which will help reveal and assess the nature and complexity of its risks.The relationship between the principles for managing risk, the framework in which it occurs and the risk management process described in this International Standard are shown in Figure 1.When implemented and maintained in accordance with this International Standard, the management of risk enables an organization to, for example:— increase the likelihood of achieving objectives;— encourage proactive management;— be aware of the need to identify and treat risk throughout the organization;— improve the identification of opportunities and threats;— comply with relevant legal and regulatory requirements and international norms;— improve mandatory and voluntary reporting;— improve governance;— improve stakeholder confidence and trust;— establish a reliable basis for decision making and planning;— improve controls;— effectively allocate and use resources for risk treatment;— improve operational effectiveness and efficiency;— enhance health and safety performance, as well as environmental protection;— improve loss prevention and incident management;— minimize losses;— improve organizational learning; and— improve organizational resilience.This International Standard is intended to meet the needs of a wide range of stakeholders, including:a) those responsible for developing risk management policy within their organization;b) those accountable for ensuring that risk is effectively managed within the organization as a whole or within a specific area, project or activity;c) those who need to evaluate an organization's effectiveness in managing risk; andd) developers of standards, guides, procedures and codes of practice that, in whole or in part, set out how risk is to be managed within the specific context of these documents.The current management practices and processes of many organizations include components of risk management, and many organizations have already adopted a formal risk management process for particular types of risk or circumstances. In such cases, an organization can decide to carry out a critical review of its existing practices and processes in the light of this International Standard.In this International Standard, the expressions “risk management” and “managing risk” are both used. In general terms, “risk management” refers to the architecture (principles, framework and process) for managing risks effectively, while “managing risk” refers to applying that architecture to particular risks.Figure 1
—
Relationships between the risk management principles, framework and process","1   ScopeThis International Standard provides principles and generic guidelines on risk management.This International Standard can be used by any public, private or community enterprise, association, group or individual. Therefore, this International Standard is not specific to any industry or sector.NOTE For convenience, all the different users of this International Standard are referred to by the general term “organization”.This International Standard can be applied throughout the life of an organization, and to a wide range of activities, including strategies and decisions, operations, processes, functions, projects, products, services and assets.This International Standard can be applied to any type of risk, whatever its nature, whether having positive or negative consequences.Although this International Standard provides generic guidelines, it is not intended to promote uniformity of risk management across organizations. The design and implementation of risk management plans and frameworks will need to take into account the varying needs of a specific organization, its particular objectives, context, structure, operations, processes, functions, projects, products, services, or assets and specific practices employed.It is intended that this International Standard be utilized to harmonize risk management processes in existing and future standards. It provides a common approach in support of standards dealing with specific risks and/or sectors, and does not replace those standards.This International Standard is not intended for the purpose of certification.",ISO 31000:2009(en)
8,"When the Cyber Apex Consortium contractor begins representational (open-source) architecture testing and operational architecture testing, using institution-specific parameters, (see Figure 10), different risk factors will apply. Poorly managed software integration for testing purposes has the potential to generate setbacks. Improper testing and evaluation methods could result in a cyber technology product not fully ready for deployment. The same is true of inaccurate threat modeling. _Not having deployable-ready technologies available to the customer can lead to schedule delays. _Sub-standard software performance may have the same effect. Since the NGCI Apex program will be broken up into distinct project efforts, some risks may be project dependent. ",Information technology — Service management — Part 1: Service management system requirements,https://www.iso.org/obp/ui/#!iso:std:51986:en,"IntroductionThe requirements in this part of ISO/IEC 20000 include the design, transition, delivery and improvement of services that fulfil service requirements and provide value for both the customer and the service provider. This part of ISO/IEC 20000 requires an integrated process approach when the service provider plans, establishes, implements, operates, monitors, reviews, maintains and improves a service management system (SMS).Co-ordinated integration and implementation of an SMS provides ongoing control and opportunities for continual improvement, greater effectiveness and efficiency. The operation of processes as specified in this part of ISO/IEC 20000 requires personnel to be well organized and co-ordinated. Appropriate tools can be used to enable the processes to be effective and efficient.The most effective service providers consider the impact on the SMS through all stages of the service lifecycle, from strategy through design, transition and operation, including continual improvement.This part of ISO/IEC 20000 requires the application of the methodology known as “Plan-Do-Check-Act” (PDCA) to all parts of the SMS and the services. The PDCA methodology, as applied in this part of ISO/IEC 20000, can be briefly described as follows.Plan: establishing, documenting and agreeing the SMS. The SMS includes the policies, objectives, plans and processes to fulfil the service requirements.Do: implementing and operating the SMS for the design, transition, delivery and improvement of the services.Check: monitoring, measuring and reviewing the SMS and the services against the policies, objectives, plans and service requirements and reporting the results.Act: taking actions to continually improve performance of the SMS and the services.When used within an SMS, the following are the most important aspects of an integrated process approach and the PDCA methodology:a) understanding and fulfilling the service requirements to achieve customer satisfaction;b) establishing the policy and objectives for service management;c) designing and delivering services based on the SMS that add value for the customer;d) monitoring, measuring and reviewing performance of the SMS and the services;e) continually improving the SMS and the services based on objective measurements.Figure 1 illustrates how the PDCA methodology can be applied to the SMS, including the service management processes specified in Clauses 5 to 9, and the services. Each element of the PDCA methodology is a vital part of a successful implementation of an SMS. The improvement process used in this part of ISO/IEC 20000 is based on the PDCA methodology.Figure 1
—
PDCA methodology applied to service managementThis part of ISO/IEC 20000 enables a service provider to integrate its SMS with other management systems in the service provider's organization. The adoption of an integrated process approach and the PDCA methodology enables the service provider to align or fully integrate multiple management system standards. For example, an SMS can be integrated with a quality management system based on ISO 9001 or an information security management system based on ISO/IEC 27001.ISO/IEC 20000 is intentionally independent of specific guidance. The service provider can use a combination of generally accepted guidance and its own experience.Users of an International Standard are responsible for its correct application. An International Standard does not purport to include all necessary statutory and regulatory requirements and contractual obligations of the service provider. Conformity to an International Standard does not of itself confer immunity from statutory and regulatory requirements.For the purposes of research on service management standards, users are encouraged to share their views on ISO/IEC 20000-1 and their priorities for changes to the rest of the ISO/IEC 20000 series. Click on the link below to take part in the online survey.ISO/IEC 20000-1 online survey","1   Scope1.1   GeneralThis part of ISO/IEC 20000 is a service management system (SMS) standard. It specifies requirements for the service provider to plan, establish, implement, operate, monitor, review, maintain and improve an SMS. The requirements include the design, transition, delivery and improvement of services to fulfil service requirements. This part of ISO/IEC 20000 can be used by:a) an organization seeking services from service providers and requiring assurance that their service requirements will be fulfilled;b) an organization that requires a consistent approach by all its service providers, including those in a supply chain;c) a service provider that intends to demonstrate its capability for the design, transition, delivery and improvement of services that fulfil service requirements;d) a service provider to monitor, measure and review its service management processes and services;e) a service provider to improve the design, transition and delivery of services through effective implementation and operation of an SMS;f) an assessor or auditor as the criteria for a conformity assessment of a service provider's SMS to the requirements in this part of ISO/IEC 20000.Figure 2 illustrates an SMS, including the service management processes. The service management processes and the relationships between the processes can be implemented in different ways by different service providers. The nature of the relationship between a service provider and the customer will influence how the service management processes are implemented.Figure 2
—
Service management system1.2   ApplicationAll requirements in this part of ISO/IEC 20000 are generic and are intended to be applicable to all service providers, regardless of type, size and the nature of the services delivered. Exclusion of any of the requirements in Clauses 4 to 9 is not acceptable when a service provider claims conformity to this part of ISO/IEC 20000, irrespective of the nature of the service provider's organization.Conformity to the requirements in Clause 4 can only be demonstrated by a service provider showing evidence of fulfilling all of the requirements in Clause 4. A service provider cannot rely on evidence of the governance of processes operated by other parties for the requirements in Clause 4.Conformity to the requirements in Clauses 5 to 9 can be demonstrated by the service provider showing evidence of fulfilling all requirements. Alternatively, the service provider can show evidence of fulfilling the majority of the requirements themselves and evidence of the governance of processes operated by other parties for those processes, or parts of processes, that the service provider does not operate directly.The scope of this part of ISO/IEC 20000 excludes the specification for a product or tool. However, organizations can use this part of ISO/IEC 20000 to help them develop products or tools that support the operation of an SMS.NOTE ISO/IEC TR 20000-3 provides guidance on scope definition and applicability of this part of ISO/IEC 20000. This includes further explanation about the governance of processes operated by other parties.",ISO/IEC 20000-1:2011(en)
9,"When the Cyber Apex Consortium contractor begins representational (open-source) architecture testing and operational architecture testing, using institution-specific parameters, (see Figure 10), different risk factors will apply. Poorly managed software integration for testing purposes has the potential to generate setbacks. Improper testing and evaluation methods could result in a cyber technology product not fully ready for deployment. The same is true of inaccurate threat modeling. _Not having deployable-ready technologies available to the customer can lead to schedule delays. _Sub-standard software performance may have the same effect. Since the NGCI Apex program will be broken up into distinct project efforts, some risks may be project dependent. ",Industrial automation systems and integration — Product data representation and exchange — Part 35: Conformance testing methodology and framework: Abstract test methods for standard data access interface (SDAI) implementations,https://www.iso.org/obp/ui/#!iso:std:29286:en,"IntroductionISO 10303 is an International Standard for the computer-interpretable representation of product information and for the exchange of product data. The objective is to provide a neutral mechanism capable of describing products throughout their life cycle. This mechanism is suitable not only for neutral file exchange, but also as a basis for implementing and sharing product databases, and as a basis for archiving.This part of ISO 10303 is a member of the conformance testing methodology and framework series. It specifies the abstract test methods for SDAI implementations. SDAI is the standard data access interface specification to data that has been defined using ISO 10303-11. SDAI is specified in ISO 10303-22. This part follows the general concepts of conformance testing defined in ISO 10303-31.Major subdivisions in this part of ISO 10303 are:— Abstract test cases, groups, suite and verdict criteria in clause 6;— SDAI operations wrapped in EXPRESS procedures and functions with verdict criteria in clause 7;— The EXPRESS structure test schema ESTS as the target for the abstract test cases in clause 8.","1   ScopeThis part of ISO 10303 specifies the abstract test methods and requirements for conformance testing of an implementation of a language binding of the SDAI. Since the SDAI is specified independently of any programming language, the abstract test methods presented in this part are applicable to all SDAI language bindings. The abstract test methods support as well the various implementation classes as specified in ISO 10303-22.The following are within the scope of this part of ISO 10303:— abstract test methods for software systems that implement the SDAI;— the specification, in a manner that is independent of any SDAI language binding, of the methods and approaches for testing of various SDAI operations;— the specification and documentation of abstract test cases.The following are outside the scope of this part of ISO 10303:— the development of test data and/or test programs for specific language bindings;— the specification of test methods, algorithms, or programs for the conformance testing of applications that interact with SDAI implementations;— the architecture and implementation approach for a conformance test system that realizes the test methods specified in this part of ISO 10303.",ISO/TS 10303-35:2003(en)
10,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.","Digital analytics and web analyses for purposes of market, opinion and social research — Vocabulary and service requirements",https://www.iso.org/obp/ui/#!iso:std:66187:en,"IntroductionAnalyses of digital behaviour and online digital statements by persons and companies have rapidly increased in importance. Examples are the measurement of the behaviour of website visitors, the measurement of behaviour by means of cookies, and the registration and measurement of statements and sentiments of users of social media.This document provides insight into the working methods of service providers in the fields of digital analytics and web analyses research and, in this way, provides clients with transparency regarding the services they offer. This document is intended to supplement and be used in conjunction with ISO 20252.Digital analytics and web analyses for the purpose of market, opinion and social research can be separated from the equivalent analyses carried out for non-research purposes. In both cases, the protection of privacy of the persons analysed is regulated by legal provisions that apply to the particular project and, furthermore, by the relevant professional codes of conduct and other ethical guidelines.","1   ScopeThis document specifies the terms and definitions, as well as the service requirements, for organizations and professionals that conduct digital analytics and web analyses for collecting, analysing and reporting of digital data for purposes of market, opinion and social research by various methods and techniques. It provides the criteria against which the quality of such services can be assessed and evaluated.This document applies to digital analytics and web analyses conducted by service providers on their own initiative, commissioned by clients or conducted by clients themselves.This document applies to digital and web analysis research activities such as:— understanding the usage of websites via the use of cookies, page impressions and other means, navigation across sites, time spent by visitors and their actions;— online metered panels, e.g. on-going measurement of web visitation via meters installed on panellists’ desktop, mobile or tablet devices;— tag-based solutions to measure online usage at universe level, which can be integrated with metered panel data to provide a hybrid measurement;— social media analytics which collect, aggregate and analyse online comments, and user-generated content such as blogs, forums and comments on news sites or other sites.NOTE Universe can also be known as population.This document can be construed to cover all forms of digital data collection including from desktop computers, tablets, mobile devices and over the top (OTT) devices as well as internet of things (IoT) devices where applicable.",ISO 19731:2017(en)
11,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.",Space systems — Cube satellites (CubeSats),https://www.iso.org/obp/ui/#!iso:std:60496:en,"IntroductionRecent years have seen an increase in the number of student satellites developed at universities around the world. To date, most university satellites require several years to develop and significant financial resources, making them prohibitive for small programs. New technological developments in small low-power electronics make smaller, lower-cost satellites feasible.The CubeSat program has developed a picosatellite standard that significantly reduces the cost and development time of picosatellites with a specific form factor. In addition, CubeSats can serve as platforms for in-space experimentation, as well as a means of space-qualifying future small-satellite hardware.The CubeSat Standard is an evolution of the picosatellites developed for Stanford’s OPAL mission. CubeSats are constrained to a 100 mm cube (not including deployment interface rails) with a mass of one kilogram or less. Led by Stanford University’s Space Systems Development Lab (SSDL), the CubeSat project is developed jointly by universities and industry worldwide. Within this international community CubeSat developments at the California Polytechnic State University (CalPoly) have been twofold: first, develop the standardized launcher-interface/deployer mechanism for CubeSats, and second, demonstrate the feasibility of developing a working CubeSat using low-cost, commercial off-the-shelf components. The project involves a multidisciplinary team of software, aerospace, manufacturing, electrical, and mechanical engineering undergraduate students.In recent years, more sophisticated capabilities have been demonstrated in CubeSats by major space corporations and major space customers. CubeSat concepts for inclusion in Mars exploration are in development. Entire companies have been established to solely support the global CubeSat marketplace.","1   ScopeThis document addresses CubeSats, CubeSat Deployer and related verification of assurance/quality terms and metrics.This document defines a unique class of picosatellite, the CubeSat. CubeSats are ideal as space development projects for universities around the world. In addition to their significant role in educating space scientists and engineers, CubeSats provide a low-cost platform for testing and space qualification of the next generation of small payloads in space. A key component of the project is the development of a standard CubeSat Deployer.This Deployer is capable of releasing a number of CubeSats as secondary payloads on a wide range of launchers. The standard Deployer requires all CubeSats to conform to common physical requirements, and share a standard Deployer interface. CubeSat development time and cost can be significantly reduced by the development of standards that are shared by a large number of spacecraft.Normative control of the CubeSat design, qualification and acceptance testing is generally applied from other small satellite specific standards with the exception of CubeSat/Deployer launch environment test.",ISO 17770:2017(en)
12,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.",Space systems — Design qualification and acceptance tests of small spacecraft and units,https://www.iso.org/obp/ui/#!iso:std:66008:en,"IntroductionThere is an increasing demand for small/micro/nano/pico satellite development and utilization worldwide; yet, there is no clear and globally accepted definition of what is considered “small”, “micro”, “nano” or “pico” satellites. These satellites are often built with emphasis on low cost and fast delivery. They are characterized by extensive use of non-space-qualified commercial-off-the-shelf (COTS) units. For the sake of convenience, the term “small spacecraft” is used throughout this document as a generic term to refer to these satellites.A small spacecraft is a satellite that utilizes non-traditional risk-taking development and management approaches to achieve low cost and fast delivery with a small number of team. To achieve these two points, low cost and fast delivery, satellite design relies on the use of non-space-qualified commercial-off-the-shelf (COTS) units, making satellite size inherently smaller. The design accepts a certain level of risk associated with the use of COTS.A certain set of tests is necessary to ensure the mission success of small spacecraft. Applying the same test requirements and methods as those applied to traditional large/medium satellites, however, will nullify the low-cost and fast-delivery advantages possessed by small spacecraft.This document is meant to improve the reliability of small spacecraft, especially those with commercial purpose, while maintaining the low-cost and fast-delivery nature of small spacecraft. This document intends to promote worldwide trade of small spacecraft products by providing a minimum level of assurance that a product made of non-space-qualified commercial-off-the-shelf parts and units can work in space. This document also aims to serve as a testing guideline for those who intend to enter satellite manufacturing through development of small spacecraft products.",,ISO 19683:2017(en)
13,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.",Ships and marine technology — Guidelines for the assessment of speed and power performance by analysis of speed trial data,https://www.iso.org/obp/ui/#!iso:std:61902:en,"IntroductionThis International Standard concerns the method of analysing the results obtained from speed trials.The primary purpose of speed and power trials is to determine a ship’s performance in terms of ship’s speed, power and propeller shaft speed under prescribed ship’s conditions and thereby verify the satisfactory attainment of a ship’s speed stipulated by Energy Efficiency Design Index (EEDI) regulations and/or contract. Ship’s speed is that realized under conditions stipulated by contract and/or EEDI regulations, which are usually; smooth hull and propeller surfaces, no wind, no waves, no current and deep water of 15°C.In general it cannot be expected that all such stipulated conditions will be met during the actual trials. In practice, certain corrections for the environmental conditions have to be considered, such as for water depth, wind, waves and current [1][2].The purpose of this International Standard is to define the basic requirements for the performance of speed trials, and provide procedures for evaluation and correction of speed trial data, covering all influences which may be relevant to the individual trial runs based on sound scientific grounds, thereby enabling owners and others to have confidence in the validity of the final results.This International Standard is intended to help the interested parties achieve the desired target accuracy of, within 2 % in shaft power and 0,1 kn in speed.The procedure specified in this International Standard has been developed largely on the basis of published data on speed trials and on ship’s performance, the more important among them being listed in Clause 2.ISO has invited the International Towing Tank Conference (ITTC) to co-operate on the development of a new standard for speed/power trials taking into account the new guidelines issued by ITTC and approved by MEPC65 for EEDI. The contribution of the STA-group and the ITTC is highly appreciated.Substitution of terms clauseThis International Standard is generally applied to those ships for which survey and certification of EEDI is required under International Maritime Organization (IMO) Resolution MEPC.214(63) [as amended by MEPC.234(65)]. In the case of other ships, to which the above IMO resolutions are not applicable, the terms or phrases of this International Standard are deemed to be replaced as necessary (e.g. “agreement between the Shipbuilder, the Owner and the Verifier” shall be read as “agreement between the Shipbuilder and the Owner” etc.)","1   ScopeThe primary purpose of speed and power trials is to determine a ship’s performance in terms of ship’s speed, power and propeller shaft speed under prescribed ship’s conditions and thereby verify the satisfactory attainment of a ship’s speed stipulated by EEDI regulations and/or contract.This International Standard defines and specifies the following procedures to be applied in the preparation, execution, analysis and reporting of speed trials for ships, with reference to the effects which may have an influence upon the speed, power and propeller shaft speed relationship:— the responsibility of each party involved,— the trial preparations,— the ship’s condition,— the limiting weather and sea conditions,— the trial procedure,— the execution of the trial,— the measurements required,— the data acquisition and recording,— the procedures for the evaluation and correction,— the processing of the results.The contracted ship’s speed and the ship’s speed for EEDI are determined for stipulated conditions and at specific draughts (contract draught and/or EEDI draught). For EEDI, the environmental conditions are: no wind, no waves, no current and deep water of 15°C.Normally, such stipulated conditions are unlikely to be experienced in part or in full during the actual trials. In practice, certain corrections for the environmental conditions such as water depth, surface wind, waves, current and deviating ship draught, have to be considered. For this purpose, during the speed and power trials, not only are shaft power and ship’s speed measured, but also relevant ship data and environmental conditions.The applicability of this International Standard is limited to ships of the displacement type.In this International Standard, it was decided that the unit to express the amount of an angle should be “rad” (radian) and that the unit of speed should be “m/s” (metres per second). Nevertheless, “°” (degree) as a unit for an angle and “knots” as a unit for speed may be used. However, the units for the angles and speeds which appear in calculation formulas are to be “rad” and “m/s” without exception. Moreover, for the convenience of the users of this International Standard, numerical values using the units of degree and knots are stated jointly at appropriate places.If it is physically impossible to meet the conditions in this International Standard, a practical treatment is allowed based on the documented mutual agreement among the Owner, the Verifier and the Shipbuilder.",ISO 15016:2015(en)
14,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.",Ships and marine technology — Computer applications — Specification of Maritime Safety Markup Language (MSML),https://www.iso.org/obp/ui/#!iso:std:41172:en,,"1   Scope1.1   InclusionsThis Publicly Available Specification specifies the XML application MSML (Maritime Safety Markup Language). MSML is a language for structuring information and the goal is to create an open standard that can be used generally in the maritime sector. This Publicly Available Specification emphasizes the following aspects of MSML:• functional applicability in the maritime arena with focus on repair and maintenance and related safety aspects;• secure transfer of information between vessel and shore base;• extensibility to incorporate increased functionality;• use of standardized XML support whenever needed.The basis of MSML is the data model which defines what kind of maritime data, related to vessel and shore base, it is possible to store. The data model represents the current state and only limited historic information is kept in the data model. This Publicly Available Specification defines the following information areas of the data model:• the vessel,• the actual use and status of the vessel,• the shore base,• the actual use and status of one berth of the shore base,• the relation between the vessel and the shore base,• the history of repair and maintenance and what has been done on each occasion.The data model of MSML makes it possible to describe the following states:• a vessel with/without defined task,• a berth of a shore base with/without defined task,• a vessel and a berth of a shore base with/without relation.Defining states makes it possible to define transactions, i.e. sequences of actions for fulfilling specific intentions. We have the general transactions:• assigning task to/removing task from vessel,• assigning task to/removing task from a berth of a shore base (for a specific vessel),• assigning relation/removing relation vessel — a berth of a shore base,• assigning/removing specific pieces of data,• reading data.This Publicly Available Specification defines perspectives associated with the data model of MSML. The term indicates that the MSML data model can be seen from different views. Perspectives contain information that is orthogonal to the data model. Two perspectives are defined:• inspection — an inherent perspective that makes it possible to reference results of vessel inspections;• repair and maintenance — currently the main focus of MSML.There are also other sources of information that are associated with MSML. The data model of MSML assumes that information concerning individual crew and passenger members is handled adequately using the muster list and passenger list, respectively. However, for the data model of MSML groups of individuals are considered. The primary purpose of MSML is to make it possible to identify available capabilities and resources and not to handle individuals. However, a reference is included in MSML where to find detailed information.A certain amount of shore base information is included in the data model of MSML. The main reason to include this information is to support vessel safety related aspects and vessel repair and maintenance.1.2   LimitationsAn MSML instance will contain extensive history information only if included in the definition of a perspective. For example, repair and maintenance contains an extensive history of what has been changed and when, but there is no extensive history information for e.g. bunkering. The reason for including history is that actions made in the past can affect future events and decisions. However, some minor historic information is included in the data model of MSML (i.e. not within perspectives), e.g. the history of vessel name changes. If other types of extensive history information are needed, a new version of the MSML instance has to be stored for each significant change and this must be handled outside the scope of MSML (probably using a native XML database).The data model of MSML should not primarily be seen as a support for normal work onboard. Instead it is an add-on support for transfers of safety related information to/from the vessel from/to external units and within the vessel. For example, current propeller revolutions per minute cannot be extracted from the computerized MSML system, instead the value is read directly from the ordinary equipment. In the same way the shore base part of the MSML data model is seen as a support to the safety related vessel information and not from normal work ashore. Generally, normal dynamic information during a voyage between two shore bases is not included within MSML. On the other hand, alarms and malfunctioning units are generally safety related and of interest internally and externally and thus included in the data model of MSML. If alarms are set automatically or not is a question outside the scope of MSML.1.3   ExclusionsThe following aspects are not included in the scope of MSML:• aspects concerning costs and fees;• geographic information;• logs, e.g. log of communication;• specific cargo information, e.g. tracing, Smart and Secure Tradelanes;• presentation of information;• users and their authorities;• actual use of data and instances, e.g. definition of messages;• bindings to protocols.1.4   SummaryTo sum up the main characteristics of MSML, it• contains an XML-based data model for information exchange and processing in safety-critical maritime applications;• does not describe how data is used;• supports information security and extensibility;• is a framework and future open standard for the maritime safety sector.The principal system dependences on MSML are shown in Figure 2.Figure 2
—
Principal system dependences on MSMLWe see that MSML will affect system design and definition of messages but MSML is not affected. Thus MSML can remain a stable standard for a wide variety of applications.",ISO/PAS 22853:2005(en)
15,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.",Guidelines for assessing the adverse environmental impact of fire effluents — Part 2: Methodology for compiling data on environmentally significant emissions from fires,https://www.iso.org/obp/ui/#!iso:std:50635:en,"IntroductionPollution of indoor and outdoor environments by complex mixtures of physical and chemical combustion products is a causative agent of human health and environmental problems on a global scale. Uncontrolled and incomplete combustion processes are responsible for the emission of chemical and physical pollutants in quantities that affect humans and the environment. This problem is severe, not only in industrialized regions, but also in less developed, predominantly agricultural areas where people depend on biomass as fuel for cooking, heating and lighting.General awareness of the fact that fires can present acute and persistent adverse effects on the environment has been accentuated by a number of high impact incidents over the past half century. Annex A contains a limited number of examples of emissions associated with various types of fires which could be expected to affect the environment adversely. These examples should not be considered as describing typical observations as fires and fire impacts are generally not comparable.The serious consequences of such events have confirmed that the environmental impact of fires is an international issue that urgently needs to be dealt with globally and systematically. The ISO 26367 series of International Standards provides a framework for a common treatment of the environmental impact of fires in answer to this pressing need.This document provides methods for the compilation of relevant data for assessing damage after a fire and for use in environmental fire hazard and risk assessments.In view of the fact that relevant quantitative data on environmentally hazardous components of fire effluents cannot routinely be obtained from accidental fires, appropriate data may also be obtained from real-scale fire tests and simulations involving physical fire models.The Sixth EC Environmental Action Programme, Environment 2010: Our Future, Our Choice spells out the objective of controlling levels of man-made chemicals so that they do not give rise to significant adverse impacts on human health or the environment[1]. In the case of eco-toxicity indicators, toxicity impact potential (TIP) characterization factors are used and are often developed using a multimedia environmental fate model to predict the movement and distribution of a given substance in environmental regions of interest[2].In the case of organohalogen compounds that are known to have natural as well as man-made sources, TIP characterization can be difficult, a fact acknowledged by the Convention for the Protection of the Marine Environment of the North-East Atlantic (OSPAR) in quality status report, QSR 2000, as follows: “Many substances occur naturally in soils, plants and animals. It is therefore important to distinguish between the natural concentrations and fluxes of these substances and the extent to which they are augmented by human activities. Such distinctions are essential if informed decisions are to be made regarding the management of contaminants”[3].This document is principally intended for use by the following parties: environmental regulatory authorities, fire fighters and investigators, storage facility operators, materials and product manufacturers, property owners, and public health authorities.","1   ScopeThis document specifies a methodology for compiling the information needed to assess the environmental damage caused by a fire incident. This includes conducting a site reconnaissance, establishing data quality objectives and designing sampling programmes. This document also provides a standardized method for reporting the results of the compilation and findings of the analyses, for use in contingency planning or for the assessment of the potential adverse environmental impact of a specific fire incident. This document does not include specific instruction on sampling and analysis of fire effluents. Sampling and analysis are the focus of a future document in the ISO 26367 series. This document is applicable to uncontrolled fires, including fires in commercial and domestic premises, unenclosed commercial sites, agricultural storage sites, wildland and forest fires, as well as fires involving road, rail and maritime transport systems.This document focuses on the fire effluents that are environmentally significant, including pollutants causing short-term effects (e.g. pollutants causing biotope damage and components of smog) and long-term effects (e.g. persistent organic pollutants, POP). Since it is not possible to treat all potential pollutants that could be found in fire effluents in a single document, a list of those pollutants specifically addressed in this document is given below:a) pollutants with short-term effects: halogenated acids (HX), metals, nitrogen oxides (NOx), particulates, and sulfur oxides (SOx);b) pollutants with long-term effects: metals, particulates, perfluorinated compounds (PFC), polyaromatic hydrocarbons (PAH), polychlorinated biphenyls (PCB), and polyhalogenated dioxins and furans (PXDD/PXDF).The reporting template provided in Annex D proposes additional potential pollutants and indicators for inclusion in the compilation. Not all of the pollutants and indicators listed in Table D.1 are relevant to every fire site, and others not mentioned in the table can apply.This document does not include direct acute toxicity issues on humans, which are covered by other standards, such as ISO 13344 and ISO 13571.",ISO 26367-2:2017(en)
16,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.",Guidelines for assessing the adverse environmental impact of fire effluents — Part 1: General,https://www.iso.org/obp/ui/#!iso:std:43529:en,"IntroductionIn view of the fact that relevant quantitative data on environmentally hazardous components of fire effluents cannot routinely be obtained from accidental fires, appropriate data may also have to be obtained from real scale fire tests and simulations involving physical fire models.General awareness of the fact that large fires present dramatic and persistent adverse effects on the environment has been accentuated by a number of high-impact incidents over the past half-century. Annex A contains a list of major fire incidents in recent years.The serious consequences of such events have confirmed that the environmental impact of fires is a pressing international issue that urgently needs to be dealt with globally and systematically. This part of ISO 26367 provides a framework for a common treatment of the environmental impact of fires in answer to this pressing need.It is principally intended for use by the following parties:— fire-fighters and investigators;— building owners and managers;— storage facility operators;— materials and product manufacturers;— insurance providers;— environmental regulatory authorities;— civil defence organizations;— public health authorities.","1   ScopeThis part of ISO 26367 gives guidelines whose primary focus is the assessment of the adverse environmental impact of fire effluents, including those from fires occurring in commercial and domestic premises, unenclosed commercial sites, industrial and agricultural sites, as well as those involving road, rail and maritime transport systems. Its scope does not extend to direct acute toxicity issues, which are covered by other existing International Standards.It is intended to serve as a tool for the development of standard protocols fora) the assessment of local and remote adverse environmental impacts of fires, and the definition of appropriate preventive measures,b) post-fire analyses to identify the nature and extent of the adverse environmental impacts of fires, andc) the collection of relevant data for use in environmental fire hazard assessments.This part of ISO 26367 is intended as an umbrella document to set the scene concerning what should be considered when determining the environmental impact of fires. It is not a comprehensive catalogue of methods and models defining how to determine the environmental impact of fires, intended to be addressed by other parts of ISO 26367.",ISO 26367-1:2011(en)
17,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.",Geographic information — Imagery and gridded data,https://www.iso.org/obp/ui/#!iso:std:29775:en,"IntroductionThis Technical Report is intended to identify the manner by which ISO/TC 211 should handle imagery and gridded data in the context of its standards.A natural image is a radiometric representation of the real world, as seen by an optical or other sensor. A synthetic image is a generated depiction of spatial data in a visual form. Both natural and synthetic image data are being used increasingly in the area of geographic information/geomatics. Gridded data is the representation of attribute values in terms of a spatial grid. All three of these forms of spatial information can be handled in a similar manner by representation in terms of a raster or matrix structure. Both a raster and a matrix are array structures that may be coded somewhat differently due to the characteristics of the data.An increasingly large volume of natural and synthetic image and gridded data is being produced. For example, current imaging satellites include LandSAT, RADARSAT, SPOT, ERS, MOS, JERS and NOAA. Also, there are military satellite images and other public and private domain image sources. There are current plans to launch more than one hundred Earth observing satellites by the year 2005, with 60 of those scheduled for launch by the end of 1999, with some of these satellites generating as many as 22 000 scenes per day. Digital orthophoto mapping is another field in which major financial investment is being made and in which a large volume of raster data is being produced. Obviously, there will be great demand for this imagery to be in a standard format in order to be useful with other sources of data.Large volumes of synthetic imagery are being produced by the scanning of the current large inventory of paper maps and charts, and it is expected that the volume of this data will exceed the production of vector based data sets for a long time. Synthetic raster maps include scanned paper map products, such as topographic maps, nautical charts, soil and vegetation maps and other such products. They also include raster data sets generated directly from vector data sets. Since the demand for image and gridded data is large and will be rapidly increasing in the near future, it is necessary to address this form of data in ISO/TC 211. Many countries and international organizations are producing Digital Elevation Models (DEMs). Many other forms of gridded data such as georeferenced socio-economic data including land use data, meteorological and bathymetric data are being compiled in large volumes.Earth observation networks and information infrastructures are being developed in many countries to further improve the access and use of remote sensing data, products and services, with the objective to provide state-of-the-art interfaces between the data archives and their users. The interoperability between data archives — an important element of which is interoperable standards — will greatly facilitate a more effective operation by government bodies and the value-added sector.The specific aim of this work item is to analyse the characteristics of imagery and gridded data and make recommendations with respect to how this data can be handled in ISO/TC 211. There is a significant overlap between many of the current ISO/TC 211 work items and those areas that require standardization to support imagery and gridded data. For example, it is possible to share many metadata elements between vector and raster representations, but some unique metadata will be required to handle particular raster related aspects.The main interests that will benefit from the standardization of raster and matrix data formats will be the distributors and end-users of raster data. Currently, each satellite effectively defines its own ""standard"" based on the characteristics of its sensors. There also exists a large number of ""standard"" formats for the exchange and distribution of synthetic raster data such as scanned paper maps. Integration of data is difficult at best.Although there are some aspects of sensor characteristics that are unique to particular data sources, there is a high degree of commonality underlying the basic parameters. In addition, many of the aspects of the existing ISO/TC 211 work on vector standards, such as geographic referencing, quality, metadata, positioning services, and portrayal are applicable to raster and matrix data.","1   ScopeThis Technical Report reviews the manner in which raster and gridded data is currently being handled in the Geomatics community in order to propose how this type of data should be supported by geographic information standards.This Technical Report identifies those aspects of imagery and gridded data that have been standardized or are being standardized in other ISO committees and external standards organizations, and that influence or support the establishment of raster and gridded data standards for geographic information. It also describes the components of those identified ISO and external imagery and gridded data standards that can be harmonized with the ISO 19100 series of geographic information/geomatics standards.A plan is presented for ISO/TC 211 to address imagery and gridded data in an integrated manner, within the ISO 19100 series of geographic information standards.",ISO/TR 19121:2000(en)
18,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.",Geographic information — Metadata — Part 2: Extensions for imagery and gridded data,https://www.iso.org/obp/ui/#!iso:std:39229:en,"IntroductionImagery and gridded data are important information sources and products used within a geospatial environment by geographic information systems. The production of imagery and gridded data follows one or more process chains that begin with remote sensing data, scanned maps, field data collection or other sensing methods and end with the creation of the end data products. The production process needs to be documented in order to maintain quality control over the end products. In addition, metadata about the geometry of the measuring process and the properties of the measuring equipment need to be retained with the raw data in order to support the production process.Within the suite of ISO geographic information standards, ISO 19115 defines the guidelines for describing geographic information and services. While the ISO 19115 metadata model does provide some provisions for imagery and gridded data, the requirements were not fully developed at the time ISO 19115:2003 was drafted. To permit the development of ISO 19115 to proceed, inclusion of metadata definitions for imagery and gridded data was deferred until the framework for these data was more fully specified within the suite of ISO geographic information standards. Additionally, other standards that implement metadata for imagery and gridded data have been surveyed and are described in ISO/TR 19121.The object of this part of ISO 19115 is to provide the additional structure to more extensively describe the derivation of geographic imagery and gridded data. This structure is intended to augment ISO 19115.","1   ScopeThis part of ISO 19115 extends the existing geographic metadata standard by defining the schema required for describing imagery and gridded data. It provides information about the properties of the measuring equipment used to acquire the data, the geometry of the measuring process employed by the equipment, and the production process used to digitize the raw data. This extension deals with metadata needed to describe the derivation of geographic information from raw data, including the properties of the measuring system, and the numerical methods and computational procedures used in the derivation. The metadata required to address coverage data in general is addressed sufficiently in the general part of ISO 19115.",ISO 19115-2:2009(en)
19,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.","Geographic information — Imagery, gridded and coverage data framework",https://www.iso.org/obp/ui/#!iso:std:43041:en,"IntroductionGridded data, including imagery, is a major form of geographic information. Over the past two decades many, largely incompatible, standards have been developed that are widely used for the interchange of geographic imagery and gridded data. These include standards developed by ISO, as well as those developed by other organizations. With so many different imagery and gridded data standards, each standard aimed at different but overlapping information communities, there is a considerable legacy problem. Working with data encoded using different formats is often difficult because all of the necessary information for interworking has not been recorded using some of these standards. It is not possible to develop a new comprehensive standard to replace what exists or to simply endorse one existing standard (or industrial specification) to “solve” the interworking problem, because very large volumes of data exist in the various formats already in use. The Technical Report ISO/TR 19121:2000 identified the existing work on imagery and gridded data that had been ongoing in ISO and external technical organizations. What is required is a structure that allows for the specification of the content in a manner independent of and compatible with the various different encoding standards.The area of imagery, gridded and coverage data is one of the most challenging within the field of geographic information. The data appears to be simple; however, there is significant structural complexity. While most data is organized in simple grids, there are many different traversal methods for grids and structures that support the distribution of attributes over a space. Sensor information and associated georeferencing are an important aspect of imagery, gridded and coverage geographic information.This Technical Specification endeavours to address the harmonization of the broad legacy of existing imagery and gridded data. The approach specified is not to build a very flexible standard that encompasses everything with a broad array of options, since that does not create compatibility. One can be fooled into thinking things are standardized, because two data sets use incompatible subsets of the same set of general standards. All that would be accomplished would be to give an ISO label to the existing diversity and incompatibility. Compatibility is required for the underlying structure and primary elements of information content, regardless of how that information content is expressed. The purpose of this Technical Specification is to provide a framework within which interworking can occur. The approach used is to define a set of a few common information content structures for geographic imagery, gridded data and certain types of coverage data, which can be expressed using different encoding mechanisms and different interchange standards. The compatibility results from the common underlying content models that are expressed as a generic set of UML patterns for application schemas.This Technical Specification recognizes that there are many overlapping imagery and gridded data specifications in wide use that differ significantly in how the information content is structured for encoding and in what choices of information form the content model. Different types of encoding may be appropriate in different situations. However, differences in content are difficult to reconcile. The existing different encoding standards do not necessarily conflict because they represent different ways of providing the same information in different contexts. Differences in content are also permitted for different situations, but the content definition must be the same in similar situations for interchange to be achieved without loss of information.Most of the existing specifications for imagery and gridded data used in industry specify how content is to be expressed, rather than the content itself. They relate content to encoding, encapsulation and transfer of data. Those content descriptions that do appear to vary from one specification to another may not be in conflict or incompatible but reflect different real world situations that require different treatments.This Technical Specification combines a number of well-defined content structures in accordance with ISO 19123, the International Standard for coverage geometry and functions together with metadata, spatial referencing and other aspects of imagery, gridded and coverage data into a framework. This will foster a convergence at the content model level for existing imagery, gridded and coverage data while allowing for backward compatibility with the identified suite of existing standards.","1   ScopeThis Technical Specification defines the framework for imagery, gridded and coverage data. This framework defines a content model for the content type imagery and for other specific content types that can be represented as coverage data. These content models are represented as a set of generic UML patterns for application schemas.",ISO/TS 19129:2009(en)
20,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.",Space data and information transfer systems — Digital motion imagery,https://www.iso.org/obp/ui/#!iso:std:69837:en,,"1   INTRODUCTION1.1   PURPOSE AND SCOPEThe purpose of this document is to provide a common reference and framework of standards for digital motion video and imagery, and to provide recommendations for utilization of international standards for sharing or distributing motion video and imagery between spacecraft elements and ground systems.The scope of this document includes traditional real-time streaming video and television, including human and robotic spacecraft-to-spacecraft and spacecraft-to-ground systems, as well as video recorded and distributed later, either as a real-time stream or as a file transfer. In this context, real-time streaming includes all modes where video is sent from a spacecraft in a continuous stream and is intended for immediate use when received, regardless of the latency of the transmission path. Other specialized motion imagery applications, such as high-speed scientific motion imagery and multi-spectral motion imagery, are not addressed in this document. However, if a specialized imagery camera system has a requirement to interface to spacecraft systems in a video mode, it would be required to match these interfaces.Ground-systems-to-ground-systems video distribution is obviously a key component of the entire video system. However, this is not the primary focus of this document. Currently, there are significant differences in the ways mission video products are exchanged between the various space agencies on the ground. This is the result of differences in network topologies between space agencies, and agreements for video sharing. Those differences preclude there being a standard methodology for delivering video imagery between agencies. Prior to the commencement of video transmission between space agencies, system design reviews and performance testing should be done between the ground systems in use to assure operability when video imagery comes from spacecraft.1.2   APPLICABILITYThis document is a CCSDS Recommended Standard. It is intended for all missions that produce, consume, or transcode video imagery from low-bandwidth video such as web streaming through high-bandwidth video such as high-definition television imagery.1.3   NOMENCLATURE1.3.1   NORMATIVE TEXTThe following conventions apply for the normative specifications in this Recommended Standard:a) the words ‘shall’ and ‘must’ imply a binding and verifiable specification;b) the word ‘should’ implies an optional, but desirable, specification;c) the word ‘may’ implies an optional specification;d) the words ‘is’, ‘are’, and ‘will’ imply statements of fact.NOTE — These conventions do not imply constraints on diction in text that is clearly informative in nature.1.3.2   INFORMATIVE TEXTIn the normative sections of this document, informative text is set off from the normative specifications either in notes or under one of the following subsection headings:— Overview;— Background;— Rationale;— Discussion.1.4   REFERENCESThe following publications contain provisions which, through reference in this text, constitute provisions of this document. At the time of publication, the editions indicated were valid. All publications are subject to revision, and users of this document are encouraged to investigate the possibility of applying the most recent editions of the publications indicated below. The CCSDS Secretariat maintains a register of currently valid CCSDS publications.[1]Studio Encoding Parameters of Digital Television for Standard 4:3 and Wide Screen 16:9 Aspect Ratios. ITU-R BT.601-7. Geneva: ITU, 2011.[2]Television—SDTV Digital Signal/Data—Serial Digital Interface. SMPTE ST 259:2008. White Plains, New York: SMPTE, 2008.[3]Digital Interfaces for HDTV Studio Signals. ITU-R BT.1120-8. Geneva: ITU, 2012.[4]1.5 Gb/s Signal/Data Serial Interface. SMPTE ST 292-1:2012. White Plains, New York: SMPTE, 2012.[5]High-Definition Multimedia Interface Specification. Version 1.4. Sunnyvale, California: HDMI Licensing, LLC, 2009.[6]Electrical Characteristics of Low Voltage Differential Signaling (LVDS) Interface Circuits. Revision A. TIA/EIA-644-A. Arlington, Virginia: TIA, February 2001.[7]Serial Digital Interface-Based Transport Interface for Compressed Television Signals in Networked Television Production Based on Recommendation ITU-R BT.1120. ITU-R BT.1577. Geneva: ITU, 2002.[8]Television—Serial Data Transport Interface (SDTI). SMPTE ST 305:2005. White Plains, New York: SMPTE, 2005.[9]Teletext Systems. ITU-R BT.653-3. Geneva: ITU, 1998.[10]Television—Time and Control Code. SMPTE ST 12-1:2008. White Plains, New York: SMPTE, 2008.[11]Television—Transmission of Time Code in the Ancillary Data Space. SMPTE ST 12-2:2008. White Plains, New York: SMPTE, 2008.[12]Ancillary Data Packet and Space Formatting. SMPTE ST 291:2011. White Plains, New York: SMPTE, 2011.[13]Vertical Ancillary Data Mapping of Caption Data and Other Related Data. SMPTE ST 334-1:2007. White Plains, New York: SMPTE, 2007.[14]Metadata Element Dictionary Structure. SMPTE ST 335:2012. White Plains, New York: SMPTE, 2012.[15]Metadata Dictionary Registry of Metadata Element Descriptions. SMPTE RP 210.10:2007. White Plains, New York: SMPTE, 2007.[16]Ultra High Definition Television—Mapping into Single-link or Multi-link 10 Gb/s Serial Signal/Data Interface. SMPTE ST 2036-3:2010. White Plains, New York: SMPTE, 2010.[17]1280×720, 16:9 Progressively-Captured Image Format for Production and International Programme Exchange in the 60 Hz Environment. ITU-R BT.1543. Geneva: ITU, 2001.[18]1280 x 720 Progressive Image 4:2:2 and 4:4:4 Sample Structure—Analog and Digital Representation and Analog Interface. SMPTE ST 296:2012. White Plains, New York: SMPTE, 2012.[19]Parameter Values for the HDTV Standards for Production and International Programme Exchange. ITU-R BT.709-5. Geneva: ITU, 2002.[20]Television—1920 x 1080 Image Sample Structure, Digital Representation and Digital Timing Reference Sequences for Multiple Picture Rates. SMPTE ST 274:2008. White Plains, New York: SMPTE, 2008.[21]Dual Link 1.5 Gb/s Digital Interface for 1920 x 1080 and 2048 x 1080 Picture Frames. SMPTE ST 372:2011. White Plains, New York: SMPTE, 2011.[22]Television—3 Gb/s Signal/Data Serial Interface. SMPTE ST 424:2006. White Plains, New York: SMPTE, 2006.[23]Ultra High Definition Television—Image Parameter Values for Program Production. SMPTE ST 2036-1:2009. White Plains, New York: SMPTE, 2009.[24]Ultra High Definition Television—Audio Characteristics and Audio Channel Mapping for Program Production. SMPTE ST 2036-2-2008. White Plains, New York: SMPTE, 2008.[25]2048 × 1080 and 4096 × 2160 Digital Cinematography Production Image Formats FS/709. SMPTE ST 2048-1:2011. White Plains, New York: SMPTE, 2011.[26]2048 × 1080 Digital Cinematography Production Image FS/709 Formatting for Serial Digital Interface. SMPTE ST 2048-2:2011. White Plains, New York: SMPTE, 2011.[27]Parameter Values for Ultra-High Definition Television Systems for Production and International Programme Exchange. ITU-R BT.2020-1. Geneva: ITU, 2014.[28]Information Technology—Coding of Audio-Visual Objects-Part 10: Advanced Video Coding. 8th ed. International Standard, ISO/IEC 14496-10:2014. Geneva: ISO, 2014.[29]Advanced Video Coding for Generic Audiovisual Services. ITU-T H.264. Geneva: ITU, 2012.[30]Data Services in Digital Television Broadcasting. ITU-R BT.1301-1. Geneva: ITU, 2011.[31]Interface for Digital Component Video Signals in 525-Line and 625-Line Television Systems Operating at the 4:2:2 Level of Recommendation ITU-R BT.601. ITU-R BT.656-5. Geneva: ITU, 2007.[32]Information Technology—JPEG 2000 Image Coding System: Motion JPEG 2000. 2nd ed. International Standard, ISO/IEC 15444-3:2007. Geneva: ISO, 2007.[33]Information Technology—Generic Coding of Moving Pictures and Associated Audio Information—Part 7: Advanced Audio Coding (AAC). 4th ed. International Standard, ISO/IEC 13818-7:2006. Geneva: ISO, 2006.[34]Digital Audio Interface—Part 3: Consumer Applications. Edition 3.1 (2009-12-10). IEC 60958-3:2006+AMD1:2009 CSV. Geneva: IEC, 2009.[35]IP over CCSDS Space Links. Issue 1. Recommendation for Space Data System Standards (Blue Book), CCSDS 702.1-B-1. Washington, D.C.: CCSDS, September 2012.[36]J. Postel. User Datagram Protocol. STD 6. Reston, Virginia: ISOC, August 1980.[37]CCSDS File Delivery Protocol (CFDP). Issue 4. Recommendation for Space Data System Standards (Blue Book), CCSDS 727.0-B-4. Washington, D.C.: CCSDS, January 2007.",ISO 21077:2016(en)
21,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.",Geographic information — Content components and encoding rules for imagery and gridded data — Part 1: Content model,https://www.iso.org/obp/ui/#!iso:std:32581:en,"IntroductionGeographic imagery and gridded thematic data are widely used in the geospatial community and related fields.A preliminary work item on imagery and gridded data components, carried out by ISO/TC 211 in 1999 to 2000, provides a summary of the conceptual classification of gridded data based on spatial and attribute properties and identifies five basic components of imagery and gridded data (ISO/TC 211 N 1017). ISO/TS 19101-2, ISO 19123 and ISO/TS 19129 specify domains and ranges of imagery, grids and coverages, and their associated relationships. ISO/TS 19129 breaks down the metadata into discovery, structural, acquisition and quality metadata. However, there are no detailed descriptions on each category and no clear associations with metadata defined in ISO 19115:2003, ISO 19115-2, ISO/TS 19130 and ISO/TS 19130-2.Imagery is acquired by remote sensors directly or derived from source imagery. Value-added image processing can be used to derive physical properties of a remote object from images (ISO/TS 19101-2). Besides the derived images, imagery can also be integrated with other data sources to produce new gridded coverage data for a specific theme, called thematic data, which is widely used in various applications. However, the characteristics of thematic data are not covered by the existing International Standards and Technical Specifications noted above.ISO/TS 19130 identifies the type of remote sensors by the measurand of the sensor, e.g. optical radiation, microwave energy and SONAR (acoustic) energy. Images acquired by optical sensors have different appearances and characteristics compared with those by a microwave sensor, e.g. SAR data.The framework defined in ISO/TS 19129 describes imagery, gridded and coverage data at multiple levels, including an abstract level, a content model level and an encoding level. The first two levels combine a number of well-defined content structures in accordance with ISO 19123 and define the contents of continuous quadrilateral gridded coverages with grids of both constant and variable cell sizes. However, the content model level does not specify the necessary metadata for common understanding when integrating datasets encoded in different formats. At the encoding level, ISO/TS 19129 does not provide the explicit encoding rules for mapping content model to machine-independent encoding structure, which is crucial for the mapping and translation of images in different formats without losing information.Based on the frameworks defined in ISO/TS 19101-2 and ISO 19123, this Technical Specification specifies the categories of imagery and gridded data and establishes a corresponding hierarchical content model. Categories of imagery and gridded data are defined based on thematic and spatial attributes and sensor types. The content model is then defined to describe the required content components of each category, including the spatial and attribute structures and the critical metadata entries as well. These metadata entries are specified as the minimum required metadata information for the purpose of common understanding. Traditionally, remote sensing data products generally have a header part and a data part. This Technical Specification describes the minimum content requirements for the header part.For ease of implementation, this Technical Specification defines encoding rules to map the content models to XML-based encodings, following the general encoding rules defined in ISO 19118 and the encoding rules for UML-to-GML application schema defined in ISO 19136:2007, Annex E. Since GMLCOV schema (OGC 09-146r2) is optimized for handling coverages, the coverage component of the schema can be based on GMLCOV.An increasingly large volume of image and gridded data, both natural and synthetic, is being produced because more remote sensors are becoming available. These data are encoded in diverse formats, such as GeoTIFF, BIIF, HDF-EOS, JPEG 2000, NetCDF and others as described in ISO/TR 19121. These encoding formats follow different data models, preventing them from being interoperable. In order to encode the contents defined in this Technical Specification into these data formats, ISO 19163 has been split into multiple parts with this Technical Specification defining the content components and general encoding rules and the subsequent parts defining the binding between the contents and individual physical data formats.","1   ScopeThis Technical Specification classifies imagery and regularly spaced gridded thematic data into types based on attribute property, sensor type and spatial property, and defines an encoding-neutral content model for the required components for each type of data. It also specifies logical data structures and the rules for encoding the content components in the structures.The binding between the content and a specific encoding format will be defined in the subsequent parts of ISO 19163.This Technical Specification does not address LiDAR, SONAR data and ungeoreferenced gridded data.The logical data structures and the rules for encoding the content components will be addressed in the subsequent parts of ISO 19163.",ISO/TS 19163-1:2016(en)
22,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.",Geographic information — Calibration and validation of remote sensing imagery sensors and data — Part 1: Optical sensors,https://www.iso.org/obp/ui/#!iso:std:60080:en,"IntroductionImaging sensors are one of the major data sources for geographic information. Typical spatial outcomes of the production process are vector maps, Digital Elevation Models, and three-dimensional city models. There are typically two streams of spectral data analysis, that is, the statistical method, which includes image segmentation, and the physics-based method, which relies on characterization of specific spectral absorption features.In each of the cases, the quality of the end products fully depends on the quality of the measuring instruments that has originally sensed the data. The quality of measuring instruments is determined and documented by calibration.A calibration is often a costly and time-consuming process. Therefore, a number of different strategies are used that combine longer time intervals between subsequent calibrations with simplified intermediate calibration procedures that bridge the time gap and still guarantee a traceable level of quality. Those intermediate calibrations are called validations in this part of ISO 19159.This part of ISO 19159 standardizes the calibration of remote sensing imagery sensors and the validation of the calibration information and procedures. It does not address the validation of the data and the derived products.Many types of imagery sensors exist for remote sensing tasks. Apart from the different technologies, the need for a standardization of the various sensor types has different levels of priority. In order to meet those requirements, ISO 19159 has been split into more than one part. Part 1 covers optical sensors, i.e. airborne photogrammetric cameras and spaceborne optical sensors. Part 2 is intended to cover laser scanning, also known as LIDAR (Light detection and ranging).Parts 3 and 4 are planned to cover RADAR (radio detection and ranging) with the subtopics SAR (synthetic aperture radar) and InSAR (interferometric SAR) as well as SONAR (sound detection and ranging) that is applied in hydrography.","1   ScopeThis part of ISO 19159 defines the calibration and validation of airborne and spaceborne remote sensing imagery sensors.The term ""calibration"" refers to geometry, radiometry, and spectral, and includes the instrument calibration in a laboratory as well as in situ calibration methods.The validation methods address validation of the calibration information.This part of ISO 19159 also addresses the associated metadata related to calibration and validation which have not been defined in other geographic information International Standards.The specified sensors include optical sensors of the frame camera and line camera types (2D CCD scanners).",ISO/TS 19159-1:2014(en)
23,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.",Geographic information — Metadata — XML schema implementation — Part 2: Extensions for imagery and gridded data,https://www.iso.org/obp/ui/#!iso:std:57104:en,"IntroductionThe importance of metadata describing digital geographic data is explained in detail in the text of ISO 19115 and in the text of ISO 19115-2 (for imagery and gridded data). ISO 19115 and ISO 19115-2 are abstract in that they provide a worldwide view of metadata relative to geographic information, but no encoding.Since ISO 19115 does not provide any encoding, implementation of geographic information metadata could vary based on the interpretation of metadata producers. In an attempt to facilitate the standardization of implementations, ISO/TS 19139 provides a definitive, rule-based encoding for carrying out ISO 19115. ISO/TS 19139 provides Extensible Markup Language (XML) schemas that are meant to enhance interoperability by providing a common specification for describing, validating and exchanging metadata about geographic datasets, dataset series, individual geographic features, feature attributes, feature types, feature properties, etc.This Technical Specification utilizes ISO/TS 19139 specification and extends it to define XML Schema implementation for ISO 19115-2. It provides a definitive, rule-based encoding for carrying out ISO 19115-2.",1   ScopeThis Technical Specification defines Geographic Metadata for imagery and gridded data (gmi) encoding. This is an XML Schema implementation derived from ISO 19115-2.,ISO/TS 19139-2:2012(en)
24,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.",Geographic information — Calibration and validation of remote sensing imagery sensors and data — Part 2: Lidar,https://www.iso.org/obp/ui/#!iso:std:64768:en,"IntroductionImaging sensors are one of the major data sources for geographic information. The image data capture spatial and spectral measurements are applied for numerous applications ranging from road/town planning to geological mapping. Typical spatial outcomes of the production process are vector maps, Digital Elevation Models, and 3-dimensional city models. There are typically two streams of spectral analysis data, i.e. the statistical method, which includes image segmentation and the physics-based method which relies on characterisation of specific spectral absorption features.In each of the cases the quality of the end products fully depends on the quality of the measuring instruments that has originally sensed the data. The quality of measuring instruments is determined and documented by calibration.A calibration is often a costly and time consuming process. Therefore, a number of different strategies are in place that combine longer time intervals between subsequent calibrations with simplified intermediate calibration procedures that bridge the time gap and still guarantee a traceable level of quality. Those intermediate calibrations are called validations in this part of ISO/TS 19159.The ISO 19159 series standardizes the calibration of remote sensing imagery sensors and the validation of the calibration information and procedures. It does not address the validation of the data and the derived products.Many types of imagery sensors exist for remote sensing tasks. Apart from the different technologies the need for a standardization of the various sensor types has different levels of priority. In order to meet those requirements, the ISO 19159 series has been split into more than one part.This part of ISO/TS 19159 covers the airborne land lidar sensor (light detection and ranging). It includes the data capture and the calibration. The result of a lidar data capture is a lidar cloud according to the ISO 19156:2011. The bathymetric lidar is not included in the ISO 19159 series.ISO 19159-3 and ISO 19159-4 are planned to cover RADAR (Radio detection and ranging) with the subtopics SAR (Synthetic Aperture RADAR) and InSAR (Interferometric SAR) as well as SONAR (Sound detection and ranging) that is applied in hydrography.","1   ScopeThis part of ISO/TS 19159 defines the data capture method, the relationships between the coordinate reference systems and their parameters, as well as the calibration of airborne lidar (light detection and ranging) sensors.This part of ISO/TS 19159 also standardizes the service metadata for the data capture method, the relationships between the coordinate reference systems and their parameters and the calibration procedures of airborne lidar systems as well as the associated data types and code lists that have not been defined in other ISO geographic information international standards.",ISO/TS 19159-2:2016(en)
25,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.","Geographic information — Imagery sensor models for geopositioning — Part 2: SAR, InSAR, lidar and sonar",https://www.iso.org/obp/ui/#!iso:std:56113:en,"IntroductionThe purpose of this Technical Specification is to specify the geolocation information that an imagery data provider shall supply in order for the user to be able to find the earth location of the data using a detailed physical sensor model for Synthetic Aperture Radar (SAR), Light Detection And Ranging (lidar) and Sound Navigation And Ranging (sonar). The intent is to standardize sensor descriptions and specify the minimum geolocation metadata requirements for data providers and geopositioning imagery systems. Observations in this document are the generic meaning of the word; observations are not in the meaning of ISO 19156 observations.Vast amounts of data from imaging systems have been collected, processed and distributed by government mapping and remote sensing agencies and by commercial data vendors. In order for this data to be useful in extraction of geographic information, further processing of the data are needed. Geopositioning, which determines the ground coordinates of an object from image coordinates, is a fundamental processing step. Because of the diversity of sensor types and the lack of a common sensor model standard, data from different producers may contain different parametric information, lack parameters required to describe the sensor that produces the data, or lack ancillary information necessary for geopositioning and analysing the data. Often, a separate software package must be developed to deal with data from each individual sensor or data producer. Standard sensor models and geolocation metadata allow agencies or vendors to develop generalized software products that are applicable to data from multiple data producers or from multiple sensors. With such standards, different producers can describe the geolocation information of their data in the same way, thus promoting interoperability of data between application systems and facilitating data exchange.Part 1 provided a location model and metadata relevant to all sensors. It also included metadata specific to whiskbroom, pushbroom, and frame sensors, and some metadata for Synthetic Aperture Radar (SAR) sensors. In addition, it provided metadata for functional fit geopositioning, whether the function was part of a correspondence model or a true replacement model. It also provided a schema for these metadata elements. Comments on Part 1 stated that metadata needed to be specified for additional sensors. The technology of such sensors has now become sufficiently mature that standardization is now possible. This Technical Specification extends the specification of the set of metadata elements required for geolocation by providing physical sensor models for LIght Detection And Ranging (lidar) and SOund Navigation And Ranging (sonar), and it presents a more detailed set of elements for SAR. This Technical Specification also defines the metadata needed for the aerial triangulation of airborne and spaceborne images. This Technical Specification also provides a schema for all of these metadata elements.","1   ScopeThis Technical Specification supports exploitation of remotely sensed images. It specifies the sensor models and metadata for geopositioning images remotely sensed by Synthetic Aperture Radar (SAR), Interferometric Synthetic Aperture Radar (InSAR), LIght Detection And Ranging (lidar), and SOund Navigation And Ranging (sonar) sensors. The specification also defines the metadata needed for the aerial triangulation of airborne and spaceborne images.This Technical Specification specifies the detailed information that shall be provided for a sensor description of SAR, InSAR, lidar and sonar sensors with the associated physical and geometric information necessary to rigorously construct a Physical Sensor Model. For the case where precise geoposition information is needed, this Technical Specification identifies the mathematical formulae for rigorously constructing Physical Sensor Models that relate two-dimensional image space to three-dimensional ground space and the calculation of the associated propagated error.This Technical Specification does not specify either how users derive geoposition data or the format or content of the data the users generate.",ISO/TS 19130-2:2014(en)
26,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.",Geographic information - Imagery sensor models for geopositioning,https://www.iso.org/obp/ui/#!iso:std:51789:en,"IntroductionThe purpose of this Technical Specification is to specify the geolocation information that an imagery data provider shall supply in order for the user to be able to find the earth location of the data using a Physical Sensor Model, a True Replacement Model or a Correspondence Model. Detailed Physical Sensor Models are defined for passive electro-optical visible/infrared (IR) sensors (frame, pushbroom and whiskbroom) and for an active microwave sensing system (Synthetic Aperture Radar). A set of components from which models for other sensors can be constructed is also provided. Metadata required for geopositioning using a True Replacement Model, a Correspondence Model, or ground control points are also specified. The intent is to standardize sensor descriptions and specify the minimum geolocation metadata requirements for data providers and geopositioning imagery systems.Vast amounts of data from imaging systems are collected, processed and distributed by government mapping and remote sensing agencies and commercial data vendors. In order for this data to be useful in extraction of geographic information, it requires further processing. Geopositioning, which determines the ground coordinates of an object from image coordinates, is a fundamental processing step. Because of the diversity of sensor types and the lack of a common sensor model standard, data from different producers can contain different parametric information, lack parameters required to describe the sensor that produces the data, or lack ancillary information necessary for geopositioning and analysing the data. Consequently, a separate software package often has to be developed to deal with data from each individual sensor or data producer. Standard sensor models and geolocation metadata allow agencies or vendors to develop generalized software products that are applicable to data from multiple data producers or from multiple sensors. With such a standard, different producers can describe the geolocation information of their data in the same way, thus promoting interoperability of data between application systems and facilitating data exchange.This Technical Specification defines the set of metadata elements specified for providing sensor model and other geopositioning data to users. For the case where a Physical Sensor Model is provided, it includes a location model and metadata relevant to all sensors; it also includes metadata specific to whiskbroom, pushbroom, frame, and SAR sensors. It also includes metadata for functional fit geopositioning, where the function is part of a Correspondence Model or a True Replacement Model. This Technical Specification also provides a schema for all of these metadata elements.","1   ScopeThis Technical Specification identifies the information required to determine the relationship between the position of a remotely sensed pixel in image coordinates and its geoposition. It supports exploitation of remotely sensed images. It defines the metadata to be distributed with the image to enable user determination of geographic position from the observations.This Technical Specification specifies several ways in which information in support of geopositioning may be provided.a) It may be provided as a sensor description with the associated physical and geometric information necessary to rigorously construct a Physical Sensor Model. For the case where precise geoposition information is needed, this Technical Specification identifies the mathematical formulae for rigorously constructing Physical Sensor Models that relate two-dimensional image space to three-dimensional ground space and the calculation of the associated propagated errors. This Technical Specification provides detailed information for three types of passive electro-optical/infrared (IR) sensors (frame, pushbroom and whiskbroom) and for an active microwave sensing system [Synthetic Aperture Radar (SAR)]. It provides a framework by which these sensor models can be extended to other sensor types.b) It may be provided as a True Replacement Model, using functions whose coefficients are based on a Physical Sensor Model so that they provide information for precise geopositioning, including the calculation of errors, as precisely as the Physical Sensor Model they replace.c) It may be provided as a Correspondence Model that provides a functional fitting based on observed relationships between the geopositions of a set of ground control points and their image coordinates.d) It may be provided as a set of ground control points that can be used to develop a Correspondence Model or to refine a Physical Sensor Model or True Replacement Model.This Technical Specification does not specify either how users derive geoposition data or the format or content of the data the users generate.",ISO/TS 19130:2010(en)
27,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.",Geographic information — Web map server interface,https://www.iso.org/obp/ui/#!iso:std:32546:en,"IntroductionA Web Map Service (WMS) produces maps of spatially referenced data dynamically from geographic information. This International Standard defines a “map” to be a portrayal of geographic information as a digital image file suitable for display on a computer screen. A map is not the data itself. WMS-produced maps are generally rendered in a pictorial format such as PNG, GIF or JPEG, or occasionally as vector-based graphical elements in Scalable Vector Graphics (SVG) or Web Computer Graphics Metafile (WebCGM) formats.This International Standard defines three operations: one returns service-level metadata; another returns a map whose geographic and dimensional parameters are well-defined; and an optional third operation returns information about particular features shown on a map. Web Map Service operations can be invoked using a standard web browser by submitting requests in the form of Uniform Resource Locators (URLs). The content of such URLs depends on which operation is requested. In particular, when requesting a map the URL indicates what information is to be shown on the map, what portion of the Earth is to be mapped, the desired coordinate reference system, and the output image width and height. When two or more maps are produced with the same geographic parameters and output size, the results can be accurately overlaid to produce a composite map. The use of image formats that support transparent backgrounds (e.g. GIF or PNG) allows underlying maps to be visible. Furthermore, individual maps can be requested from different servers. The Web Map Service thus enables the creation of a network of distributed map servers from which clients can build customized maps. Illustrative examples of map request URLs and their resulting maps are shown in Annex G.This International Standard applies to a Web Map Service instance that publishes its ability to produce maps rather than its ability to access specific data holdings. A basic WMS classifies its geographic information holdings into “Layers” and offers a finite number of predefined “Styles” in which to display those layers. This International Standard supports only named Layers and Styles, and does not include a mechanism for user-defined symbolization of feature data.NOTE The Open Geospatial Consortium (OGC) Styled Layer Descriptor (SLD) specification [6] defines a mechanism for user-defined symbolization of feature data instead of named Layers and Styles. In brief, an SLD-enabled WMS retrieves feature data from a Web Feature Service [7] and applies explicit styling information provided by the user in order to render a map.","1   ScopeThis International Standard specifies the behaviour of a service that produces spatially referenced maps dynamically from geographic information. It specifies operations to retrieve a description of the maps offered by a server to retrieve a map, and to query a server about features displayed on a map. This International Standard is applicable to pictorial renderings of maps in a graphical format; it is not applicable to retrieval of actual feature data or coverage data values.",ISO 19128:2005(en)
28,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.",Geographic information — Reference model — Part 2: Imagery,https://www.iso.org/obp/ui/#!iso:std:39983:en,"IntroductionThis Technical Specification provides a reference model for processing of geographic imagery which is frequently done in open distributed manners. The motivating themes addressed in this reference model are given below.In terms of volume, imagery is the dominant form of geographic information.— Stored geographic imagery volume will grow to the order of an exabyte.— National imagery archives are multiple petabytes in size; ingesting a terabyte per day.— Individual application data centers are archiving hundreds of terabytes of imagery.— Tens of thousands of datasets have been catalogued but are not yet online.Large volumes of geographic imagery will not be portrayed directly by humans. Human attention is the scarce resource, and is insufficient to view petabytes of data. Semantic processing will be required: for example, automatic detection of features; data mining based on geographic concepts.Information technology allows the sharing of geographic information products through processing of geographic imagery. Standards are needed to increase creation of products. A number of existing standards are used for the exchange of geographic imagery.Examples of technical, legal, and administrative hurdles to moving imagery online include— technical issues of accessibility — geocoding, geographic access standards,— maintenance of intellectual property rights,— maintenance of individual privacy rights as resolution increases, and— technical issues of compatibility requiring standards.Governments have been the predominant suppliers of remotely sensed data in the past. This is changing with the commercialization of remotely sensed data acquisition. Geographic imagery is a key input to decision support for policy makers.The ultimate challenge is to enable the geographic imagery collected from different sources to become an integrated digital representation of the Earth widely accessible for humanity’s critical decisions.Currently a large number of standards exist that describe imagery data. The processing of imagery across multiple organizations and information technologies (IT) is hampered by the lack of a common abstract architecture. The establishment of a common framework will foster convergence at the framework level. In the future, multiple implementation standards are needed for data format and service interoperability to carry out the architecture defined in this Technical Specification.The objective of this Technical Specification is the coordinated development of standards that allow the benefits of distributed geographic image processing to be realized in an environment of heterogeneous IT resources and multiple organizational domains. An underlying assumption is that uncoordinated standardization activities made without a plan cannot be united under the necessary framework.This Technical Specification provides a reference model for the processing of geographic imagery which is frequently done in open distributed manners. The basis for defining an information system in this Technical Specification is the Reference Model for Open Distributed Processing (RM-ODP) [78]. A brief description of RM-ODP can be referenced in Annex B. The basis for defining geographic information in this specification is the ISO 19100 family of standards.The RM-ODP[78] viewpoints are used in the following fashion:— Typical users and their business activities, and policies to carry out those activities, are addressed in the Enterprise Viewpoint.— Data structures and the progressive addition of value to the resulting products are found in the schemas of the Information Viewpoint.— Individual processing services and the chaining of services are addressed in the Computational Viewpoint.Approaches to deploy the components of the Information and Computational viewpoints to distributed physical locations are addressed in the Engineering Viewpoint.","1   ScopeThis part of ISO 19101 defines a reference model for standardization in the field of geographic imagery processing. This reference model identifies the scope of the standardization activity being undertaken and the context in which it takes place. The reference model includes gridded data with an emphasis on imagery. Although structured in the context of information technology and information technology standards, this Technical Specification is independent of any application development method or technology implementation approach.",ISO/TS 19101-2:2008(en)
29,"Currently, the Department of Homeland Security (DHS) and the United States Coast Guard (USCG) do not have sufficient access to surveillance sensors that provide Maritime Domain Awareness (MDA) to support USCG mission needs, nor provide coverage of remote regions. In response, the DHS Science and Technology Directorate (S&T) created The Port and Surveillance Program, with Advanced Sensor Analytics Project (ASAP) a project within the Program, which is a technology demonstration that taps the burgeoning commercial space market and government investments in fusion and analytics, to provide a capability that will enhance MDA across all of the USCG mission space. The ASAP project consists of 3 subprojects: Polar Scout, Data Analytics, and Arctic Communications. Polar Scout: Polar Scout involves the identification and development of space-based sensor feeds. The focus is identifying and leveraging existing sensors (both commercial and government) and procuring new sensors to fill MDA gaps. Polar Scout consists of an integrated system of United States Government (USG)-owned sensors (6U CubeSats) and commercial sensors (HE360 CubeSats). These sensors will be tasked to collect maritime data in support of DHS missions. The collected data is fused with weather, Automatic Identification System (AIS), air-tracks and other data to provide the DHS and the USCG with maritime situational awareness. Based on the fused information, the Polar Scout system could then request or gather information (e.g. imagery data) from other established data sources. The information received and fused within Polar Scout is then provided as input to the Data Analytics to perform additional analysis. Data Analytics: Data Analytics provides sensor exploitation enhancement of tracking, tipping, and queuing of MDA relevant objects, to support the Coast Guard and other DHS missions. The intent is to ensure that analytics performed will provide S&T with a deeper understanding of commercial space-based data sets and corresponding data correlation, predictive and prescriptive analytics.",Information technology — Computer graphics and image processing — Image Processing and Interchange (IPI) — Functional specification — Part 5: Basic Image Interchange Format (BIIF),https://www.iso.org/obp/ui/#!iso:std:27853:en,,"1 ScopeThis part of ISO/IEC 12087 establishes the specification of the Basic Image Interchange Format (BIIF) part of the standard. BIIF is a standard developed to provide a foundation for interoperability in the interchange of imagery and imagery-related data among applications. This part of ISO/IEC 12087 provides a detailed description of the overall structure of the format, as well as specification of the valid data and format for all fields defined with BIIF. Annex C contains a model profile in tables to assist in profile development.As part of the ISO/IEC 12087 family of image processing and interchange standards, BIIF conforms to the architectural and data object specifications of ISO/IEC 12087-1, the Common Architecture for Imaging. BIIF supports a profiling scheme that is a combination of the approaches taken for ISO/IEC 12087-2 (PIKS), ISO/IEC 10918 (JPEG), ISO/IEC 8632 (CGM), and ISO/IEC 9973 (The Procedures for Registration of Graphical Items). It is intended that profiles of the BIIF will be established as an International Standardised Profile (ISP) through the normal ISO processes (ISO/IEC TR 10000).The scope and field of application of this part of ISO/IEC 12087 includes the capability to perpetuate a proven interchange capability in support of commercial and government imagery, Programmer’s Imaging Kernel System Data, and other imagery technology domains in that priority order.This part of ISO/IEC 12087 provides a data format container for image, symbol, and text, along with a mechanism for including image-related support data.This part of ISO/IEC 12087 satisfies the following requirements:— Provides a means whereby diverse applications can share imagery and associated information.— Allows an application to exchange comprehensive information to users with diverse needs or capabilities, allowing each user to select only those data items that correspond to their needs and capabilities.— Minimizes preprocessing and postprocessing of data.— Minimizes formatting overhead, particularly for those applications exchanging only a small amount of data and for bandwidth-limited systems.— Provides a mechanism (Transportable File Structure, TFS) to interchange PIKS image and image-related objects— Provides extensibility to accommodate future data, including objects.When the extensibility of this part of ISO/IEC 12087, or the inherent constraints of the structured format of BIIF, do not meet the needs of a more complex application, the concepts and features of 12087-3 (IlF) should be considered as a more appropriate method of image interchange. For example, the ability to support complex combinations of heterogeneous pixel types, self defining pixel structures, or abstract structures can be done with IIF.",ISO/IEC 12087-5:1998(en)
30,"Polar Scout: The successful execution of this Polar Scout project will result in a Arctic MDA demonstrations that demonstrates SAR missions employing CubeSats to detect and geolocate 406 MHz emergency beacons. The Data Analytics project will provide predictive analytics with the ability to process information from various data sources including space-based, airborne, and ground-based. Additionally, ASAP is an open architecture (e.g., open APIs and data models) that will enable rapid integration of these solution components into Components existing systems across varied DHS missions by leveraging the Integrated Maritime Domain Enterprise (IMDE/CSS) and DOD/J39's Project DataHub.","Geographic information — Imagery sensor models for geopositioning — Part 2: SAR, InSAR, lidar and sonar",https://www.iso.org/obp/ui/#!iso:std:56113:en,"IntroductionThe purpose of this Technical Specification is to specify the geolocation information that an imagery data provider shall supply in order for the user to be able to find the earth location of the data using a detailed physical sensor model for Synthetic Aperture Radar (SAR), Light Detection And Ranging (lidar) and Sound Navigation And Ranging (sonar). The intent is to standardize sensor descriptions and specify the minimum geolocation metadata requirements for data providers and geopositioning imagery systems. Observations in this document are the generic meaning of the word; observations are not in the meaning of ISO 19156 observations.Vast amounts of data from imaging systems have been collected, processed and distributed by government mapping and remote sensing agencies and by commercial data vendors. In order for this data to be useful in extraction of geographic information, further processing of the data are needed. Geopositioning, which determines the ground coordinates of an object from image coordinates, is a fundamental processing step. Because of the diversity of sensor types and the lack of a common sensor model standard, data from different producers may contain different parametric information, lack parameters required to describe the sensor that produces the data, or lack ancillary information necessary for geopositioning and analysing the data. Often, a separate software package must be developed to deal with data from each individual sensor or data producer. Standard sensor models and geolocation metadata allow agencies or vendors to develop generalized software products that are applicable to data from multiple data producers or from multiple sensors. With such standards, different producers can describe the geolocation information of their data in the same way, thus promoting interoperability of data between application systems and facilitating data exchange.Part 1 provided a location model and metadata relevant to all sensors. It also included metadata specific to whiskbroom, pushbroom, and frame sensors, and some metadata for Synthetic Aperture Radar (SAR) sensors. In addition, it provided metadata for functional fit geopositioning, whether the function was part of a correspondence model or a true replacement model. It also provided a schema for these metadata elements. Comments on Part 1 stated that metadata needed to be specified for additional sensors. The technology of such sensors has now become sufficiently mature that standardization is now possible. This Technical Specification extends the specification of the set of metadata elements required for geolocation by providing physical sensor models for LIght Detection And Ranging (lidar) and SOund Navigation And Ranging (sonar), and it presents a more detailed set of elements for SAR. This Technical Specification also defines the metadata needed for the aerial triangulation of airborne and spaceborne images. This Technical Specification also provides a schema for all of these metadata elements.","1   ScopeThis Technical Specification supports exploitation of remotely sensed images. It specifies the sensor models and metadata for geopositioning images remotely sensed by Synthetic Aperture Radar (SAR), Interferometric Synthetic Aperture Radar (InSAR), LIght Detection And Ranging (lidar), and SOund Navigation And Ranging (sonar) sensors. The specification also defines the metadata needed for the aerial triangulation of airborne and spaceborne images.This Technical Specification specifies the detailed information that shall be provided for a sensor description of SAR, InSAR, lidar and sonar sensors with the associated physical and geometric information necessary to rigorously construct a Physical Sensor Model. For the case where precise geoposition information is needed, this Technical Specification identifies the mathematical formulae for rigorously constructing Physical Sensor Models that relate two-dimensional image space to three-dimensional ground space and the calculation of the associated propagated error.This Technical Specification does not specify either how users derive geoposition data or the format or content of the data the users generate.",ISO/TS 19130-2:2014(en)
31,"Polar Scout: The successful execution of this Polar Scout project will result in a Arctic MDA demonstrations that demonstrates SAR missions employing CubeSats to detect and geolocate 406 MHz emergency beacons. The Data Analytics project will provide predictive analytics with the ability to process information from various data sources including space-based, airborne, and ground-based. Additionally, ASAP is an open architecture (e.g., open APIs and data models) that will enable rapid integration of these solution components into Components existing systems across varied DHS missions by leveraging the Integrated Maritime Domain Enterprise (IMDE/CSS) and DOD/J39's Project DataHub.",Geographic information - Imagery sensor models for geopositioning,https://www.iso.org/obp/ui/#!iso:std:51789:en,"IntroductionThe purpose of this Technical Specification is to specify the geolocation information that an imagery data provider shall supply in order for the user to be able to find the earth location of the data using a Physical Sensor Model, a True Replacement Model or a Correspondence Model. Detailed Physical Sensor Models are defined for passive electro-optical visible/infrared (IR) sensors (frame, pushbroom and whiskbroom) and for an active microwave sensing system (Synthetic Aperture Radar). A set of components from which models for other sensors can be constructed is also provided. Metadata required for geopositioning using a True Replacement Model, a Correspondence Model, or ground control points are also specified. The intent is to standardize sensor descriptions and specify the minimum geolocation metadata requirements for data providers and geopositioning imagery systems.Vast amounts of data from imaging systems are collected, processed and distributed by government mapping and remote sensing agencies and commercial data vendors. In order for this data to be useful in extraction of geographic information, it requires further processing. Geopositioning, which determines the ground coordinates of an object from image coordinates, is a fundamental processing step. Because of the diversity of sensor types and the lack of a common sensor model standard, data from different producers can contain different parametric information, lack parameters required to describe the sensor that produces the data, or lack ancillary information necessary for geopositioning and analysing the data. Consequently, a separate software package often has to be developed to deal with data from each individual sensor or data producer. Standard sensor models and geolocation metadata allow agencies or vendors to develop generalized software products that are applicable to data from multiple data producers or from multiple sensors. With such a standard, different producers can describe the geolocation information of their data in the same way, thus promoting interoperability of data between application systems and facilitating data exchange.This Technical Specification defines the set of metadata elements specified for providing sensor model and other geopositioning data to users. For the case where a Physical Sensor Model is provided, it includes a location model and metadata relevant to all sensors; it also includes metadata specific to whiskbroom, pushbroom, frame, and SAR sensors. It also includes metadata for functional fit geopositioning, where the function is part of a Correspondence Model or a True Replacement Model. This Technical Specification also provides a schema for all of these metadata elements.","1   ScopeThis Technical Specification identifies the information required to determine the relationship between the position of a remotely sensed pixel in image coordinates and its geoposition. It supports exploitation of remotely sensed images. It defines the metadata to be distributed with the image to enable user determination of geographic position from the observations.This Technical Specification specifies several ways in which information in support of geopositioning may be provided.a) It may be provided as a sensor description with the associated physical and geometric information necessary to rigorously construct a Physical Sensor Model. For the case where precise geoposition information is needed, this Technical Specification identifies the mathematical formulae for rigorously constructing Physical Sensor Models that relate two-dimensional image space to three-dimensional ground space and the calculation of the associated propagated errors. This Technical Specification provides detailed information for three types of passive electro-optical/infrared (IR) sensors (frame, pushbroom and whiskbroom) and for an active microwave sensing system [Synthetic Aperture Radar (SAR)]. It provides a framework by which these sensor models can be extended to other sensor types.b) It may be provided as a True Replacement Model, using functions whose coefficients are based on a Physical Sensor Model so that they provide information for precise geopositioning, including the calculation of errors, as precisely as the Physical Sensor Model they replace.c) It may be provided as a Correspondence Model that provides a functional fitting based on observed relationships between the geopositions of a set of ground control points and their image coordinates.d) It may be provided as a set of ground control points that can be used to develop a Correspondence Model or to refine a Physical Sensor Model or True Replacement Model.This Technical Specification does not specify either how users derive geoposition data or the format or content of the data the users generate.",ISO/TS 19130:2010(en)
32,"The major components of the ASAP system are listed below. Metric Name Current Capability Threshold Objective Project Objective Current Status406 MHz Emergency Beacon without GPS Time-to-detectionAverage via COSPAS-SARSAT: 46 minAverage extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Average extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Waiting on T&E in 2Q and 3Q FY18406 MHz Emergency Beacon without GPS Time-to-geolocationAverage via COSPAS-SARSAT: 90 minAverage extrapolated from project T&E: 60 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project) Time to provide location as a web service: less than 1 minute from geolocationAverage extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Waiting on T&E in 2Q and 3Q FY18406 MHz Emergency Beacon without GPS geolocation accuracyAverage via COSPAS-SARSAT Doppler method: within 5 kmAverage via T&E: 10 km (location accuracy less important than time-to-geolocation for maritime Arctic SAR; tasked rescue assets have direction-finding capabilities)Average via T&E: 5 km (location accuracy less important than time-to-geolocation for maritime Arctic SAR; tasked rescue assets have direction-finding capabilities)Waiting on T&E in 2Q and 3Q FY18Channel 16 detection distress detection beyond 20 nmi of coastN/A from Rescue 21 or COSPAS-SARSATDetect, record, and playback channel 16 transmissions(i) Also automatically identify distress words (e.g., mayday, pan-pan), (ii) Display detection region on CG COPWaiting on T&E in 2Q and 3Q FY18Channel 70 detection distress detection beyond 20 nmi of coastN/A from Rescue 21 or COSPAS-SARSATDetect, record, and display channel 70 digital selective calling transmissions(i) Display detection region on CG COP (ii) Decode DSC message and display any embedded location information on CG COPWaiting on T&E in 2Q and 3Q FY1826 September 2016 Page 20 of 22An open architecture, open APIs, and an open data model will be central to the design of the ASAP processing node. The ASAP processing node will maintain a set of interfaces (APIs) that are meant to ensure interoperability between data systems across the various suite of sensors and sensor types. This open architecture will enable the ASAP software to interoperate with other components existing systems. As mentioned previously, the ASAP open framework will consist of standards for three different levels: Infrastructure Services, Common Services, and Mission Services.o IMDE/CSS is a DHS S&T program that will be leveraged by ASAP and will be part of the ASAP open architecture. IMDE/CSS will demonstrate an enterprise-wide system of maritime nodes providing streams of near real-time sensor data and access to non-sensor data including information, analysis, and archives. This system will access, process, and fuse data from mission feeds and diversified sensor sources and make it available through a web-based portal to all system users. IMDE/CSS is built on a scalable and _pluggable micro-service architecture with robust core services to include logging, identity management, data entitlement, and federated discovery.o ASAP plans to integrate onto IMDE in a later phase of the project. Polar Scout is not planned to integrate onto IMDE, however the Data Analytics does plan to integrate onto IMDE. The timeline (initially or at a later phase) of integrating onto IMDE is still under review.Identification and development of space-based sensor feeds, where the focus is the identification and leveraging of existing commercial and government sensors; and procuring new sensors to fill MDA gaps. There are two variants of CubeSats and SmallSats that are under development by HE360 and USAF ORS.o ORS: DHS S&T has executed an MOU with USAF ORS to collaborate in the demonstration of an on-orbit capability to collect relevant data in support of the DHS mission and the ORS mission to enable rapid low-cost space capability. ORS has developed a flexible, reusable, government-owned design for _6U CubeSats. ORS employs autonomous manufacturing, modular open system architecture, and open standards. ORS will demonstrate autonomous _open manufacturing of low volume, high-value assets and autonomous digital techniques to provide mission assurance. DHS plans to leverage a 2-ball ORS constellation to perform RF signal detection in support of the USCG mission.o HE360: HE360 is a commercial company focused on space-based sensing of radio-frequency (RF) signals and analytic applications of such data sets. The company intends to launch a constellation of satellites (3) able to perform RF signal detection, geolocation, and identification in early 2017. HE360 is under contract with DHS S&T to support on-orbit testing, evaluations, and integration of their systems with Mobile CubeSat Command and Control (MC3) System o MC3 System: The MC3 System is a network of fully autonomous ground stations. ASAP will purchase two MC3 ground stations and is working with the USCGs Rapid Development Program to determine locations where MC3 will be installed.26 September 2016 Page 21 of 22MC3 will be integrated into the USCGs Rescue Operations Centers, supporting Arctic MDA including search-and-rescue.Through collaboration with ORS, HE360 and MC3, DHS S&T and USCG will demonstrate, test, and evaluate detecting emergency distress beacons from space utilizing multiple on-orbit constellations of CubeSat systems. The space-based sensor data will be incorporated into predictive analytics and fusion tools to improve the arctic MDA mission area.",Geographic information — Ubiquitous public access — Reference model,https://www.iso.org/obp/ui/#!iso:std:32572:en,"IntroductionRecent advances of web-based and mobile computing technologies have ushered in an era where the general public are not only consumers of content, but also act as creators or providers of new, enriched content.One sector in particular that is experiencing rapid change and growth is that of geographic information.“Location” in a general sense is one of the basic requirements of all mobile users. In early development, users were only “consuming” location-based content, but with ever increasingly sophisticated mobile hardware devices and the ever expanding extent of telecommunications networking and sensor web enabled infrastructure, mobile users are now able to create many types of geographic data. Creation of content can be on an individual level, using a coordinate location to enable navigation to a new café described in a blog entry, or as a collaborative effort, such as collecting GPS tracks and incorporating them into the Open Street Map project.Smaller devices, embedded systems, wireless communication, and sensor networks (ubiquitous computing technologies) require methods of handling geographic information in terms of both production and consumption. Beyond the previously limited public consumption of geographic information, ubiquitous computing technologies provide the infrastructure for the general public to produce, distribute, and consume geographic information. These concepts are manifested as “seamless access from anywhere and at any time to easy-to-use geographic information and services”. We refer to these concepts as Ubiquitous Public Access to geographic information.The goal of ubiquitous public access to geographic information (UPA-to-GI) is to make the user experience of any “smart” device intuitive to understand, along with being easy to use. To achieve this goal, contextual information that is gathered from varied sources is managed efficiently within the UPA architecture. Therefore, systems or services for UPA to geographic information need to support a delivery mechanism of contextual information.This International Standard defines the requirements of standardization for systems and services supporting ubiquitous public access to geographic information, and describes a comprehensive set of fundamental facets that specify an abstract description of the elements for UPA to geographic information.This International Standard further establishes a series of models comprised as a conceptual framework that, when implemented, will support the development of a set of systems and services for enabling ubiquitous public access to geographic information. In a UPA environment, general users are no longer only passive consumers of geographic information, but rather active participants in several steps of the data and information management lifecycle such as collection, creation and capture, and/or use and dissemination.Ubiquitous public access to geographic information might be thought of as a type of geographic information service. However, the currently available standards used in mobile environments are based on web technologies which are not efficient enough to handle the requirements of UPA. In order to provide relevant geographic information to users, the context of the users is described.The reference model specified here defines a group of models which form a framework that supports methods of extracting geographically explicit context information from varied information sources, such as a lexicon, photos, videos, and others sources. Additional models in the framework specify how geographic data produced and distributed by the general public can be semantically linked to meet the user’s contextual requests, and how heterogeneous geographic content can be seamlessly accessed, integrated, and provided to a user regardless of the kind of device the user operates.","1   ScopeThis International Standard defines a reference model for ubiquitous public access (UPA) to geographic information. This reference model uses standard concepts from both the Open distributed processing — Reference model (RM-ODP) in ISO/IEC 10746-1 and ISO 19101.The reference model specified in this International Standard defines the following:— conceptual models for ubiquitous public access (UPA) to geographic information;— a reference model and framework to support current and future specification development in this area;— the semantics of information and processing within systems and services for the UPA of geographic information;— the architectural relationship between this International Standard and other ISO geographic information standards.This International Standard is applicable to location-based services (LBS), ubiquitous computing environments, linked open data, and other domains that require a seamless public access to geographic information.Although structured in the context of information technology and information technology standards, this International Standard is independent of any application development method or technology implementation approach.",ISO 19154:2014(en)
33,"The major components of the ASAP system are listed below. Metric Name Current Capability Threshold Objective Project Objective Current Status406 MHz Emergency Beacon without GPS Time-to-detectionAverage via COSPAS-SARSAT: 46 minAverage extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Average extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Waiting on T&E in 2Q and 3Q FY18406 MHz Emergency Beacon without GPS Time-to-geolocationAverage via COSPAS-SARSAT: 90 minAverage extrapolated from project T&E: 60 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project) Time to provide location as a web service: less than 1 minute from geolocationAverage extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Waiting on T&E in 2Q and 3Q FY18406 MHz Emergency Beacon without GPS geolocation accuracyAverage via COSPAS-SARSAT Doppler method: within 5 kmAverage via T&E: 10 km (location accuracy less important than time-to-geolocation for maritime Arctic SAR; tasked rescue assets have direction-finding capabilities)Average via T&E: 5 km (location accuracy less important than time-to-geolocation for maritime Arctic SAR; tasked rescue assets have direction-finding capabilities)Waiting on T&E in 2Q and 3Q FY18Channel 16 detection distress detection beyond 20 nmi of coastN/A from Rescue 21 or COSPAS-SARSATDetect, record, and playback channel 16 transmissions(i) Also automatically identify distress words (e.g., mayday, pan-pan), (ii) Display detection region on CG COPWaiting on T&E in 2Q and 3Q FY18Channel 70 detection distress detection beyond 20 nmi of coastN/A from Rescue 21 or COSPAS-SARSATDetect, record, and display channel 70 digital selective calling transmissions(i) Display detection region on CG COP (ii) Decode DSC message and display any embedded location information on CG COPWaiting on T&E in 2Q and 3Q FY1826 September 2016 Page 20 of 22An open architecture, open APIs, and an open data model will be central to the design of the ASAP processing node. The ASAP processing node will maintain a set of interfaces (APIs) that are meant to ensure interoperability between data systems across the various suite of sensors and sensor types. This open architecture will enable the ASAP software to interoperate with other components existing systems. As mentioned previously, the ASAP open framework will consist of standards for three different levels: Infrastructure Services, Common Services, and Mission Services.o IMDE/CSS is a DHS S&T program that will be leveraged by ASAP and will be part of the ASAP open architecture. IMDE/CSS will demonstrate an enterprise-wide system of maritime nodes providing streams of near real-time sensor data and access to non-sensor data including information, analysis, and archives. This system will access, process, and fuse data from mission feeds and diversified sensor sources and make it available through a web-based portal to all system users. IMDE/CSS is built on a scalable and _pluggable micro-service architecture with robust core services to include logging, identity management, data entitlement, and federated discovery.o ASAP plans to integrate onto IMDE in a later phase of the project. Polar Scout is not planned to integrate onto IMDE, however the Data Analytics does plan to integrate onto IMDE. The timeline (initially or at a later phase) of integrating onto IMDE is still under review.Identification and development of space-based sensor feeds, where the focus is the identification and leveraging of existing commercial and government sensors; and procuring new sensors to fill MDA gaps. There are two variants of CubeSats and SmallSats that are under development by HE360 and USAF ORS.o ORS: DHS S&T has executed an MOU with USAF ORS to collaborate in the demonstration of an on-orbit capability to collect relevant data in support of the DHS mission and the ORS mission to enable rapid low-cost space capability. ORS has developed a flexible, reusable, government-owned design for _6U CubeSats. ORS employs autonomous manufacturing, modular open system architecture, and open standards. ORS will demonstrate autonomous _open manufacturing of low volume, high-value assets and autonomous digital techniques to provide mission assurance. DHS plans to leverage a 2-ball ORS constellation to perform RF signal detection in support of the USCG mission.o HE360: HE360 is a commercial company focused on space-based sensing of radio-frequency (RF) signals and analytic applications of such data sets. The company intends to launch a constellation of satellites (3) able to perform RF signal detection, geolocation, and identification in early 2017. HE360 is under contract with DHS S&T to support on-orbit testing, evaluations, and integration of their systems with Mobile CubeSat Command and Control (MC3) System o MC3 System: The MC3 System is a network of fully autonomous ground stations. ASAP will purchase two MC3 ground stations and is working with the USCGs Rapid Development Program to determine locations where MC3 will be installed.26 September 2016 Page 21 of 22MC3 will be integrated into the USCGs Rescue Operations Centers, supporting Arctic MDA including search-and-rescue.Through collaboration with ORS, HE360 and MC3, DHS S&T and USCG will demonstrate, test, and evaluate detecting emergency distress beacons from space utilizing multiple on-orbit constellations of CubeSat systems. The space-based sensor data will be incorporated into predictive analytics and fusion tools to improve the arctic MDA mission area.",Information technology — Open data protocol (OData) v4.0 — Part 2: OData JSON Format,https://www.iso.org/obp/ui/#!iso:std:69209:en,,,ISO/IEC 20802-2:2016(en)
34,"The major components of the ASAP system are listed below. Metric Name Current Capability Threshold Objective Project Objective Current Status406 MHz Emergency Beacon without GPS Time-to-detectionAverage via COSPAS-SARSAT: 46 minAverage extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Average extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Waiting on T&E in 2Q and 3Q FY18406 MHz Emergency Beacon without GPS Time-to-geolocationAverage via COSPAS-SARSAT: 90 minAverage extrapolated from project T&E: 60 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project) Time to provide location as a web service: less than 1 minute from geolocationAverage extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Waiting on T&E in 2Q and 3Q FY18406 MHz Emergency Beacon without GPS geolocation accuracyAverage via COSPAS-SARSAT Doppler method: within 5 kmAverage via T&E: 10 km (location accuracy less important than time-to-geolocation for maritime Arctic SAR; tasked rescue assets have direction-finding capabilities)Average via T&E: 5 km (location accuracy less important than time-to-geolocation for maritime Arctic SAR; tasked rescue assets have direction-finding capabilities)Waiting on T&E in 2Q and 3Q FY18Channel 16 detection distress detection beyond 20 nmi of coastN/A from Rescue 21 or COSPAS-SARSATDetect, record, and playback channel 16 transmissions(i) Also automatically identify distress words (e.g., mayday, pan-pan), (ii) Display detection region on CG COPWaiting on T&E in 2Q and 3Q FY18Channel 70 detection distress detection beyond 20 nmi of coastN/A from Rescue 21 or COSPAS-SARSATDetect, record, and display channel 70 digital selective calling transmissions(i) Display detection region on CG COP (ii) Decode DSC message and display any embedded location information on CG COPWaiting on T&E in 2Q and 3Q FY1826 September 2016 Page 20 of 22An open architecture, open APIs, and an open data model will be central to the design of the ASAP processing node. The ASAP processing node will maintain a set of interfaces (APIs) that are meant to ensure interoperability between data systems across the various suite of sensors and sensor types. This open architecture will enable the ASAP software to interoperate with other components existing systems. As mentioned previously, the ASAP open framework will consist of standards for three different levels: Infrastructure Services, Common Services, and Mission Services.o IMDE/CSS is a DHS S&T program that will be leveraged by ASAP and will be part of the ASAP open architecture. IMDE/CSS will demonstrate an enterprise-wide system of maritime nodes providing streams of near real-time sensor data and access to non-sensor data including information, analysis, and archives. This system will access, process, and fuse data from mission feeds and diversified sensor sources and make it available through a web-based portal to all system users. IMDE/CSS is built on a scalable and _pluggable micro-service architecture with robust core services to include logging, identity management, data entitlement, and federated discovery.o ASAP plans to integrate onto IMDE in a later phase of the project. Polar Scout is not planned to integrate onto IMDE, however the Data Analytics does plan to integrate onto IMDE. The timeline (initially or at a later phase) of integrating onto IMDE is still under review.Identification and development of space-based sensor feeds, where the focus is the identification and leveraging of existing commercial and government sensors; and procuring new sensors to fill MDA gaps. There are two variants of CubeSats and SmallSats that are under development by HE360 and USAF ORS.o ORS: DHS S&T has executed an MOU with USAF ORS to collaborate in the demonstration of an on-orbit capability to collect relevant data in support of the DHS mission and the ORS mission to enable rapid low-cost space capability. ORS has developed a flexible, reusable, government-owned design for _6U CubeSats. ORS employs autonomous manufacturing, modular open system architecture, and open standards. ORS will demonstrate autonomous _open manufacturing of low volume, high-value assets and autonomous digital techniques to provide mission assurance. DHS plans to leverage a 2-ball ORS constellation to perform RF signal detection in support of the USCG mission.o HE360: HE360 is a commercial company focused on space-based sensing of radio-frequency (RF) signals and analytic applications of such data sets. The company intends to launch a constellation of satellites (3) able to perform RF signal detection, geolocation, and identification in early 2017. HE360 is under contract with DHS S&T to support on-orbit testing, evaluations, and integration of their systems with Mobile CubeSat Command and Control (MC3) System o MC3 System: The MC3 System is a network of fully autonomous ground stations. ASAP will purchase two MC3 ground stations and is working with the USCGs Rapid Development Program to determine locations where MC3 will be installed.26 September 2016 Page 21 of 22MC3 will be integrated into the USCGs Rescue Operations Centers, supporting Arctic MDA including search-and-rescue.Through collaboration with ORS, HE360 and MC3, DHS S&T and USCG will demonstrate, test, and evaluate detecting emergency distress beacons from space utilizing multiple on-orbit constellations of CubeSat systems. The space-based sensor data will be incorporated into predictive analytics and fusion tools to improve the arctic MDA mission area.",Information technology — Security techniques — A framework for identity management — Part 1: Terminology and concepts,https://www.iso.org/obp/ui/#!iso:std:57914:en,"IntroductionData processing systems commonly gather a range of information on their users, be it a person, piece of equipment, or piece of software connected to them, and make decisions based on the gathered information. Such identity-based decisions may concern access to applications or other resources.To address the need to efficiently and effectively implement systems that make identity-based decisions, ISO/IEC 24760 specifies a framework for the issuance, administration, and use of data that serves to characterize individuals, organizations or information technology components which operate on behalf of individuals or organizations.For many organizations the proper management of identity information is crucial to maintain security of the organizational processes. For individuals, correct identity management is important to protect privacy.ISO/IEC 24760 specifies fundamental concepts and operational structures of identity management with the purpose to realize information system management so that information systems can meet business, contractual, regulatory and legal obligations.This part of ISO/IEC 24760 specifies the terminology and concepts for identity management, to promote a common understanding in the field of identity management. It also provides a bibliography of documents related to standardization of various aspects of identity management.","1   ScopeThis part of ISO/IEC 24760— defines terms for identity management, and— specifies core concepts of identity and identity management and their relationships.This part of ISO/IEC 24760 is applicable to any information system that processes identity information.A bibliography of documents describing various aspects of identity information management is provided.",ISO/IEC 24760-1:2011(en)
35,"The major components of the ASAP system are listed below. Metric Name Current Capability Threshold Objective Project Objective Current Status406 MHz Emergency Beacon without GPS Time-to-detectionAverage via COSPAS-SARSAT: 46 minAverage extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Average extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Waiting on T&E in 2Q and 3Q FY18406 MHz Emergency Beacon without GPS Time-to-geolocationAverage via COSPAS-SARSAT: 90 minAverage extrapolated from project T&E: 60 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project) Time to provide location as a web service: less than 1 minute from geolocationAverage extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Waiting on T&E in 2Q and 3Q FY18406 MHz Emergency Beacon without GPS geolocation accuracyAverage via COSPAS-SARSAT Doppler method: within 5 kmAverage via T&E: 10 km (location accuracy less important than time-to-geolocation for maritime Arctic SAR; tasked rescue assets have direction-finding capabilities)Average via T&E: 5 km (location accuracy less important than time-to-geolocation for maritime Arctic SAR; tasked rescue assets have direction-finding capabilities)Waiting on T&E in 2Q and 3Q FY18Channel 16 detection distress detection beyond 20 nmi of coastN/A from Rescue 21 or COSPAS-SARSATDetect, record, and playback channel 16 transmissions(i) Also automatically identify distress words (e.g., mayday, pan-pan), (ii) Display detection region on CG COPWaiting on T&E in 2Q and 3Q FY18Channel 70 detection distress detection beyond 20 nmi of coastN/A from Rescue 21 or COSPAS-SARSATDetect, record, and display channel 70 digital selective calling transmissions(i) Display detection region on CG COP (ii) Decode DSC message and display any embedded location information on CG COPWaiting on T&E in 2Q and 3Q FY1826 September 2016 Page 20 of 22An open architecture, open APIs, and an open data model will be central to the design of the ASAP processing node. The ASAP processing node will maintain a set of interfaces (APIs) that are meant to ensure interoperability between data systems across the various suite of sensors and sensor types. This open architecture will enable the ASAP software to interoperate with other components existing systems. As mentioned previously, the ASAP open framework will consist of standards for three different levels: Infrastructure Services, Common Services, and Mission Services.o IMDE/CSS is a DHS S&T program that will be leveraged by ASAP and will be part of the ASAP open architecture. IMDE/CSS will demonstrate an enterprise-wide system of maritime nodes providing streams of near real-time sensor data and access to non-sensor data including information, analysis, and archives. This system will access, process, and fuse data from mission feeds and diversified sensor sources and make it available through a web-based portal to all system users. IMDE/CSS is built on a scalable and _pluggable micro-service architecture with robust core services to include logging, identity management, data entitlement, and federated discovery.o ASAP plans to integrate onto IMDE in a later phase of the project. Polar Scout is not planned to integrate onto IMDE, however the Data Analytics does plan to integrate onto IMDE. The timeline (initially or at a later phase) of integrating onto IMDE is still under review.Identification and development of space-based sensor feeds, where the focus is the identification and leveraging of existing commercial and government sensors; and procuring new sensors to fill MDA gaps. There are two variants of CubeSats and SmallSats that are under development by HE360 and USAF ORS.o ORS: DHS S&T has executed an MOU with USAF ORS to collaborate in the demonstration of an on-orbit capability to collect relevant data in support of the DHS mission and the ORS mission to enable rapid low-cost space capability. ORS has developed a flexible, reusable, government-owned design for _6U CubeSats. ORS employs autonomous manufacturing, modular open system architecture, and open standards. ORS will demonstrate autonomous _open manufacturing of low volume, high-value assets and autonomous digital techniques to provide mission assurance. DHS plans to leverage a 2-ball ORS constellation to perform RF signal detection in support of the USCG mission.o HE360: HE360 is a commercial company focused on space-based sensing of radio-frequency (RF) signals and analytic applications of such data sets. The company intends to launch a constellation of satellites (3) able to perform RF signal detection, geolocation, and identification in early 2017. HE360 is under contract with DHS S&T to support on-orbit testing, evaluations, and integration of their systems with Mobile CubeSat Command and Control (MC3) System o MC3 System: The MC3 System is a network of fully autonomous ground stations. ASAP will purchase two MC3 ground stations and is working with the USCGs Rapid Development Program to determine locations where MC3 will be installed.26 September 2016 Page 21 of 22MC3 will be integrated into the USCGs Rescue Operations Centers, supporting Arctic MDA including search-and-rescue.Through collaboration with ORS, HE360 and MC3, DHS S&T and USCG will demonstrate, test, and evaluate detecting emergency distress beacons from space utilizing multiple on-orbit constellations of CubeSat systems. The space-based sensor data will be incorporated into predictive analytics and fusion tools to improve the arctic MDA mission area.",Information technology — Security techniques — A framework for identity management — Part 2: Reference architecture and requirements,https://www.iso.org/obp/ui/#!iso:std:57915:en,"IntroductionData processing systems commonly gather a range of information on its users be it a person, piece of equipment, or piece of software connected to it and make decisions based on the gathered information. Such identity-based decisions may concern access to applications or other resources.To address the need to efficiently and effectively implement systems that make identity-based decisions, this part of ISO/IEC 24760 specifies a framework for the issuance, administration, and use of data that serves to characterize individuals, organizations, or information technology components, which operate on behalf of individuals or organizations.For many organizations, the proper management of identity information is crucial to maintain security of the organizational processes. For individuals, correct identity management is important to protect privacy.ISO/IEC 24760 specifies fundamental concepts and operational structures of identity management with the purpose to realize information system management so that information systems can meet business, contractual, regulatory, and legal obligations.This part of ISO/IEC 24760 defines a reference architecture for an identity management system that includes key architectural elements and their interrelationships. These architectural elements are described in respect to identity management deployments models. This part of ISO/IEC 24760 specifies requirements for the design and implementation of an identity management system so that it can meet the objectives of stakeholders involved in the deployment and operation of that system.This part of ISO/IEC 24760 is intended to provide a foundation for the implementation of other International Standards related to identity information processing such as— ISO/IEC 29100, Information technology ─ Security techniques ─ Privacy framework,— ISO/IEC 29101, Information technology ─ Security techniques ─ Privacy reference architecture,— ISO/IEC 29115, Information technology ─ Security techniques ─ Entity authentication assurance framework, and— ISO/IEC 29146, Information technology ─ Security techniques ─ A framework for access management.","1   ScopeThis part of ISO/IEC 24760— provides guidelines for the implementation of systems for the management of identity information, and— specifies requirements for the implementation and operation of a framework for identity management.This part of ISO/IEC 24760 is applicable to any information system where information relating to identity is processed or stored.",ISO/IEC 24760-2:2015(en)
36,"The major components of the ASAP system are listed below. Metric Name Current Capability Threshold Objective Project Objective Current Status406 MHz Emergency Beacon without GPS Time-to-detectionAverage via COSPAS-SARSAT: 46 minAverage extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Average extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Waiting on T&E in 2Q and 3Q FY18406 MHz Emergency Beacon without GPS Time-to-geolocationAverage via COSPAS-SARSAT: 90 minAverage extrapolated from project T&E: 60 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project) Time to provide location as a web service: less than 1 minute from geolocationAverage extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Waiting on T&E in 2Q and 3Q FY18406 MHz Emergency Beacon without GPS geolocation accuracyAverage via COSPAS-SARSAT Doppler method: within 5 kmAverage via T&E: 10 km (location accuracy less important than time-to-geolocation for maritime Arctic SAR; tasked rescue assets have direction-finding capabilities)Average via T&E: 5 km (location accuracy less important than time-to-geolocation for maritime Arctic SAR; tasked rescue assets have direction-finding capabilities)Waiting on T&E in 2Q and 3Q FY18Channel 16 detection distress detection beyond 20 nmi of coastN/A from Rescue 21 or COSPAS-SARSATDetect, record, and playback channel 16 transmissions(i) Also automatically identify distress words (e.g., mayday, pan-pan), (ii) Display detection region on CG COPWaiting on T&E in 2Q and 3Q FY18Channel 70 detection distress detection beyond 20 nmi of coastN/A from Rescue 21 or COSPAS-SARSATDetect, record, and display channel 70 digital selective calling transmissions(i) Display detection region on CG COP (ii) Decode DSC message and display any embedded location information on CG COPWaiting on T&E in 2Q and 3Q FY1826 September 2016 Page 20 of 22An open architecture, open APIs, and an open data model will be central to the design of the ASAP processing node. The ASAP processing node will maintain a set of interfaces (APIs) that are meant to ensure interoperability between data systems across the various suite of sensors and sensor types. This open architecture will enable the ASAP software to interoperate with other components existing systems. As mentioned previously, the ASAP open framework will consist of standards for three different levels: Infrastructure Services, Common Services, and Mission Services.o IMDE/CSS is a DHS S&T program that will be leveraged by ASAP and will be part of the ASAP open architecture. IMDE/CSS will demonstrate an enterprise-wide system of maritime nodes providing streams of near real-time sensor data and access to non-sensor data including information, analysis, and archives. This system will access, process, and fuse data from mission feeds and diversified sensor sources and make it available through a web-based portal to all system users. IMDE/CSS is built on a scalable and _pluggable micro-service architecture with robust core services to include logging, identity management, data entitlement, and federated discovery.o ASAP plans to integrate onto IMDE in a later phase of the project. Polar Scout is not planned to integrate onto IMDE, however the Data Analytics does plan to integrate onto IMDE. The timeline (initially or at a later phase) of integrating onto IMDE is still under review.Identification and development of space-based sensor feeds, where the focus is the identification and leveraging of existing commercial and government sensors; and procuring new sensors to fill MDA gaps. There are two variants of CubeSats and SmallSats that are under development by HE360 and USAF ORS.o ORS: DHS S&T has executed an MOU with USAF ORS to collaborate in the demonstration of an on-orbit capability to collect relevant data in support of the DHS mission and the ORS mission to enable rapid low-cost space capability. ORS has developed a flexible, reusable, government-owned design for _6U CubeSats. ORS employs autonomous manufacturing, modular open system architecture, and open standards. ORS will demonstrate autonomous _open manufacturing of low volume, high-value assets and autonomous digital techniques to provide mission assurance. DHS plans to leverage a 2-ball ORS constellation to perform RF signal detection in support of the USCG mission.o HE360: HE360 is a commercial company focused on space-based sensing of radio-frequency (RF) signals and analytic applications of such data sets. The company intends to launch a constellation of satellites (3) able to perform RF signal detection, geolocation, and identification in early 2017. HE360 is under contract with DHS S&T to support on-orbit testing, evaluations, and integration of their systems with Mobile CubeSat Command and Control (MC3) System o MC3 System: The MC3 System is a network of fully autonomous ground stations. ASAP will purchase two MC3 ground stations and is working with the USCGs Rapid Development Program to determine locations where MC3 will be installed.26 September 2016 Page 21 of 22MC3 will be integrated into the USCGs Rescue Operations Centers, supporting Arctic MDA including search-and-rescue.Through collaboration with ORS, HE360 and MC3, DHS S&T and USCG will demonstrate, test, and evaluate detecting emergency distress beacons from space utilizing multiple on-orbit constellations of CubeSat systems. The space-based sensor data will be incorporated into predictive analytics and fusion tools to improve the arctic MDA mission area.",Information technology — Security techniques — A framework for identity management — Part 3: Practice,https://www.iso.org/obp/ui/#!iso:std:57916:en,"IntroductionData processing systems commonly gather a range of information on their users, be it a person, piece of equipment, or piece of software connected to it and make decisions based on the gathered information. Such identity-based decisions may concern access to applications or other resources.To address the need to efficiently and effectively implement systems that make identity-based decisions, ISO/IEC 24760 specifies a framework for the issuance, administration, and use of data that serves to characterize individuals, organizations or information technology components, which operate on behalf of individuals or organizations.For many organizations, the proper management of identity information is crucial to maintain security of the organizational processes. For individuals, correct identity management is important to protect privacy.This part of ISO/IEC 24760 specifies fundamental concepts and operational structures of identity management with the purpose to realize information system management, so that information systems can meet business, contractual, regulatory and legal obligations.This part of ISO/IEC 24760 presents practices for identity management. These practices cover assurance in controlling identity information use, controlling the access to identity information and other resources based on identity information, and controlling objectives that should be implemented when establishing and maintaining an identity management system.This part of ISO/IEC 24760 consists of the following parts:— ISO/IEC 24760-1: Terminology and concepts;— ISO/IEC 24760-2: Reference architecture and requirements;— ISO/IEC 24760-3: Practice.ISO/IEC 24760 is intended to provide foundations for other identity management related International Standards including the following:— ISO/IEC 29100, Privacy framework;— ISO/IEC 29101, Privacy reference architecture;— ISO/IEC 29115, Entity authentication assurance framework;— ISO/IEC 29146, A framework for access management.","1   ScopeThis part of ISO/IEC 24760 provides guidance for the management of identity information and for ensuring that an identity management system conforms to ISO/IEC 24760-1 and ISO/IEC 24760-2.This part of ISO/IEC 24760 is applicable to an identity management system where identifiers or PII relating to entities are acquired, processed, stored, transferred or used for the purposes of identifying or authenticating entities and/or for the purpose of decision making using attributes of entities. Practices for identity management can also be addressed in other standards.",ISO/IEC 24760-3:2016(en)
37,"The major components of the ASAP system are listed below. Metric Name Current Capability Threshold Objective Project Objective Current Status406 MHz Emergency Beacon without GPS Time-to-detectionAverage via COSPAS-SARSAT: 46 minAverage extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Average extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Waiting on T&E in 2Q and 3Q FY18406 MHz Emergency Beacon without GPS Time-to-geolocationAverage via COSPAS-SARSAT: 90 minAverage extrapolated from project T&E: 60 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project) Time to provide location as a web service: less than 1 minute from geolocationAverage extrapolated from project T&E: 30 min (""Average extrapolated"" means estimate for a full constellation based on T&E results of project)Waiting on T&E in 2Q and 3Q FY18406 MHz Emergency Beacon without GPS geolocation accuracyAverage via COSPAS-SARSAT Doppler method: within 5 kmAverage via T&E: 10 km (location accuracy less important than time-to-geolocation for maritime Arctic SAR; tasked rescue assets have direction-finding capabilities)Average via T&E: 5 km (location accuracy less important than time-to-geolocation for maritime Arctic SAR; tasked rescue assets have direction-finding capabilities)Waiting on T&E in 2Q and 3Q FY18Channel 16 detection distress detection beyond 20 nmi of coastN/A from Rescue 21 or COSPAS-SARSATDetect, record, and playback channel 16 transmissions(i) Also automatically identify distress words (e.g., mayday, pan-pan), (ii) Display detection region on CG COPWaiting on T&E in 2Q and 3Q FY18Channel 70 detection distress detection beyond 20 nmi of coastN/A from Rescue 21 or COSPAS-SARSATDetect, record, and display channel 70 digital selective calling transmissions(i) Display detection region on CG COP (ii) Decode DSC message and display any embedded location information on CG COPWaiting on T&E in 2Q and 3Q FY1826 September 2016 Page 20 of 22An open architecture, open APIs, and an open data model will be central to the design of the ASAP processing node. The ASAP processing node will maintain a set of interfaces (APIs) that are meant to ensure interoperability between data systems across the various suite of sensors and sensor types. This open architecture will enable the ASAP software to interoperate with other components existing systems. As mentioned previously, the ASAP open framework will consist of standards for three different levels: Infrastructure Services, Common Services, and Mission Services.o IMDE/CSS is a DHS S&T program that will be leveraged by ASAP and will be part of the ASAP open architecture. IMDE/CSS will demonstrate an enterprise-wide system of maritime nodes providing streams of near real-time sensor data and access to non-sensor data including information, analysis, and archives. This system will access, process, and fuse data from mission feeds and diversified sensor sources and make it available through a web-based portal to all system users. IMDE/CSS is built on a scalable and _pluggable micro-service architecture with robust core services to include logging, identity management, data entitlement, and federated discovery.o ASAP plans to integrate onto IMDE in a later phase of the project. Polar Scout is not planned to integrate onto IMDE, however the Data Analytics does plan to integrate onto IMDE. The timeline (initially or at a later phase) of integrating onto IMDE is still under review.Identification and development of space-based sensor feeds, where the focus is the identification and leveraging of existing commercial and government sensors; and procuring new sensors to fill MDA gaps. There are two variants of CubeSats and SmallSats that are under development by HE360 and USAF ORS.o ORS: DHS S&T has executed an MOU with USAF ORS to collaborate in the demonstration of an on-orbit capability to collect relevant data in support of the DHS mission and the ORS mission to enable rapid low-cost space capability. ORS has developed a flexible, reusable, government-owned design for _6U CubeSats. ORS employs autonomous manufacturing, modular open system architecture, and open standards. ORS will demonstrate autonomous _open manufacturing of low volume, high-value assets and autonomous digital techniques to provide mission assurance. DHS plans to leverage a 2-ball ORS constellation to perform RF signal detection in support of the USCG mission.o HE360: HE360 is a commercial company focused on space-based sensing of radio-frequency (RF) signals and analytic applications of such data sets. The company intends to launch a constellation of satellites (3) able to perform RF signal detection, geolocation, and identification in early 2017. HE360 is under contract with DHS S&T to support on-orbit testing, evaluations, and integration of their systems with Mobile CubeSat Command and Control (MC3) System o MC3 System: The MC3 System is a network of fully autonomous ground stations. ASAP will purchase two MC3 ground stations and is working with the USCGs Rapid Development Program to determine locations where MC3 will be installed.26 September 2016 Page 21 of 22MC3 will be integrated into the USCGs Rescue Operations Centers, supporting Arctic MDA including search-and-rescue.Through collaboration with ORS, HE360 and MC3, DHS S&T and USCG will demonstrate, test, and evaluate detecting emergency distress beacons from space utilizing multiple on-orbit constellations of CubeSat systems. The space-based sensor data will be incorporated into predictive analytics and fusion tools to improve the arctic MDA mission area.",Information technology — Biometrics — The use of biometric technology in commercial Identity Management applications and processes,https://www.iso.org/obp/ui/#!iso:std:45167:en,"IntroductionThis Technical Report provides support for the further development of ISO/IEC biometric standards in the context of cross-jurisdictional and societal applications of biometrics, including standardization of both existing and future technologies.The contents of this Technical Report are recommended practices and guidelines and they are not mandatory. Legal requirements of the respective countries take precedence and biometric data should be obtained in accordance with local norms of behaviour. This Technical Report does not reduce any rights or obligations provided by applicable laws. Compliance with any recommendations in the Technical Report does not, in itself, confer immunity from legal obligations.Examples of the benefits to be gained by following the recommendations and guidelines in this Technical Report are— enhanced acceptance by subjects of systems using biometric technology,— improved public perception and understanding of these systems,— smoother introduction and operation of these systems,— potential long-term cost reduction (whole life costs),— adoption of commonly approved good privacy practice,— interoperability both domestically and internationally, and— implemented solutions having a greater degree of vendor independence.The primary stakeholders are identified as— users – those who use the results of the biometric data,— developers of technical standards,— subjects – those who provide the biometric sample,— writers of system specifications, system architects, and IT designers, and— public policy makers.","1   Scope1.1   In scopeThis Technical Report will discuss— concepts and considerations for the use of biometrics in a commercial Identity Management Solutions,— items that need to be considered when integrating biometrics into a commercial Identity Management Solutions, and— implementation Issues when implementing biometrics into commercial Identity Management Solutions.1.2   ExclusionsThis Technical Report will not— define an architecture and framework for IDM,— discuss any specification or assessment of government policy,— discuss the business need for a biometric database or process,— discuss the specific biometrics and which ones are to be used in particular systems,— consider the legality and acceptability in particular jurisdictions and cultures,— analyse the general structure of identifiers and the global identification of objects (e.g. object identifiers), and— discuss technical specifications in relation to the use of trusted biometric hardware and software.",ISO/IEC TR 29144:2014(en)
38,"3.3.1 Polar Scout Trade Space AnalysisFour broad range of solutions were considered in the ASAP trade space analysis: terrestrial, maritime, airborne, and space-based. To assess the alternatives, the trade study examined a 200 by 200 NMI area 100 NM north of Barrow, Alaska, and the entire Juneau Alaska region. Each option was assessed by its ability to provide continuous coverage (24 hours per day, seven days per week) and provide a minimum of a 30 minute revisit time.Each alternative was assessed in the following areas:Platform acquisition costAccess to area of interestTechnical maturity/feasibility3.3.1.1 Preferred AlternativeWithin each solution category (terrestrial, maritime, airborne, and space-based) many potential configurations were constructed and assessed. The space-based solutions using CubeSats proved to be the optimal solution in the ability to provide a 30-minute revisit and were the least expensive alternatives. The best land-based, airborne, and maritime solutions were cost-prohibitive or had insufficient range when compared to the CubeSat space-based solution.3.3.1.2 Logistics ConsiderationsLogistics were not taken into consideration in the trade space analysis since other factors disqualified the non-CubeSat alternatives due to their high cost of acquiring the asset, access to coverage area, and technical maturity.3.3.1.3 Polar Scout Technical Approach3.3.1.3.1 Design, Development, and Integration ApproachPolar Scout is leveraging design and development activities performed under other government or commercial contracts.The 6U CubeSats have been under development for approximately 5 years by ORS. When DHS decided to leverage the ORS 6U CubeSats, the CubeSats had already passed their CDR milestone approximately two years prior. The milestones26 September 2016 Page 23 of 22monitored by DHS will be bus and payload integration activities, bus and payload test activities (e.g. thermal, environmental, etc) and ground processing activities. DHS is leveraging a payload design (by Rincon) that is currently in operations on another CubeSat. This payload will be integrated into the 6U CubeSats. The milestones monitored by DHS will be payload fabrication, payload and bus integration, payload test activities (e.g. thermal, environmental, etc), and payload processing activities. The ASAP Program Manager has decision authority of all 6U flight build, payload integration and test activities, and deviations that are directly related to the operations of the 6U CubeSats.Polar Scout is leveraging the HE360 commercial development effort to acquire additional EPIRB data. For the HE360 contract, the ASAP Program Manager has decision authority for only those design, development, and test activities related to the collection and processing of EPRIB data.A portion of the Analytics contract (RadiantBlue/HumanGeo) will be for RadiantBlue/HumanGeo to perform as the ground integrator for Polar Scout. RadiantBlue/HumanGeo will integrate the EPIRB data from both HE360 and the 6U and provide that data to the USCG and NOAA as agreed upon. To reduce cost and to further separate the Polar Scout and Analytics efforts, the ASAP project is also investigating the possibility of having Rincon integrate the EPRIB data from both the 6Us and HE360.3.3.1.3.2 Communication ApproachThe ASAP Program Manager has stood up several IPTs to monitor the Polar Scout activities. Stakeholders for the 6U CubeSat effort meet weekly in the Technical IPT and the Ground IPT. Through these two IPTs, the Polar Scout Program Manager gains insight into the status, issues, risks, and resolution of challenges. The ASAP Program Manager also has a bi-weekly technical and budget meeting with the 6U prime contractor. In addition the ASAP Program Manager conducts a status meeting with Aerospace to review project status, actions, risks, and open items.For the HE360 contract, HE360 provides monthly status and budget reports. TEMs are held approximately monthly to review the status, issues, risks, and resolution of challenges.3.3.2 AnalyticsFor the RadiantBlue/HumanGeo contract, the Program Manager meets with the team bi-weekly to review the status, issues, risks, and resolution of challenges. These meeting just began during the month of March 2017.",Space systems — Structural components and assemblies,https://www.iso.org/obp/ui/#!iso:std:46099:en,"IntroductionStructures are the backbones of all spaceflight systems. A structural failure could cause the loss of human lives for manned space systems or could jeopardize the intended mission for unmanned space systems. Currently, there is no International Standard that covers all the aspects that can be used for spaceflight structural items such as spacecraft platforms, interstage adaptors, launch vehicle buses and rocket motor cases.The purpose of this International Standard is to establish general requirements for structures. It provides the uniform requirements necessary to minimize the duplication of effort and the differences between approaches taken by the participating nations and their commercial space communities in developing structures. In addition, the use of agreed-upon standards will facilitate cooperation and communication among space progammes.","1   ScopeThis International Standard establishes requirements for the design; material selection and characterization; fabrication; testing and inspection of all structural items in space systems, including expendable and reusable launch vehicles, satellites and their payloads. This International Standard, when implemented for a particular space system, will assure high confidence in achieving safe and reliable operation in all phases of its planned mission.This International Standard applies specifically to all structural items, including fracture-critical hardware used in space systems during all phases of the mission, with the following exceptions: adaptive structures, engines and thermal protection systems.",ISO 10786:2011(en)
39,"Algorithm data package refers to computer software, executable on a Linux, Unix, or Windows-based computer system, that implements the APID algorithm and contains:o   An executable binary file that contains the applications main entry point and any code that was statically linked to the application target;o   Resource files and software libraries necessary for the execution of the executable file; ando   Computer software documentation corresponding to the use, operation, and maintenance of the application.Application Program Interface refers to computer software documentation describing how software components may interact with other software components. It details, in human-readable format, the expected behavior of software libraries. It may contain the set of public methods, variables used as arguments, and return values for public methods. It shall contain a set of protocols, routines, and tools necessary to build application software interacting with the computer software for which the API was written. ",Systems and software engineering — Requirements for designers and developers of user documentation,https://www.iso.org/obp/ui/#!iso:std:43073:en,"IntroductionAnyone who uses application software needs accurate information about how the software will help the user accomplish a task. The documentation may be the first tangible item that the user sees and therefore influences the user’s first impressions of the software product. If the information is supplied in a convenient form and is easy to find and understand, the user can quickly become proficient at using the product. Hence, well-designed documentation not only assists the user and helps to reduce the cost of training and support, but also enhances the reputation of the product, its producer, and its suppliers.Although software developers aim to design user interfaces that behave so intuitively that very little separate documentation is needed, this is rarely possible. Today’s software offers increasingly robust functionality, not only within applications, but also across applications that intelligently exchange information with one another. Further, most software designs include underlying rules and calculations, or algorithms, that affect the results a user can obtain when using the software. Such underlying programming mechanics are discernable by users, but only through laborious testing. For these reasons and more, user documentation remains an essential component of usable software products.Documentation is often regarded as something done after the software has been implemented. However, for high-quality software documentation, its development should be regarded as an integral part of the software life cycle process. If done properly, documentation or information management is a big enough job to require process planning in its own right.This International Standard was developed to assist users of ISO/IEC 15288:2008, Systems and software engineering — System life cycle processes, or ISO/IEC 12207:2008, Systems and software engineering — Software life cycle processes, to design and develop documentation as part of the software life cycle processes. It defines the documentation process from the documentation developer's standpoint.NOTE Other International Standards in the ISO/IEC 265NN family are in preparation or planned to address the documentation and information management processes from the viewpoints of managers, assessors and testers, and acquirers and suppliers.In addition to defining a standard process, this International Standard also covers the documentation product. This International Standard specifies the structure, content, and format for documentation, and also provides informative guidance for user documentation style.Earlier standards tended to view the results of the documentation process as a single book or multivolume set: a one-time deliverable. Increasingly, documentation designers recognize that most user documentation is now produced from managed re-use of previously developed information (single-source documentation), adapted for new software versions or presentation in various on-screen and printed media. While this International Standard does not describe how to set up a content management system (CMS), it is applicable for documentation organizations practicing single-source documentation.This International Standard is independent of the software tools that may be used to produce documentation, and applies to both printed documentation and on-screen documentation. Much of its guidance is applicable to user documentation for systems including hardware as well as software user documentation.This International Standard conforms to ISO/IEC 12207:2008 as an implementation of subclause 7.2.1, Software Documentation Management Process, for software user documentation. This International Standard may be used as a conformance or a guidance document for documentation products, projects, and organizations claiming conformance to ISO/IEC 15288:2008 or to ISO/IEC 12207:2008.The primary sources for this International Standard are previous standards IEEE Std 1063-2001, IEEE standard for software user documentation, and ISO/IEC 18019:2004, Software and system engineering — Guidelines for the design and preparation of user documentation for application software.","1   ScopeThis clause presents the scope, purpose, organization, and candidate uses of this International Standard.This International Standard supports the interest of software users in consistent, complete, accurate, and usable documentation. It includes both approaches to standardization: a) process standards, which specify the way in which documentation products are to be developed; and b) documentation product standards, which specify the characteristics and functional requirements of the documentation.The first part of this International Standard covers the user documentation process for designers and developers of documentation. It describes how to establish what information users need, how to determine the way in which that information should be presented to the users, and how to prepare the information and make it available. It is not limited to the design and development phase of the life cycle, but includes activities throughout the information management and documentation processes.The second part of this International Standard provides minimum requirements for the structure, information content, and format of user documentation, including both printed and on-screen documents used in the work environment by users of systems containing software. It applies to printed user manuals, online help, tutorials, and user reference documentation.This International Standard neither encourages nor discourages the use of either printed or electronic (on-screen) media for documentation, or of particular documentation development or management tools or methodologies.This International Standard may be helpful for developing the following types of documentation, although it does not cover all aspects of them:• documentation of products other than software;• multimedia systems using animation, video, and sound;• computer-based training (CBT) packages and specialized course materials intended primarily for use in formal training programs;• documentation produced for installers, computer operators, or system administrators who are not end users;• maintenance documentation describing the internal operation of systems software;• documentation incorporated into the user interface itself.This International Standard is applicable to documentation designers and developers, including a variety of specialists:• information designers and architects who plan the structure and format of documentation products in a documentation set;• usability specialists and business analysts who identify the tasks that the intended users will perform with the software;• those who develop and edit the written content for user documentation;• graphic designers with expertise in electronic media;• user interface designers and ergonomics experts working together to design the presentation of the documentation on the screen.This International Standard may also be consulted by those with other roles and interests in the documentation process:• managers of the software development process or the documentation process;• acquirers of documentation prepared by suppliers;• usability testers, documentation reviewers, subject-matter experts;• developers of tools for creating on-screen documentation;• human-factors experts who identify principles for making documentation more accessible and easily used.This International Standard is intended for use in all types of organizations, whether or not a dedicated documentation department is present, and may be used as a basis for local standards and procedures. Readers are assumed to have experience or knowledge of software development or documentation development processes.Users of this International Standard should adopt a style manual for use within their own organizations to complement the guidance provided in the annexes to this International Standard, or adopt an industry-recognized style guide. Annex A provides guidance for the content of a style guide, and Annexes B and C provide guidance on style.The order of clauses in this International Standard does not imply that the documentation should be developed in this order or presented to the user in this order.In each clause, the requirements are media-independent, as far as possible. Requirements specific to either print or electronic media are identified as such, particularly in Clause 12. Annex D provides guidance for the design of printed documentation.The checklists in Annex E may be used at each phase of the documentation process to check that the appropriate steps have been carried out and that the finished documentation satisfies quality criteria.The checklists in Annexes F and G may be used to track conformance with the requirements of this International Standard for documentation processes and products.The bibliography lists works that provide guidance on the processes of managing, preparing, and testing user documentation.",ISO/IEC 26514:2008(en)
40,"Algorithm data package refers to computer software, executable on a Linux, Unix, or Windows-based computer system, that implements the APID algorithm and contains:o   An executable binary file that contains the applications main entry point and any code that was statically linked to the application target;o   Resource files and software libraries necessary for the execution of the executable file; ando   Computer software documentation corresponding to the use, operation, and maintenance of the application.Application Program Interface refers to computer software documentation describing how software components may interact with other software components. It details, in human-readable format, the expected behavior of software libraries. It may contain the set of public methods, variables used as arguments, and return values for public methods. It shall contain a set of protocols, routines, and tools necessary to build application software interacting with the computer software for which the API was written. ",Systems and software engineering — Requirements for managers of user documentation,https://www.iso.org/obp/ui/#!iso:std:43070:en,"IntroductionEffective management of the software user documentation tasks is essential in order to ensure that documentation is usable, accurate, delivered when needed by the users, produced efficiently, and maintained consistent with the software. This International Standard addresses the management of user documentation in relation to both initial development and subsequent releases of the software and user documentation.Anyone who uses application software needs accurate information about how the software will help the user accomplish a task. The documentation can be the first tangible item that the user sees, and if so, it can influence the user’s first impressions of the product. If the information is supplied in a convenient form and is easy to find and understand, the users can quickly become proficient at using the product. Hence, a well-managed documentation process not only assists the user and helps to reduce the cost of training and support, but also enhances the reputation of the product, its producer, and its suppliers.Although many software designers aim to have user interfaces that behave so intuitively that very little separate documentation is needed, this approach is rarely possible in practice. User documentation is an essential component of usable software products.Documentation is often regarded as something done after the software has been implemented. However, for quality software documentation, its development should be regarded as an integral part of the software life-cycle process from the planning and design stages onwards. If done properly, documentation or information management is a big enough job to require process planning in its own right.This International Standard was developed to assist users of ISO/IEC 15288:2008 (IEEE Std 15288-2008), Systems and software engineering — System life cycle processes, or ISO/IEC 12207:2008 (IEEE Std 12207-2008), Systems and software engineering — Software life cycle processes, to manage software user documentation as part of the software life cycle. This International Standard defines the documentation process from the manager's standpoint. It was developed to assist those who provide input to, perform, and evaluate user documentation management.NOTE: Other International Standards in the ISO/IEC 265NN family address the documentation and information management processes from the viewpoint of documentation designers/developers, testers and reviewers, and acquirers and suppliers.This International Standard applies to people or organizations producing suites of documentation, to those undertaking a single documentation project, and for documentation produced internally as well as to documentation contracted to outside service organizations. Beyond the development and production of a user manual, help system, or set of documentation for a single software product, it applies to a broader range of documentation management situations, including user documentation for those who install, implement, administer, and operate software for end users. Frequently, user documentation managers are responsible for the development and reuse of information (content management) for:• multiple updates of user documentation as the software version is updated;• multiple reuses or adaptations of information to support related software products;• multiple translated or localized versions of user documentation;• a portfolio of unrelated documentation projects being managed concurrently within an organization.This International Standard is not intended to advocate the use of either printed or electronic (on-screen) media for documentation, or of any particular information management, content management, documentation testing, or project management tools or protocols. The requirements are media-independent, as far as possible. This International Standard may be applied to user documentation for systems including software as well as to software user documentation.","1   ScopeThis International Standard supports the needs of software users for consistent, complete, accurate, and usable documentation. It provides requirements for strategy, planning, performance, and control for documentation managers. It specifies procedures for managing user documentation throughout the software life cycle. It also includes requirements for key documents produced for user documentation management, including documentation plans and documentation management plans.This International Standard provides an overview of the software documentation and information management processes which are specialized for user documentation in this International Standard. It also presents aspects of portfolio planning and content management for user documentation. Specifically, it addresses the following:• management requirements in starting a project, including setting up procedures and specifications, establishing infrastructure, and building a team, with examples of roles needed on a user documentation team;• measurements and estimates needed for management control;• the application of management control to user documentation work;• the use of supporting processes such as change management, schedule and cost control, resource management, quality management and process improvement.The works listed in the Bibliography provide guidance on the processes of managing, preparing, and testing user documentation.NOTE 1: Related standards of value to documentation managers and others involved in the process include ISO/IEC 26514:2008, Systems and software engineering — Requirements for designers and developers of user documentation (also available as IEEE Std 26514-2010, IEEE Standard for Adoption of ISO/IEC 26514:2008, Systems and Software Engineering — Requirements for Designers and Developers of User Documentation); ISO/IEC 26513:2009, Systems and software engineering — Requirements for testers and reviewers of user documentation (also available as IEEE Std 26513-2010, IEEE Standard for Adoption of ISO/IEC 26513:2009, Systems and Software Engineering — Requirements for Testers and Reviewers of User Documentation); and ISO/IEC/IEEE 26512:2011, Systems and software engineering — Requirements for acquirers and suppliers of user documentation.This International Standard is applicable for use by managers of user documentation projects or organizations with information designers and documentation developers. This International Standard may also be consulted by those with other roles and interests in the documentation process:• managers of the software development process;• acquirers of documentation prepared by suppliers;• experienced writers who develop the written content for user documentation;• developers of tools for creating on-screen documentation;• human-factors experts who identify principles for making documentation more accessible and easily used;• graphic designers with expertise in electronic media;• user interface designers and ergonomics experts working together to design the presentation of the documentation on the screen.This International Standard may be applied to manage the following types of documentation, although it does not cover all aspects of them:• documentation for user assistance, training, marketing, and systems documentation for product design and development, based on reuse of user documentation topics;• documentation of products other than software;• multimedia marketing presentations using animation, video, and sound;• computer-based training (CBT) packages and specialized course materials intended primarily for use in formal training programs;• maintenance documentation describing the internal operation of systems software.NOTE 2: ISO/IEC/IEEE 15289:2011 provides more detailed content for life-cycle process information items (documentation).",ISO/IEC/IEEE 26511:2011(en)
41,"Algorithm data package refers to computer software, executable on a Linux, Unix, or Windows-based computer system, that implements the APID algorithm and contains:o   An executable binary file that contains the applications main entry point and any code that was statically linked to the application target;o   Resource files and software libraries necessary for the execution of the executable file; ando   Computer software documentation corresponding to the use, operation, and maintenance of the application.Application Program Interface refers to computer software documentation describing how software components may interact with other software components. It details, in human-readable format, the expected behavior of software libraries. It may contain the set of public methods, variables used as arguments, and return values for public methods. It shall contain a set of protocols, routines, and tools necessary to build application software interacting with the computer software for which the API was written. ","Systems and software engineering — Content management for product life-cycle, user and service management documentation",https://www.iso.org/obp/ui/#!iso:std:43090:en,,,ISO/IEC/IEEE 26531:2015(en)
42,"Algorithm data package refers to computer software, executable on a Linux, Unix, or Windows-based computer system, that implements the APID algorithm and contains:o   An executable binary file that contains the applications main entry point and any code that was statically linked to the application target;o   Resource files and software libraries necessary for the execution of the executable file; ando   Computer software documentation corresponding to the use, operation, and maintenance of the application.Application Program Interface refers to computer software documentation describing how software components may interact with other software components. It details, in human-readable format, the expected behavior of software libraries. It may contain the set of public methods, variables used as arguments, and return values for public methods. It shall contain a set of protocols, routines, and tools necessary to build application software interacting with the computer software for which the API was written. ",Systems and software engineering — Requirements for acquirers and suppliers of user documentation,https://www.iso.org/obp/ui/#!iso:std:43071:en,"IntroductionThis International Standard was developed to assist users of ISO/IEC 15288:2008 (IEEE Std 15288-2008) or ISO/IEC 12207:2008 (IEEE Std 12207-2008) to acquire or supply software user documentation and documentation services as part of the software life cycle processes. It defines the documentation process from the acquirer’s standpoint and the supplier’s standpoint. The accurate description of the requirements for user documentation is essential in order to ensure that the documentation meets the needs of its users. This International Standard addresses the identification, definition, and fulfillment of user documentation requirements as part of the acquisition and supply processes.This International Standard covers the requirements for information items used in the acquisition of user documentation products: the Acquisition Plan, Document Specification, Statement of Work, Request for Proposals, and the Proposal. It also discusses the use of a Documentation Management Plan and a Document Plan as they arise in the acquisition and supply processes.This International Standard is independent of the software tools that can be used to produce documentation, and applies to both printed documentation and on-screen documentation. Much of its guidance is applicable to user documentation for systems including hardware as well as software.Earlier standards tended to view the results of the documentation process as a single book or multi-volume set: a one-time deliverable. Increasingly, documentation acquirers and suppliers recognize that most user documentation is now produced from managed re-use of previously developed information (single-source documentation) adapted for new software versions, or presentation in various on-screen and printed media. While this International Standard does not describe how to set up a content management system, it is applicable for documentation organizations practising single-source documentation, as well as for acquirers and suppliers of one-time deliverables.Anyone who uses application software needs accurate information about how the software will help the user accomplish a task. The documentation can be the first tangible item that the user sees, and so influences the user’s first impressions of the software product. If the information is supplied in a convenient form and is easy to find and understand, the user can quickly become proficient at using the product. Therefore, well-designed documentation not only assists the user and helps to reduce the cost of training and support, but also enhances the reputation of the product, its producer, and its suppliers.Although software developers intend to design user interfaces that behave so intuitively that very little separate documentation is needed, this is rarely possible. Today’s software offers increasingly robust functionality, not only within applications, but also across applications which intelligently exchange information with one another. Further, most software includes underlying rules and calculations, or algorithms that affect the results a user can obtain when using the software. These underlying programming mechanics are discernable by users, but only through laborious testing. For these and other reasons, user documentation remains an essential component of usable software products.Documentation is often regarded as something done after the software has been implemented. However, for high-quality software documentation, its development needs to be regarded as an integral part of the software life cycle. In fact, quality documentation or information management services are important enough to require specific planning.Related standards for those acquiring and supplying software user documentation include ISO/IEC 26514:2008 (IEEE Std 26514-2010), Systems and software engineering — Requirements for designers and developers of user documentation, and ISO/IEC 26513:2009 (IEEE Std 26513-2010), Systems and software engineering — Requirements for testers and reviewers of user documentation. Other International Standards are in preparation or planned to address the documentation and information management processes from the viewpoint of managers and agile projects.This International Standard is consistent with ISO/IEC 12207:2008 (IEEE Std 12207-2008) as an implementation of the Acquisition and Supply processes, which comprise the Agreement processes, and of the Information Management and Software Documentation Management Processes.This International Standard is intended for use in all types of organizations, whether they have a dedicated documentation department or not. It can be used as a basis for local standards and procedures. Readers are assumed to have experience or knowledge of general agreement processes for acquisition and supply of products and services.The order of clauses in this International Standard does not imply that the acquisition activities need to be performed in this order, nor that documentation needs to be developed in this order or presented to the user in this order.In each clause, the requirements are media-independent, as far as possible.The checklists in Annexes A and B can be used to track conformance with the requirements of this International Standard for acquirers and suppliers of documentation products.The Bibliography contains references to source material used in the development of this International Standard, as well as sources of additional information that might be useful to acquirers and suppliers.","1   ScopeThis International Standard supports the interest of software users in having consistent, complete, accurate, and usable documentation. It addresses both available approaches to standardization: a) process standards, which specify the way that documentation products are to be acquired and supplied; and b) documentation product standards, which specify the characteristics and functional requirements of the documentation.As defined in ISO/IEC 12207:2008 (IEEE Std 12207-2008) and ISO/IEC 15288:2008 (IEEE Std 15288-2008), the acquisition and supply activities comprise the agreement processes of the software life cycle. Acquisition and supply of user documentation and user documentation services are specializations of those processes. User documentation services can be acquired and supplied for any part of the software documentation management or information management process, such as:• documentation or information management;• information design and development;• documentation editing and review coordination;• documentation testing, particularly usability testing;• documentation production and packaging;• documentation distribution and delivery;• advice on the selection and implementation of documentation tools and supporting systems;• documentation process improvement.This International Standard provides an overview of the software user documentation and information management processes which may require acquisition and supply of software user documentation products and services. It applies the Agreement processes (acquisition and supply) to software user documentation, and addresses the preparation of requirements for software user documentation. These requirements are central to the user documentation specification and Statement of Work discussed in this International Standard. This International Standard also addresses requirements for primary document outputs of the acquisition and supply process: the Request for Proposal and the Proposal for user documentation products and services.This International Standard is intended for use in acquiring or supplying either printed or electronic (on-screen) documentation. It is independent of documentation development or management tools or methodologies.This International Standard might be helpful for acquiring and supplying the following types of documentation, although it does not cover all aspects of them:• documentation of products other than software;• multimedia systems using animation, video, and sound;• computer-based training (CBT) packages and specialized course materials intended primarily for use in formal training programs;• maintenance documentation describing the internal operation of systems software;• documentation incorporated into the user interface.This International Standard is applicable to acquirers and suppliers of user documentation, including a variety of specialists:• business analysts who identify the tasks that the intended users will perform with the software;• managers of the software development process or the documentation process;• managers of the acquisition process, and those who authorize and approve acquisitions;• managers and authors involved in proposal preparation.It can also be consulted by those with other roles and interests in the documentation process:• information designers and architects who plan the structure, format, and content requirements of documentation products in a documentation set or web-accessible suite;• experienced authors and editors who develop the written content for user documentation;• graphic designers with expertise in electronic media;• user interface designers and ergonomics experts working together to design the presentation of the documentation on the screen;• usability testers, documentation reviewers, subject matter experts;• developers of tools for creating on-screen documentation.",ISO/IEC/IEEE 26512:2011(en)
43,"Algorithm data package refers to computer software, executable on a Linux, Unix, or Windows-based computer system, that implements the APID algorithm and contains:o   An executable binary file that contains the applications main entry point and any code that was statically linked to the application target;o   Resource files and software libraries necessary for the execution of the executable file; ando   Computer software documentation corresponding to the use, operation, and maintenance of the application.Application Program Interface refers to computer software documentation describing how software components may interact with other software components. It details, in human-readable format, the expected behavior of software libraries. It may contain the set of public methods, variables used as arguments, and return values for public methods. It shall contain a set of protocols, routines, and tools necessary to build application software interacting with the computer software for which the API was written. ",Information technology — Programming languages — Ada,https://www.iso.org/obp/ui/#!iso:std:61507:en,"IntroductionDesign GoalsAda was originally designed with three overriding concerns: program reliability and maintenance, programming as a human activity, and efficiency. The 1995 revision to the language was designed to provide greater flexibility and extensibility, additional control over storage management and synchronization, and standardized packages oriented toward supporting important application areas, while at the same time retaining the original emphasis on reliability, maintainability, and efficiency. This third edition provides further flexibility and adds more standardized packages within the framework provided by the 1995 revision.The need for languages that promote reliability and simplify maintenance is well established. Hence emphasis was placed on program readability over ease of writing. For example, the rules of the language require that program variables be explicitly declared and that their type be specified. Since the type of a variable is invariant, compilers can ensure that operations on variables are compatible with the properties intended for objects of the type. Furthermore, error-prone notations have been avoided, and the syntax of the language avoids the use of encoded forms in favor of more English-like constructs. Finally, the language offers support for separate compilation of program units in a way that facilitates program development and maintenance, and which provides the same degree of checking between units as within a unit.Concern for the human programmer was also stressed during the design. Above all, an attempt was made to keep to a relatively small number of underlying concepts integrated in a consistent and systematic way while continuing to avoid the pitfalls of excessive involution. The design especially aims to provide language constructs that correspond intuitively to the normal expectations of users.Like many other human activities, the development of programs is becoming ever more decentralized and distributed. Consequently, the ability to assemble a program from independently produced software components continues to be a central idea in the design. The concepts of packages, of private types, and of generic units are directly related to this idea, which has ramifications in many other aspects of the language. An allied concern is the maintenance of programs to match changing requirements; type extension and the hierarchical library enable a program to be modified while minimizing disturbance to existing tested and trusted components.No language can avoid the problem of efficiency. Languages that require over-elaborate compilers, or that lead to the inefficient use of storage or execution time, force these inefficiencies on all machines and on all programs. Every construct of the language was examined in the light of present implementation techniques. Any proposed construct whose implementation was unclear or that required excessive machine resources was rejected.Language SummaryAn Ada program is composed of one or more program units. Program units may be subprograms (which define executable algorithms), packages (which define collections of entities), task units (which define concurrent computations), protected units (which define operations for the coordinated sharing of data between tasks), or generic units (which define parameterized forms of packages and subprograms). Each program unit normally consists of two parts: a specification, containing the information that must be visible to other units, and a body, containing the implementation details, which need not be visible to other units. Most program units can be compiled separately.This distinction of the specification and body, and the ability to compile units separately, allows a program to be designed, written, and tested as a set of largely independent software components.An Ada program will normally make use of a library of program units of general utility. The language provides means whereby individual organizations can construct their own libraries. All libraries are structured in a hierarchical manner; this enables the logical decomposition of a subsystem into individual components. The text of a separately compiled program unit must name the library units it requires.Program UnitsA subprogram is the basic unit for expressing an algorithm. There are two kinds of subprograms: procedures and functions. A procedure is the means of invoking a series of actions. For example, it may read data, update variables, or produce some output. It may have parameters, to provide a controlled means of passing information between the procedure and the point of call. A function is the means of invoking the computation of a value. It is similar to a procedure, but in addition will return a result.A package is the basic unit for defining a collection of logically related entities. For example, a package can be used to define a set of type declarations and associated operations. Portions of a package can be hidden from the user, thus allowing access only to the logical properties expressed by the package specification.Subprogram and package units may be compiled separately and arranged in hierarchies of parent and child units giving fine control over visibility of the logical properties and their detailed implementation.A task unit is the basic unit for defining a task whose sequence of actions may be executed concurrently with those of other tasks. Such tasks may be implemented on multicomputers, multiprocessors, or with interleaved execution on a single processor. A task unit may define either a single executing task or a task type permitting the creation of any number of similar tasks.A protected unit is the basic unit for defining protected operations for the coordinated use of data shared between tasks. Simple mutual exclusion is provided automatically, and more elaborate sharing protocols can be defined. A protected operation can either be a subprogram or an entry. A protected entry specifies a Boolean expression (an entry barrier) that must be True before the body of the entry is executed. A protected unit may define a single protected object or a protected type permitting the creation of several similar objects.Declarations and StatementsThe body of a program unit generally contains two parts: a declarative part, which defines the logical entities to be used in the program unit, and a sequence of statements, which defines the execution of the program unit.The declarative part associates names with declared entities. For example, a name may denote a type, a constant, a variable, or an exception. A declarative part also introduces the names and parameters of other nested subprograms, packages, task units, protected units, and generic units to be used in the program unit.The sequence of statements describes a sequence of actions that are to be performed. The statements are executed in succession (unless a transfer of control causes execution to continue from another place).An assignment statement changes the value of a variable. A procedure call invokes execution of a procedure after associating any actual parameters provided at the call with the corresponding formal parameters.Case statements and if statements allow the selection of an enclosed sequence of statements based on the value of an expression or on the value of a condition.The loop statement provides the basic iterative mechanism in the language. A loop statement specifies that a sequence of statements is to be executed repeatedly as directed by an iteration scheme, or until an exit statement is encountered.A block statement comprises a sequence of statements preceded by the declaration of local entities used by the statements.Certain statements are associated with concurrent execution. A delay statement delays the execution of a task for a specified duration or until a specified time. An entry call statement is written as a procedure call statement; it requests an operation on a task or on a protected object, blocking the caller until the operation can be performed. A called task may accept an entry call by executing a corresponding accept statement, which specifies the actions then to be performed as part of the rendezvous with the calling task. An entry call on a protected object is processed when the corresponding entry barrier evaluates to true, whereupon the body of the entry is executed. The requeue statement permits the provision of a service as a number of related activities with preference control. One form of the select statement allows a selective wait for one of several alternative rendezvous. Other forms of the select statement allow conditional or timed entry calls and the asynchronous transfer of control in response to some triggering event.Execution of a program unit may encounter error situations in which normal program execution cannot continue. For example, an arithmetic computation may exceed the maximum allowed value of a number, or an attempt may be made to access an array component by using an incorrect index value. To deal with such error situations, the statements of a program unit can be textually followed by exception handlers that specify the actions to be taken when the error situation arises. Exceptions can be raised explicitly by a raise statement.Data TypesEvery object in the language has a type, which characterizes a set of values and a set of applicable operations. The main classes of types are elementary types (comprising enumeration, numeric, and access types) and composite types (including array and record types).An enumeration type defines an ordered set of distinct enumeration literals, for example a list of states or an alphabet of characters. The enumeration types Boolean, Character, Wide_Character, and Wide_Wide_Character are predefined.Numeric types provide a means of performing exact or approximate numerical computations. Exact computations use integer types, which denote sets of consecutive integers. Approximate computations use either fixed point types, with absolute bounds on the error, or floating point types, with relative bounds on the error. The numeric types Integer, Float, and Duration are predefined.Composite types allow definitions of structured objects with related components. The composite types in the language include arrays and records. An array is an object with indexed components of the same type. A record is an object with named components of possibly different types. Task and protected types are also forms of composite types. The array types String, Wide_String, and Wide_Wide_String are predefined.Record, task, and protected types may have special components called discriminants which parameterize the type. Variant record structures that depend on the values of discriminants can be defined within a record type.Access types allow the construction of linked data structures. A value of an access type represents a reference to an object declared as aliased or to an object created by the evaluation of an allocator. Several variables of an access type may designate the same object, and components of one object may designate the same or other objects. Both the elements in such linked data structures and their relation to other elements can be altered during program execution. Access types also permit references to subprograms to be stored, passed as parameters, and ultimately dereferenced as part of an indirect call.Private types permit restricted views of a type. A private type can be defined in a package so that only the logically necessary properties are made visible to the users of the type. The full structural details that are externally irrelevant are then only available within the package and any child units.From any type a new type may be defined by derivation. A type, together with its derivatives (both direct and indirect) form a derivation class. Class-wide operations may be defined that accept as a parameter an operand of any type in a derivation class. For record and private types, the derivatives may be extensions of the parent type. Types that support these object-oriented capabilities of class-wide operations and type extension must be tagged, so that the specific type of an operand within a derivation class can be identified at run time. When an operation of a tagged type is applied to an operand whose specific type is not known until run time, implicit dispatching is performed based on the tag of the operand.Interface types provide abstract models from which other interfaces and types may be composed and derived. This provides a reliable form of multiple inheritance. Interface types may also be implemented by task types and protected types thereby enabling concurrent programming and inheritance to be merged.The concept of a type is further refined by the concept of a subtype, whereby a user can constrain the set of allowed values of a type. Subtypes can be used to define subranges of scalar types, arrays with a limited set of index values, and records and private types with particular discriminant values.Other FacilitiesAspect clauses can be used to specify the mapping between types and features of an underlying machine. For example, the user can specify that objects of a given type must be represented with a given number of bits, or that the components of a record are to be represented using a given storage layout. Other features allow the controlled use of low level, nonportable, or implementation-dependent aspects, including the direct insertion of machine code.The predefined environment of the language provides for input-output and other capabilities by means of standard library packages. Input-output is supported for values of user-defined as well as of predefined types. Standard means of representing values in display form are also provided.The predefined standard library packages provide facilities such as string manipulation, containers of various kinds (vectors, lists, maps, etc.), mathematical functions, random number generation, and access to the execution environment.The specialized annexes define further predefined library packages and facilities with emphasis on areas such as real-time scheduling, interrupt handling, distributed systems, numerical computation, and high-integrity systems.Finally, the language provides a powerful means of parameterization of program units, called generic program units. The generic parameters can be types and subprograms (as well as objects and packages) and so allow general algorithms and data structures to be defined that are applicable to all types of a given class.Language ChangesThis International Standard replaces the second edition of 1995. It modifies the previous edition by making changes and additions that improve the capability of the language and the reliability of programs written in the language. This edition incorporates the changes from Amendment 1 (ISO/IEC 8652:1995:AMD 1:2007), which were designed to improve the portability of programs, interfacing to other languages, and both the object-oriented and real-time capabilities.Significant changes originating in Amendment 1 are incorporated:• Support for program text is extended to cover the entire ISO/IEC 10646:2003 repertoire. Execution support now includes the 32-bit character set. See subclauses 2.1, 3.5.2, 3.6.3, A.1, A.3, and A.4.• The object-oriented model has been improved by the addition of an interface facility which provides multiple inheritance and additional flexibility for type extensions. See subclauses 3.4, 3.9, and 7.3. An alternative notation for calling operations more akin to that used in other languages has also been added. See subclause 4.1.3.• Access types have been further extended to unify properties such as the ability to access constants and to exclude null values. See clause 3.10. Anonymous access types are now permitted more freely and anonymous access-to-subprogram types are introduced. See subclauses 3.3, 3.6, 3.10, and 8.5.1.• The control of structure and visibility has been enhanced to permit mutually dependent references between units and finer control over access from the private part of a package. See subclauses 3.10.1 and 10.1.2. In addition, limited types have been made more useful by the provision of aggregates, constants, and constructor functions. See subclauses 4.3, 6.5, and 7.5.• The predefined environment has been extended to include additional time and calendar operations, improved string handling, a comprehensive container library, file and directory management, and access to environment variables. See subclauses 9.6.1, A.4, A.16, A.17, and A.18.• Two of the Specialized Needs Annexes have been considerably enhanced:• The Real-Time Systems Annex now includes the Ravenscar profile for high-integrity systems, further dispatching policies such as Round Robin and Earliest Deadline First, support for timing events, and support for control of CPU time utilization. See subclauses D.2, D.13, D.14, and D.15.• The Numerics Annex now includes support for real and complex vectors and matrices as previously defined in ISO/IEC 13813:1997 plus further basic operations for linear algebra. See subclause G.3.• The overall reliability of the language has been enhanced by a number of improvements. These include new syntax which detects accidental overloading, as well as pragmas for making assertions and giving better control over the suppression of checks. See subclauses 6.1, 11.4.2, and 11.5.In addition, this third edition makes enhancements to address two important issues, namely, the particular problems of multiprocessor architectures, and the need to further increase the capabilities regarding assertions for correctness. It also makes additional changes and additions that improve the capability of the language and the reliability of programs written in the language.The following significant changes with respect to the 1995 edition as amended by Amendment 1 are incorporated:• New syntax (the aspect specification) is introduced to enable properties to be specified for various entities in a more structured manner than through pragmas. See subclause 13.1.1.• The concept of assertions introduced in the 2005 edition is extended with the ability to specify preconditions and postconditions for subprograms, and invariants for private types. The concept of constraints in defining subtypes is supplemented with subtype predicates that enable subsets to be specified other than as simple ranges. These properties are all indicated using aspect specifications. See subclauses 3.2.4, 6.1.1, and 7.3.2.• New forms of expressions are introduced. These are if expressions, case expressions, quantified expressions, and expression functions. As well as being useful for programming in general by avoiding the introduction of unnecessary assignments, they are especially valuable in conditions and invariants since they avoid the need to introduce auxiliary functions. See subclauses 4.5.7, 4.5.8, and 6.8. Membership tests are also made more flexible. See subclauses 4.4 and 4.5.2.• A number of changes are made to subprogram parameters. Functions may now have parameters of all modes. In order to mitigate consequent (and indeed existing) problems of inadvertent order dependence, rules are introduced to reduce aliasing. A parameter may now be explicitly marked as aliased and the type of a parameter may be incomplete in certain circumstances. See subclauses 3.10.1, 6.1, and 6.4.1.• The use of access types is now more flexible. The rules for accessibility and certain conversions are improved. See subclauses 3.10.2, 4.5.2, 4.6, and 8.6. Furthermore, better control of storage pools is provided. See subclause 13.11.4.• The Real-Time Systems Annex now includes facilities for defining domains of processors and assigning tasks to them. Improvements are made to scheduling and budgeting facilities. See subclauses D.10.1, D.14, and D.16.• A number of important improvements are made to the standard library. These include packages for conversions between strings and UTF encodings, and classification functions for wide and wide wide characters. Internationalization is catered for by a package giving locale information. See subclauses A.3, A.4.11, and A.19. The container library is extended to include bounded forms of the existing containers and new containers for indefinite objects, multiway trees, and queues. See subclause A.18.• Finally, certain features are added primarily to ease the use of containers, such as the ability to iterate over all elements in a container without having to encode the iteration. These can also be used for iteration over arrays, and within quantified expressions. See subclauses 4.1.5, 4.1.6, 5.5.1, and 5.5.2.Instructions for Comment SubmissionInformal comments on this International Standard may be sent via e-mail to ada-comment@adaauth.org. If appropriate, the Project Editor will initiate the defect correction procedure.Comments should use the following format: !topic Title summarizing comment !reference Ada 2012 RMss.ss(pp) !from Author Name yy-mm-dd !keywords keywords related to topic !discussion     text of discussionwhere ss.ss is the clause or subclause number, pp is the paragraph number where applicable, and yy-mm-dd is the date the comment was sent. The date is optional, as is the !keywords line.Please use a descriptive “Subject” in your e-mail message, and limit each message to a single comment.When correcting typographical errors or making minor wording suggestions, please put the correction directly as the topic of the comment; use square brackets [ ] to indicate text to be omitted and curly braces { } to indicate text to be added, and provide enough context to make the nature of the suggestion self-evident or put additional information in the body of the comment, for example: !topic [c]{C}haracter 
!topic it[']s meaning is not definedFormal requests for interpretations and for reporting defects in this International Standard may be made in accordance with the ISO/IEC JTC 1 Directives and the ISO/IEC JTC 1/SC 22 policy for interpretations. National Bodies may submit a Defect Report to ISO/IEC JTC 1/SC 22 for resolution under the JTC 1 procedures. A response will be provided and, if appropriate, a Technical Corrigendum will be issued in accordance with the procedures.",,ISO/IEC 8652:2012(en)
44,"DHS S&T EXD is interested in high-impact approaches which can be retrofitted into existing EDS baselines as well as innovations in EDS concept development. Recent technological developments which are expected to impact critical metrics such as probability of detection (Pd) and probability of false alarm (Pfa) include new improvised explosive threat signature technology, compressive measurement, coded apertures, new or innovative sources and detectors including advanced detection, and novel classification and reconstruction algorithms. The work outlined here focuses on information theoretic analysis of security data which implements high impact approaches, as well as applying the tools of optimal detection, estimation, pattern recognition, machine learning, and information theory.",Printing specifications for optical character recognition,https://www.iso.org/obp/ui/#!iso:std:6480:en,,"1   Scope and field of applicationThis International Standard contains the basic definitions, measurement requirements, specifications and recommendations for OCR paper and print.Three major parameters of a printed document for OCR media are covered. These are :- the optical properties of the paper to be used;- the optical and dimensional properties of the ink patterns forming OCR characters;- the basic requirements related to the position of OCR characters on the paper.The major factors of each of these areas pertinent to OCR are identified. Definitions of these items are given and bases for measurements are established.Basic specifications applicable to all OCR materials are imposed and recommendations for the implementation of an OCR system are made.",ISO 1831:1980(en)
45,"DHS S&T EXD is interested in high-impact approaches which can be retrofitted into existing EDS baselines as well as innovations in EDS concept development. Recent technological developments which are expected to impact critical metrics such as probability of detection (Pd) and probability of false alarm (Pfa) include new improvised explosive threat signature technology, compressive measurement, coded apertures, new or innovative sources and detectors including advanced detection, and novel classification and reconstruction algorithms. The work outlined here focuses on information theoretic analysis of security data which implements high impact approaches, as well as applying the tools of optimal detection, estimation, pattern recognition, machine learning, and information theory.",Information technology — Biometric data interchange formats — Part 8: Finger pattern skeletal data,https://www.iso.org/obp/ui/#!iso:std:40715:en,"IntroductionWith the interest of implementing interoperable personal biometric recognition systems, this part of ISO/IEC 19794 establishes a data interchange format for pattern-based skeletal fingerprint recognition algorithms. Pattern-based algorithms process sections of biometric images. Pattern-based algorithms have been shown to work well with the demanding, but commercially driven, fingerprint sensor formats such as small-area and swipe sensors.The exchange format defined in this part of ISO/IEC 19794 describes all characteristics of a fingerprint in a small data record. Thus it allows for the extraction of both spectral information (orientation, frequency, phase, etc.) and features (minutiae, core, ridge count, etc.). Transformations like translation and rotation can also be accommodated by the format defined herein.With this part of ISO/IEC 19794 for pattern-based skeletal representation of fingerprints— interoperability among fingerprint recognition vendors based on a small data record is allowed;— proliferation of low-cost commercial fingerprint sensors with limited coverage, dynamic range, or resolution is supported;— a data record that can be used to store biometric information on a variety a storage media (including but not limited to, portable devices and smart cards) is defined;— adoption of biometrics in applications requiring interoperability is encouraged.It is recommended that biometric data protection techniques in ANSI/X9 X9.84 or ISO/IEC 15408 are used to safeguard the biometric data defined herein for confidentiality, integrity and availability.","1   ScopeThis part of ISO/IEC 19794 specifies the interchange format for the exchange of pattern-based skeletal fingerprint recognition data. The data format is generic, in that it may be applied and used in a wide range of application areas where automated fingerprint recognition is involved.",ISO/IEC 19794-8:2006(en)
46,"DHS S&T EXD is interested in high-impact approaches which can be retrofitted into existing EDS baselines as well as innovations in EDS concept development. Recent technological developments which are expected to impact critical metrics such as probability of detection (Pd) and probability of false alarm (Pfa) include new improvised explosive threat signature technology, compressive measurement, coded apertures, new or innovative sources and detectors including advanced detection, and novel classification and reconstruction algorithms. The work outlined here focuses on information theoretic analysis of security data which implements high impact approaches, as well as applying the tools of optimal detection, estimation, pattern recognition, machine learning, and information theory.",Information technology — Automatic identification and data capture techniques — Code 39 bar code symbology specification,https://www.iso.org/obp/ui/#!iso:std:43897:en,"IntroductionThe technology of bar coding is based on the recognition of patterns encoded in bars and spaces of defined dimensions. There are numerous methods of encoding information in bar code form, known as symbologies. Code 39 is one such symbology. The rules defining the translation of characters into bar and space patterns and other essential features are known as the symbology specification.In the past, symbology specifications were developed and published by a number of organizations, resulting in certain instances in conflicting requirements for certain symbologies.Manufacturers of bar code equipment and users of bar code technology require publicly available standard symbology specifications to which they can refer when developing equipment and application standards.","1   ScopeThis International Standard specifies the requirements for the bar code symbology known as Code 39; it specifies Code 39 symbology characteristics, data character encodation, dimensions, tolerances, decoding algorithms and parameters to be defined by applications. It specifies the Symbology Identifier prefix strings for Code 39 symbols.",ISO/IEC 16388:2007(en)
47,"DHS S&T EXD is interested in high-impact approaches which can be retrofitted into existing EDS baselines as well as innovations in EDS concept development. Recent technological developments which are expected to impact critical metrics such as probability of detection (Pd) and probability of false alarm (Pfa) include new improvised explosive threat signature technology, compressive measurement, coded apertures, new or innovative sources and detectors including advanced detection, and novel classification and reconstruction algorithms. The work outlined here focuses on information theoretic analysis of security data which implements high impact approaches, as well as applying the tools of optimal detection, estimation, pattern recognition, machine learning, and information theory.",Ergonomics of human-system interaction — Part 210: Human-centred design for interactive systems,https://www.iso.org/obp/ui/#!iso:std:52075:en,"IntroductionHuman-centred design is an approach to interactive systems development that aims to make systems usable and useful by focusing on the users, their needs and requirements, and by applying human factors/ergonomics, and usability knowledge and techniques. This approach enhances effectiveness and efficiency, improves human well-being, user satisfaction, accessibility and sustainability; and counteracts possible adverse effects of use on human health, safety and performance.There is a substantial body of human factors/ergonomics and usability knowledge about how human-centred design can be organized and used effectively. This part of ISO 9241 aims to make this information available to help those responsible for managing hardware and software design and re-design processes to identify and plan effective and timely human-centred design activities.The human-centred approach to design described in this part of ISO 9241 complements existing systems design approaches. It can be incorporated in approaches as diverse as object-oriented, waterfall and rapid application development.The principles of human-centred design and the related activities have not changed substantially since ISO 13407 was produced and have been validated by ten years of application. This part of ISO 9241 reflects this by making requirements as well as recommendations.","1   ScopeThis part of ISO 9241 provides requirements and recommendations for human-centred design principles and activities throughout the life cycle of computer-based interactive systems. It is intended to be used by those managing design processes, and is concerned with ways in which both hardware and software components of interactive systems can enhance human–system interaction.NOTE 1 Computer-based interactive systems vary in scale and complexity. Examples include off-the-shelf (shrink-wrap) software products, custom office systems, process control systems, automated banking systems, Web sites and applications, and consumer products such as vending machines, mobile phones and digital television. Throughout this part of ISO 9241, such systems are generally referred to as products, systems or services although, for simplicity, sometimes only one term is used.This part of ISO 9241 provides an overview of human-centred design activities. It does not provide detailed coverage of the methods and techniques required for human-centred design, nor does it address health or safety aspects in detail. Although it addresses the planning and management of human-centred design, it does not address all aspects of project management.The information in this part of ISO 9241 is intended for use by those responsible for planning and managing projects that design and develop interactive systems. It therefore addresses technical human factors and ergonomics issues only to the extent necessary to allow such individuals to understand their relevance and importance in the design process as a whole. It also provides a framework for human factors and usability professionals involved in human-centred design. Detailed human factors/ergonomics, usability and accessibility issues are dealt with more fully in a number of standards including other parts of ISO 9241 (see Annex A) and ISO 6385, which sets out the broad principles of ergonomics.The requirements and recommendations in this part of ISO 9241 can benefit all parties involved in human-centred design and development. Annex B provides a checklist that can be used to support claims of conformance with this part of ISO 9241.NOTE 2 Annex A and the Bibliography contain information about relevant related standards.",ISO 9241-210:2010(en)
48,"Preliminary Design Summary _ The contractor shall proceed with the development of a preliminary design based on the outcome of the System Concept Review.  The contractor shall begin to refine the system specifications down to the software module level.  All software modules shall be described functionally with clearly defined inputs and outputs. The contractor shall complete a preliminary design and hold a Preliminary Design Review (PDR) within six (6) months of the contract award date.Subtask 2.1 _ System Concept Review (SCR) _ At the beginning of the contract the contractor shall have a requirements review with the DHS COR to establish the set of weapons to focus on for the algorithm development and to establish detection and false alarm goals. The contractor shall provide the prototype algorithm results at the time of the SCR review so that the decision of weapon priorities can factor in how difficult each weapon is to detect. The contractor shall coordinate with DHS to schedule a SCR meeting. The contractor shall prepare and submit to the DHS COR the proposed agenda and preliminary algorithm results ten (10) business days prior to the meeting and an update at the meeting. The contractor shall prepare and submit the minutes of the meeting to the DHS S&T COR within five (5) business days after the meeting.Subtask 2.2 _ Data CollectionSubtask 2.2.1 _ Obtain Items _ Based on the discussion with DHS from the system concept review subtask, a list of prohibited items to be used for data collection will be defined. The items will be purchased by the contractor for scanning at Transportation Security Laboratory (TSL) in Atlantic City, NJ or an alternate site.Subtask 2.2.2 _ Data Collection Plan _ Based on the discussion with DHS from the system concept review subtasks, the contractor shall prepare a data collection plan. This plan shall include key image features that are to be collected as well as the methodology for scanning each prohibited item. This includes planned concealment strategies and bag configurations. The contractor shall also prepare an outline for the data collection report describing data to be provided after the data collection subtasks (subtasks 2.2.3, 2.2.4, 2.2.5) have been completed.Subtask 2.2.3 _ Collect Data _ The contractor will conduct the data collection at the contractor site and offsite as necessary per the data collection plan. Offsite collection is assumed at the TSL. The offsite data collection is planned for one week. The contractor shall ship any necessary equipment, install it for one week of use, and de-install and return the equipment to the contractors facilities at the conclusion of data collection.    Subtask 2.2.4 _ Data Truthing _ The contractor shall review the collected data to identify and mark the threat items.Subtask 2.2.5 _ Prepare and Transmit Data Collection Report _ The Contractor shall document the items scanned and the bag, concealment, and content information. This report shall be delivered electronically to the DHS COR.Subtask 2.3 _ Image Analysis and Feature Identification _ The images collected will be scored using the current algorithm. The images where the threat is not detected will be examined to identify their unique features and to determine the potential strategies for detection.Subtask 2.4 _ PDR _ The contractor shall hold a Preliminary Design Review (PDR), within six (6) months of contract award. PDR materials shall be delivered ten (10) business days prior to PDR to the DHS S&T COR along with specifications and drawings. Approval of the PDR deliverables by the DHS S&T COR will be required prior to PDR. As part of the PDR deliverables, the contractor shall propose a system prototype baseline to include:",Information technology — Security techniques — Cryptographic algorithms and security mechanisms conformance testing,https://www.iso.org/obp/ui/#!iso:std:62286:en,"IntroductionThis document describes cryptographic algorithms and security mechanisms conformance testing methods.The purpose of this document is to address conformance testing methods of cryptographic algorithms and security mechanisms implemented in a cryptographic module. This will allow a complete security evaluation of both the cryptographic module and the implemented cryptographic algorithms and security mechanisms.This document is related to ISO/IEC 19790 and ISO/IEC 24759. ISO/IEC 19790 specifies the security requirements for cryptographic modules. At a minimum, a cryptographic module implements at least one approved security function (i.e., cryptographic algorithm or security mechanism). ISO/IEC 24759 addresses the test requirements for each of the security requirements in ISO/IEC 19790. However, ISO/IEC 24759 does not address test methods for cryptographic algorithms and security mechanisms conformance testing.","1   ScopeThis document gives guidelines for cryptographic algorithms and security mechanisms conformance testing methods.Conformance testing assures that an implementation of a cryptographic algorithm or security mechanism is correct whether implemented in hardware, software or firmware. It also confirms that it runs correctly in a specific operating environment. Testing can consist of known-answer or Monte Carlo testing, or a combination of test methods. Testing can be performed on the actual implementation or modelled in a simulation environment.This document does not include the efficiency of the algorithms or security mechanisms nor the intrinsic performance. This document focuses on the correctness of the implementation.",ISO/IEC 18367:2016(en)
49,"Preliminary Design Summary _ The contractor shall proceed with the development of a preliminary design based on the outcome of the System Concept Review.  The contractor shall begin to refine the system specifications down to the software module level.  All software modules shall be described functionally with clearly defined inputs and outputs. The contractor shall complete a preliminary design and hold a Preliminary Design Review (PDR) within six (6) months of the contract award date.Subtask 2.1 _ System Concept Review (SCR) _ At the beginning of the contract the contractor shall have a requirements review with the DHS COR to establish the set of weapons to focus on for the algorithm development and to establish detection and false alarm goals. The contractor shall provide the prototype algorithm results at the time of the SCR review so that the decision of weapon priorities can factor in how difficult each weapon is to detect. The contractor shall coordinate with DHS to schedule a SCR meeting. The contractor shall prepare and submit to the DHS COR the proposed agenda and preliminary algorithm results ten (10) business days prior to the meeting and an update at the meeting. The contractor shall prepare and submit the minutes of the meeting to the DHS S&T COR within five (5) business days after the meeting.Subtask 2.2 _ Data CollectionSubtask 2.2.1 _ Obtain Items _ Based on the discussion with DHS from the system concept review subtask, a list of prohibited items to be used for data collection will be defined. The items will be purchased by the contractor for scanning at Transportation Security Laboratory (TSL) in Atlantic City, NJ or an alternate site.Subtask 2.2.2 _ Data Collection Plan _ Based on the discussion with DHS from the system concept review subtasks, the contractor shall prepare a data collection plan. This plan shall include key image features that are to be collected as well as the methodology for scanning each prohibited item. This includes planned concealment strategies and bag configurations. The contractor shall also prepare an outline for the data collection report describing data to be provided after the data collection subtasks (subtasks 2.2.3, 2.2.4, 2.2.5) have been completed.Subtask 2.2.3 _ Collect Data _ The contractor will conduct the data collection at the contractor site and offsite as necessary per the data collection plan. Offsite collection is assumed at the TSL. The offsite data collection is planned for one week. The contractor shall ship any necessary equipment, install it for one week of use, and de-install and return the equipment to the contractors facilities at the conclusion of data collection.    Subtask 2.2.4 _ Data Truthing _ The contractor shall review the collected data to identify and mark the threat items.Subtask 2.2.5 _ Prepare and Transmit Data Collection Report _ The Contractor shall document the items scanned and the bag, concealment, and content information. This report shall be delivered electronically to the DHS COR.Subtask 2.3 _ Image Analysis and Feature Identification _ The images collected will be scored using the current algorithm. The images where the threat is not detected will be examined to identify their unique features and to determine the potential strategies for detection.Subtask 2.4 _ PDR _ The contractor shall hold a Preliminary Design Review (PDR), within six (6) months of contract award. PDR materials shall be delivered ten (10) business days prior to PDR to the DHS S&T COR along with specifications and drawings. Approval of the PDR deliverables by the DHS S&T COR will be required prior to PDR. As part of the PDR deliverables, the contractor shall propose a system prototype baseline to include:",Banking and related financial services — Triple DEA — Modes of operation — Implementation guidelines,https://www.iso.org/obp/ui/#!iso:std:33733:en,"IntroductionIn order to significantly strengthen DEA (Data Encryption Algorithm) and extend its useful lifetime, the use of Triple Data Encryption Algorithm (TDEA) modes of operation has been recommended. These TDEA modes of operation not only provide greatly increased cryptographic protection, but because they are based on DEA, the TDEA learning curve for users and vendors is reduced. Since certain TDEA modes of operation can be made backward compatible with existing DEA modes of operation, the financial community may leverage its investment in standard DEA technology by using TDEA to extend its secure lifetime.Each mode of operation provides different benefits and has different characteristics. The selection, implementation and use of a particular mode of operation is dependent upon the security requirements, risk acceptance posture, and operational needs of the financial institution and are beyond the scope of this Technical Report. This Technical Report is necessary to provide the basis for interoperability between different parties using any of the TDEA modes specified herein, provided that they use the same mode of operation and share the same secret cryptographic key(s).This Technical Report does not replace the Data Encryption Algorithm Standard nor the Triple Data Encryption Algorithm specified in ISO/IEC 18033. DEA is the basis for the TDEA modes of operation. TDEA provides increased security in keeping with advances in computing technology and cryptanalytic techniques. TDEA may be implemented in hardware, software or a combination of hardware and software.This Technical Report provides implementation guidelines for the modes of operation specified in ISO/IEC 10116.It is the responsibility of the financial institution to put overall security procedures in place with the necessary controls to ensure that the process is implemented in a secure manner. Furthermore, the process should be audited to ensure compliance with the procedures.",1   ScopeThis Technical Report provides the user with technical support and details for the safe and efficient implementation of the Triple Data Encryption Algorithm (TDEA) modes of operation for the enhanced cryptographic protection of digital data. The modes of operation described herein are specified for both enciphering and deciphering operations. The modes described in this Technical Report are implementations of the block cipher modes of operation specified in ISO/IEC 10116 using the Triple DEA algorithm (TDEA) specified in ISO/IEC 18033-3.The TDEA modes of operation may be used in both wholesale and retail financial applications. The use of this Technical Report provides the basis for the interoperability of products and facilitates the development of application standards that use the TDEA modes of operation. This Technical Report is intended for use with other ISO standards using DEA.,ISO/TR 19038:2005(en)
50,"Preliminary Design Summary _ The contractor shall proceed with the development of a preliminary design based on the outcome of the System Concept Review.  The contractor shall begin to refine the system specifications down to the software module level.  All software modules shall be described functionally with clearly defined inputs and outputs. The contractor shall complete a preliminary design and hold a Preliminary Design Review (PDR) within six (6) months of the contract award date.Subtask 2.1 _ System Concept Review (SCR) _ At the beginning of the contract the contractor shall have a requirements review with the DHS COR to establish the set of weapons to focus on for the algorithm development and to establish detection and false alarm goals. The contractor shall provide the prototype algorithm results at the time of the SCR review so that the decision of weapon priorities can factor in how difficult each weapon is to detect. The contractor shall coordinate with DHS to schedule a SCR meeting. The contractor shall prepare and submit to the DHS COR the proposed agenda and preliminary algorithm results ten (10) business days prior to the meeting and an update at the meeting. The contractor shall prepare and submit the minutes of the meeting to the DHS S&T COR within five (5) business days after the meeting.Subtask 2.2 _ Data CollectionSubtask 2.2.1 _ Obtain Items _ Based on the discussion with DHS from the system concept review subtask, a list of prohibited items to be used for data collection will be defined. The items will be purchased by the contractor for scanning at Transportation Security Laboratory (TSL) in Atlantic City, NJ or an alternate site.Subtask 2.2.2 _ Data Collection Plan _ Based on the discussion with DHS from the system concept review subtasks, the contractor shall prepare a data collection plan. This plan shall include key image features that are to be collected as well as the methodology for scanning each prohibited item. This includes planned concealment strategies and bag configurations. The contractor shall also prepare an outline for the data collection report describing data to be provided after the data collection subtasks (subtasks 2.2.3, 2.2.4, 2.2.5) have been completed.Subtask 2.2.3 _ Collect Data _ The contractor will conduct the data collection at the contractor site and offsite as necessary per the data collection plan. Offsite collection is assumed at the TSL. The offsite data collection is planned for one week. The contractor shall ship any necessary equipment, install it for one week of use, and de-install and return the equipment to the contractors facilities at the conclusion of data collection.    Subtask 2.2.4 _ Data Truthing _ The contractor shall review the collected data to identify and mark the threat items.Subtask 2.2.5 _ Prepare and Transmit Data Collection Report _ The Contractor shall document the items scanned and the bag, concealment, and content information. This report shall be delivered electronically to the DHS COR.Subtask 2.3 _ Image Analysis and Feature Identification _ The images collected will be scored using the current algorithm. The images where the threat is not detected will be examined to identify their unique features and to determine the potential strategies for detection.Subtask 2.4 _ PDR _ The contractor shall hold a Preliminary Design Review (PDR), within six (6) months of contract award. PDR materials shall be delivered ten (10) business days prior to PDR to the DHS S&T COR along with specifications and drawings. Approval of the PDR deliverables by the DHS S&T COR will be required prior to PDR. As part of the PDR deliverables, the contractor shall propose a system prototype baseline to include:",Information technology — Biometric performance testing and reporting — Part 2: Testing methodologies for technology and scenario evaluation,https://www.iso.org/obp/ui/#!iso:std:41448:en,"IntroductionThis part of ISO/IEC 19795 addresses two specific biometric performance testing methodologies: technology and scenario evaluation. The large majority of biometric tests are of one of these two generic evaluation types. Technology evaluations evaluate enrolment and comparison algorithms by means of previously collected corpuses, while scenario evaluations evaluate sensors and algorithms by processing of samples collected from Test Subjects in real time. The former is intended for generation of large volumes of comparison scores and candidate lists indicative of the fundamental discriminating power of an algorithm. The latter is intended for measurement of performance in modeled environments, inclusive of Test Subject-system interactions.This part of ISO/IEC 19795 builds on requirements and best practices specified in ISO/IEC 19795-1, which addresses specific philosophies and principles that can be applied over a broad range of test conditions.This part of ISO/IEC 19795 is meant to provide biometric system developers, deployers and end users with mechanisms for design, execution and reporting of biometric performance tests in a fashion that allows meaningful benchmarking of biometric performance within and across technologies, usage scenarios and environments.","1   ScopeThis part of ISO/IEC 19795 provides requirements and recommendations on data collection, analysis and reporting specific to two primary types of evaluation: technology evaluation and scenario evaluation.This part of ISO/IEC 19795 specifies requirements in the following areas:— development and full description of protocols for technology and scenario evaluations;— execution and reporting of biometric evaluations reflective of the parameters associated with biometric evaluation types.",ISO/IEC 19795-2:2007(en)
51,"Preliminary Design Summary _ The contractor shall proceed with the development of a preliminary design based on the outcome of the System Concept Review.  The contractor shall begin to refine the system specifications down to the software module level.  All software modules shall be described functionally with clearly defined inputs and outputs. The contractor shall complete a preliminary design and hold a Preliminary Design Review (PDR) within six (6) months of the contract award date.Subtask 2.1 _ System Concept Review (SCR) _ At the beginning of the contract the contractor shall have a requirements review with the DHS COR to establish the set of weapons to focus on for the algorithm development and to establish detection and false alarm goals. The contractor shall provide the prototype algorithm results at the time of the SCR review so that the decision of weapon priorities can factor in how difficult each weapon is to detect. The contractor shall coordinate with DHS to schedule a SCR meeting. The contractor shall prepare and submit to the DHS COR the proposed agenda and preliminary algorithm results ten (10) business days prior to the meeting and an update at the meeting. The contractor shall prepare and submit the minutes of the meeting to the DHS S&T COR within five (5) business days after the meeting.Subtask 2.2 _ Data CollectionSubtask 2.2.1 _ Obtain Items _ Based on the discussion with DHS from the system concept review subtask, a list of prohibited items to be used for data collection will be defined. The items will be purchased by the contractor for scanning at Transportation Security Laboratory (TSL) in Atlantic City, NJ or an alternate site.Subtask 2.2.2 _ Data Collection Plan _ Based on the discussion with DHS from the system concept review subtasks, the contractor shall prepare a data collection plan. This plan shall include key image features that are to be collected as well as the methodology for scanning each prohibited item. This includes planned concealment strategies and bag configurations. The contractor shall also prepare an outline for the data collection report describing data to be provided after the data collection subtasks (subtasks 2.2.3, 2.2.4, 2.2.5) have been completed.Subtask 2.2.3 _ Collect Data _ The contractor will conduct the data collection at the contractor site and offsite as necessary per the data collection plan. Offsite collection is assumed at the TSL. The offsite data collection is planned for one week. The contractor shall ship any necessary equipment, install it for one week of use, and de-install and return the equipment to the contractors facilities at the conclusion of data collection.    Subtask 2.2.4 _ Data Truthing _ The contractor shall review the collected data to identify and mark the threat items.Subtask 2.2.5 _ Prepare and Transmit Data Collection Report _ The Contractor shall document the items scanned and the bag, concealment, and content information. This report shall be delivered electronically to the DHS COR.Subtask 2.3 _ Image Analysis and Feature Identification _ The images collected will be scored using the current algorithm. The images where the threat is not detected will be examined to identify their unique features and to determine the potential strategies for detection.Subtask 2.4 _ PDR _ The contractor shall hold a Preliminary Design Review (PDR), within six (6) months of contract award. PDR materials shall be delivered ten (10) business days prior to PDR to the DHS S&T COR along with specifications and drawings. Approval of the PDR deliverables by the DHS S&T COR will be required prior to PDR. As part of the PDR deliverables, the contractor shall propose a system prototype baseline to include:",Systems and software engineering — Information technology project performance benchmarking framework — Part 4: Guidance for data collection and maintenance,https://www.iso.org/obp/ui/#!iso:std:63700:en,"IntroductionBenchmarking is an activity of comparing “objects of interest” to each other or against a benchmark to evaluate characteristic(s). In the context of the ISO/IEC 29155 series, the “object of interest” is the performance of information technology (IT) project, and the characteristic is a particular aspect of an IT project such as productivity.Benchmarking is one of the fastest-growing techniques in the area of IT project management. Instances of IT project performance benchmarking are initiated and conducted for various reasons. Among the most common reasons are the following:a) the need to compare project productivity between similar industries;b) the need to compare productivity between different project types and technologies;c) the need to find the most effective targets for IT development process improvement;d) the need to compare productivity between different suppliers;e) the need to improve project management maturity;f) the need to improve project estimation capability.Much has been written regarding the trials of establishing IT project performance benchmarking, and statistics bear witness to the high failure rate of measurement and benchmarking programs. The most probable causes for failure have been disappointment in the benchmarking outcomes due to a lack of alignment between the selected measurements and business goals, and the misunderstanding of project level measurements in relation to program and portfolio management levels. When there is no alignment between executed measurements and provided outcomes, unnecessary effort is required from the IT project teams collecting the project data. The result is decreased motivation to continue and institutionalize benchmarking.As is shown in Figure 1, the ISO/IEC 29155 series consists of multiple parts.— ISO/IEC 29155-1 provides the overall framework model for IT project performance benchmarking. It consists of activities and components that are necessary to successfully identify, define, select, apply, and improve benchmarking.— ISO/IEC 29155-2 prescribes the required tasks in individual benchmarking activities that are necessary to execute various activities to conduct and/or support successful benchmarking in an organization.— ISO/IEC 29155-3 prescribes the guidance for reporting processes and contents of typical reports.— ISO/IEC 29155-4 provides guidance for the activities to collect data of IT projects to be entered into and maintained in a benchmarking repository.Further parts might follow.Figure 1
—
IT project performance benchmarking standards overview","1   ScopeThis document provides general requirements and guidance for collecting and maintaining data of information technology (IT) projects and for delivering the benchmarking repository within benchmarking activities of “the IT project performance benchmarking framework” by prescribing the following:a) requirements and guidance for data element definitions;b) requirements and guidance for the data collection and maintenance processes within the benchmarking framework;c) requirements and guidance for maintaining benchmarking repository product and issued benchmarks.This document mainly focuses on three major activities, which are “maintain repository”, “submit IT project data”, and “measure IT project” activities.This document is intended for use by stakeholder(s) of IT project performance benchmarking (e.g. benchmarking user, benchmark provider, benchmarking service provider, and IT project team).NOTE The following are examples of how this document can be used:— by a benchmark provider, to define data elements, collect and maintain IT project data, and provide benchmarking repository product or issued benchmarks;— by a benchmarking analyst, to use benchmarking repository product and/or benchmarks for executing an instance of benchmarking;— by a benchmarking service provider, to utilize benchmarking repository product and/or benchmarks for providing benchmarking services;— by an IT service provider, to define data elements to be measured and/or to be submitted to repository owner.It is out of the scope of this document to prescribe a particular set of data element definitions, formats or contents of the benchmarking repository.",ISO/IEC 29155-4:2016(en)
52,"Preliminary Design Summary _ The contractor shall proceed with the development of a preliminary design based on the outcome of the System Concept Review.  The contractor shall begin to refine the system specifications down to the software module level.  All software modules shall be described functionally with clearly defined inputs and outputs. The contractor shall complete a preliminary design and hold a Preliminary Design Review (PDR) within six (6) months of the contract award date.Subtask 2.1 _ System Concept Review (SCR) _ At the beginning of the contract the contractor shall have a requirements review with the DHS COR to establish the set of weapons to focus on for the algorithm development and to establish detection and false alarm goals. The contractor shall provide the prototype algorithm results at the time of the SCR review so that the decision of weapon priorities can factor in how difficult each weapon is to detect. The contractor shall coordinate with DHS to schedule a SCR meeting. The contractor shall prepare and submit to the DHS COR the proposed agenda and preliminary algorithm results ten (10) business days prior to the meeting and an update at the meeting. The contractor shall prepare and submit the minutes of the meeting to the DHS S&T COR within five (5) business days after the meeting.Subtask 2.2 _ Data CollectionSubtask 2.2.1 _ Obtain Items _ Based on the discussion with DHS from the system concept review subtask, a list of prohibited items to be used for data collection will be defined. The items will be purchased by the contractor for scanning at Transportation Security Laboratory (TSL) in Atlantic City, NJ or an alternate site.Subtask 2.2.2 _ Data Collection Plan _ Based on the discussion with DHS from the system concept review subtasks, the contractor shall prepare a data collection plan. This plan shall include key image features that are to be collected as well as the methodology for scanning each prohibited item. This includes planned concealment strategies and bag configurations. The contractor shall also prepare an outline for the data collection report describing data to be provided after the data collection subtasks (subtasks 2.2.3, 2.2.4, 2.2.5) have been completed.Subtask 2.2.3 _ Collect Data _ The contractor will conduct the data collection at the contractor site and offsite as necessary per the data collection plan. Offsite collection is assumed at the TSL. The offsite data collection is planned for one week. The contractor shall ship any necessary equipment, install it for one week of use, and de-install and return the equipment to the contractors facilities at the conclusion of data collection.    Subtask 2.2.4 _ Data Truthing _ The contractor shall review the collected data to identify and mark the threat items.Subtask 2.2.5 _ Prepare and Transmit Data Collection Report _ The Contractor shall document the items scanned and the bag, concealment, and content information. This report shall be delivered electronically to the DHS COR.Subtask 2.3 _ Image Analysis and Feature Identification _ The images collected will be scored using the current algorithm. The images where the threat is not detected will be examined to identify their unique features and to determine the potential strategies for detection.Subtask 2.4 _ PDR _ The contractor shall hold a Preliminary Design Review (PDR), within six (6) months of contract award. PDR materials shall be delivered ten (10) business days prior to PDR to the DHS S&T COR along with specifications and drawings. Approval of the PDR deliverables by the DHS S&T COR will be required prior to PDR. As part of the PDR deliverables, the contractor shall propose a system prototype baseline to include:",Software engineering — Software product Quality Requirements and Evaluation (SQuaRE) — Common Industry Format (CIF) for usability test reports,https://www.iso.org/obp/ui/#!iso:std:43046:en,"IntroductionUsability of software is a key factor in predicting successful deployment of that soft­ware. Software manufacturers subject software to usability testing at various stages in a product’s development; some companies that purchase software also test prod­ucts for usability before making purchasing decisions. Testing often involves (1) sub­jects who are representative of the target population of users of the software, (2) representative tasks, and (3) measures of efficiency, effectiveness and subjective satisfaction. When this type of experimental situation exists, the testing is termed summative, i.e., the results can be expressed as statistically meaningful measures of central tendency (e.g. mean or median) and variability (e.g. standard deviation). The Common Industry Format (CIF) for Usability Test Reports is intended for use by usability professionals to report the results of summative usability testing.The CIF standardizes the types of information that are captured about testing with users. The level of detail allows the same or another organization to replicate the test procedure. The major variables are user demographics, task descriptions, context of the test, including the equipment used, the environment in which the test is conduct­ed, and the protocol by which the subjects and the test administrator(s) interact, as well as the particular metrics chosen to code the findings of the study.The CIF is intended to replace the proprietary formats employed by companies that perform usability testing, both vendors and purchasers of software. Until now there has been no standard format for reporting usability testing results. Advantages of us­ing a standardized reporting format include (1) a reduction in training time for usabili­ty staff since an individual only needs to learn to use one form regardless of how many companies he works for and (2) enhanced potential for increased communi­cation between vendors and purchasing organizations since readers of CIF-compli­ant reports will share a common language and expectations.The purpose of this International Standard is to facilitate incorporation of usability as part of the procurement decision-making process for interactive software products so that it is easier to judge whether a product meets usability goals. Exam­ples of decisions include purchasing, upgrading and automating. It provides a com­mon format for human factors engineers and usability professionals in supplier companies to report the methods and results of usability tests to customer organiza­tions.AudienceThe Common Industry Format (CIF) is meant to be used by usability professionals within supplier organizations to generate reports that can be used by customer orga­nizations in the CIF report. The CIF is also meant to be used by customer organiza­tions to verify that a particular report is CIF-compliant. The Usability Test Report itself is intended for two types of readers:— Usability professionals in customer organizations who are evaluating both the technical merit of usability tests and the usability of the products; and— Other technical professionals and managers who are using the test results to make business decisions.The CIF may also be used within a single organization if a formal report of a summative usability test needs to be generated. In this case additional material such as a list of detailed findings may be included.The report is in two main sections, an Executive Summary and a main body. The main body contains the Methods and Results sections and is aimed at the first audi­ence above. These sections (1) describe the test methodology and results in sufficient tech­nical detail to allow replication by another organization if the test is repeated, and (2) support application of test data to questions about the product’s expected costs and benefits. Understanding and interpreting these sections will require technical back­ground in human factors or usability engineering for optimal use. The second audi­ence is directed to the Introduction, which provides summary information for non-usability professionals and managers. The Introduction may also be of general inter­est to other computing professionals. Decision makers without usability engineering expertise may find the information in the main body to be useful but should rely on expert interpretation when necessary.OrganizationClause 1 describes the scope of this specification and the conformance criteria. Clause 4 provides definitions of the terms used throughout the document. Clause 5 is the main description of the specification.Additional InformationAnnex A provides a checklist that can be used to ensure inclusion of required and recommended information. A glossary is provided in Annex B to define terminology used in the report format description. A Word template for report production can be found at: http://www.ncits.org/ref-docs/CIF/CIF_template.dot. A printed version of the template can be found in Annex C. An example is provided in Annex D illustrating how the format is used followed by an informative bibliography.","1   ScopeThis International Standard is intended to be used to report the measures obtained from a test of usability as defined in ISO 9241-11: effectiveness, efficiency and satisfaction in a specified context of use.NOTE Metrics for other more-detailed usability requirements can be found in ISO/IEC 9126 parts 2 and 3.This International Standard is intended to be used by:— usability professionals within supplier organizations to generate reports that can be used by customer organizations;— customer organizations to verify that a particular report conforms to this International Standard;— human factors or other usability professionals in customer organizations who are evaluating both the technical merit of usability tests and the usability of the products; and— other technical professionals and managers in the customer organization who are using the test results to make business decisions about product suitability and purchase.The Executive Summary and Introduction in 5.2 and 5.3 provide summary information for nonusability professionals and managers.Subclauses 5.4 and 5.5 describe the test methodology and results in technical detail suitable for replication, and also support application of test data to questions about the product’s expected costs and benefits. Understanding and interpreting these sections will require technical background in human factors or usability engineering for optimal use.The report format assumes sound practice [1, 2] has been followed in the design and execution of the test. Test procedures which produce measures that summarize usability should be used, i.e. the test is summative in nature. Some usability evaluation methods, such as formative tests, are intended to identify problems rather than produce measures; the format is not structured to support the results of such testing methods.",ISO/IEC 25062:2006(en)
53,"Preliminary Design Summary _ The contractor shall proceed with the development of a preliminary design based on the outcome of the System Concept Review.  The contractor shall begin to refine the system specifications down to the software module level.  All software modules shall be described functionally with clearly defined inputs and outputs. The contractor shall complete a preliminary design and hold a Preliminary Design Review (PDR) within six (6) months of the contract award date.Subtask 2.1 _ System Concept Review (SCR) _ At the beginning of the contract the contractor shall have a requirements review with the DHS COR to establish the set of weapons to focus on for the algorithm development and to establish detection and false alarm goals. The contractor shall provide the prototype algorithm results at the time of the SCR review so that the decision of weapon priorities can factor in how difficult each weapon is to detect. The contractor shall coordinate with DHS to schedule a SCR meeting. The contractor shall prepare and submit to the DHS COR the proposed agenda and preliminary algorithm results ten (10) business days prior to the meeting and an update at the meeting. The contractor shall prepare and submit the minutes of the meeting to the DHS S&T COR within five (5) business days after the meeting.Subtask 2.2 _ Data CollectionSubtask 2.2.1 _ Obtain Items _ Based on the discussion with DHS from the system concept review subtask, a list of prohibited items to be used for data collection will be defined. The items will be purchased by the contractor for scanning at Transportation Security Laboratory (TSL) in Atlantic City, NJ or an alternate site.Subtask 2.2.2 _ Data Collection Plan _ Based on the discussion with DHS from the system concept review subtasks, the contractor shall prepare a data collection plan. This plan shall include key image features that are to be collected as well as the methodology for scanning each prohibited item. This includes planned concealment strategies and bag configurations. The contractor shall also prepare an outline for the data collection report describing data to be provided after the data collection subtasks (subtasks 2.2.3, 2.2.4, 2.2.5) have been completed.Subtask 2.2.3 _ Collect Data _ The contractor will conduct the data collection at the contractor site and offsite as necessary per the data collection plan. Offsite collection is assumed at the TSL. The offsite data collection is planned for one week. The contractor shall ship any necessary equipment, install it for one week of use, and de-install and return the equipment to the contractors facilities at the conclusion of data collection.    Subtask 2.2.4 _ Data Truthing _ The contractor shall review the collected data to identify and mark the threat items.Subtask 2.2.5 _ Prepare and Transmit Data Collection Report _ The Contractor shall document the items scanned and the bag, concealment, and content information. This report shall be delivered electronically to the DHS COR.Subtask 2.3 _ Image Analysis and Feature Identification _ The images collected will be scored using the current algorithm. The images where the threat is not detected will be examined to identify their unique features and to determine the potential strategies for detection.Subtask 2.4 _ PDR _ The contractor shall hold a Preliminary Design Review (PDR), within six (6) months of contract award. PDR materials shall be delivered ten (10) business days prior to PDR to the DHS S&T COR along with specifications and drawings. Approval of the PDR deliverables by the DHS S&T COR will be required prior to PDR. As part of the PDR deliverables, the contractor shall propose a system prototype baseline to include:",Systems and software engineering — Information technology project performance benchmarking framework — Part 1: Concepts and definitions,https://www.iso.org/obp/ui/#!iso:std:54993:en,"IntroductionThis part of ISO/IEC 29155 identifies a framework which consists of activities and components that are necessary to successfully identify, define, select, apply, and improve benchmarking for information technology (IT) project performance. It also provides definitions for IT project performance benchmarking terms.This part of ISO/IEC 29155 is intended to provide a framework about issues and considerations for data selection and comparison in information technology (IT) project performance benchmarking.The starting point for this part of ISO/IEC 29155 and the ISO/IEC 29155 series was the concept outlined by the draft ISBSG (International Software Benchmarking Standards Group) benchmarking standard. IT project performance benchmarking is a combination of several different advanced technologies and practices in the area of quantitative analysis and management. Thus the framework introduced in this part of ISO/IEC 29155 can be built on the basis of various standardized key technologies such as:— project management (e.g. PMBOK Guide and ISO 10006),— systems and software measurements (e.g. ISO/IEC 15939),— software life cycle processes (e.g. ISO/IEC 12207),— systems life cycle processes (e.g. ISO/IEC 15288),— functional size measurement (e.g. ISO/IEC 14143 series and related methods),— systems and software quality evaluations (e.g. ISO/IEC 25000 family and ISO/IEC 9126 series).This part of ISO/IEC 29155 is designed to conform to the concepts within ISO/IEC 12207 (software life cycle processes), ISO/IEC 15288 (systems life cycle processes), the ISO/IEC 14143 series (functional size measurement), the ISO/IEC 15504 series (process assessment), ISO/IEC TR 12182 (categorization of software), or ISO/IEC 14764 (maintenance of software life cycle processes).IT project performance benchmarking instances are initiated and conducted for various reasons. Among the most common reasons are:a) need to improve project management maturity,b) need to improve project estimation capability,c) need to compare productivity between different project types and technologies,d) need to compare project productivity between similar industries,e) need to find the most effective targets for IT development process improvement.Much has been written regarding the trials of establishing an IT project benchmarking framework, and statistics bear witness to the high failure rate of measurement programs. The most probable causes for failure have been disappointment in the benchmarking outcome due to a lack of alignment between the selected measurements and business goals, and the misunderstanding of project level measurements in relation to program and portfolio management levels.This part of ISO/IEC 29155 is developed as the first of multiple parts which will complete the IT project performance benchmarking framework as is shown in Figure 1. This part of ISO/IEC 29155 is the concept standard; it contains no mandatory requirements. The requirements for the benchmarking process will be specified subsequent to the standardization of the concepts and definitions. Then, the guidelines for the quality and quantity of different inputs and benchmarking outputs will be drafted, together with the domain-specific control needs.Figure 1
—
IT project performance benchmarking framework overview","1   ScopeThis part of ISO/IEC 29155 identifies a framework for information technology (IT) project performance benchmarking (e.g. development or maintenance productivity) and related aspects (e.g. data collection and software classification).The framework consists of activities and components that are necessary to successfully identify, define, select, apply, and improve benchmarking for IT project performance. It also provides definitions for IT project performance benchmarking terms.The target audience of this part of ISO/IEC 29155 are stakeholders of IT project performance benchmarking.NOTE The following are examples of how this part of ISO/IEC 29155 can be used:— by a Benchmarking service provider who wants to align their benchmarking process to be consistent with this part of ISO/IEC 29155;— by a Benchmarking user (or third-party agents) for evaluating the performance of an IT project;— by an organization internally to answer specific information needs.This part of ISO/IEC 29155 does not prescribe how to organize benchmarking. It is out of the scope of this part of ISO/IEC 29155 to prescribe the name, format, or explicit content of the documentation to be produced from the benchmarking process.",ISO/IEC 29155-1:2011(en)
54,"Current airport screening protocols rely on identification cards such as drivers licenses to perform identity verification at TSA checkpoints.  In the future, ensuring identity could be even more important if a substantial fraction of the flying public goes through an expedited screening protocol based on information (e.g., travel patterns) associated with passenger identity.  In addition, more adaptive security protocols, in which checked or carry-on baggage screening protocols are closely linked to passenger risk, would require mechanisms for high-confidence association between passengers and their bags.In this task, AF-MIT/LL will conduct a study of existing and feasible near-term identity verification and correlation methods in order to assess advantages and projected vulnerabilities with respect to potential _screening at speed checkpoint protocols.  Components that will be considered include credential authentication technologies, standoff biometrics, scan-able unique identifiers, and the underlying software architecture and databases required to manage identity-related information.  AF-MIT/LL will perform limited implementation to support the analysis; for instance, evaluation of low-fidelity face matching to verify identity across multiple passenger visits to the same airport may require some testing with actual face images extracted from surveillance video at a range of resolutions.  The output of this task will be a set of recommendations for the use of identity verification and correlation mechanisms, and an identification of near-term development efforts required to fill capability gaps.",Information technology — Biometric data interchange formats — Part 5: Face image data,https://www.iso.org/obp/ui/#!iso:std:50867:en,"IntroductionFace images, also commonly referred to as displayed portraits, have been used for many decades to verify the identity of persons. In recent years, digital face images are used in many applications including human examination as well as computer automated face recognition. Although photographic formats have been standardized in some cases such as for passports and driver licenses, there is a need to define a standard data format of digital face images to allow interoperability among vendors.This part of ISO/IEC 19794 is intended to provide a face image format for face recognition applications requiring exchange of face image data. The typical applications are1) human examination of facial images with sufficient resolution to allow a human examiner to ascertain small features such as moles and scars that might be used to verify identity,2) human verification of identity by comparison of persons against facial images,3) computer automated face biometric identification (one-to-many searching), and4) computer automated face biometric verification (one-to-one comparison).To enable many applications on a variety of devices, including devices that have limited resources available for data storage, and to improve face recognition accuracy, this part of ISO/IEC 19794 specifies not only a data format, but also scene constraints (lighting, pose, expression, etc.), photographic properties (positioning, camera focus, etc.) and digital image attributes (image resolution, image size, etc.).Several face image types are introduced to define categories that satisfy requirements of some applications:• Basic: This is the fundamental Face Image Type that specifies a record format including header and representation data. All Face Image Types adhere to the properties of this type. No mandatory scene, photographic and digital requirements are specified for this image type.• Frontal: A Basic Face Image Type that adheres to additional requirements appropriate for frontal face recognition and/or human examination. Two types of Frontal Face Image Types are defined in this part of ISO/IEC 19794, Full Frontal and Token Frontal (or simply Token).• Full Frontal: A Face Image Type that specifies frontal images with sufficient resolution for human examination as well as reliable computer face recognition. This type of Face Image Type includes the full head with all hair in most cases, as well as neck and shoulders. This image type is suitable for permanent storage of the face information, and it is applicable to portraits for passport, driver license, and “mugshot” images.• Token Frontal: A Face Image Type that specifies frontal images with a specific geometric size and eye positioning based on the width and height of the image. This image type is suitable for minimizing the storage requirements for computer face recognition tasks such as verification while still offering vendor independence and human verification (versus human examination which requires more detail) capabilities.• Post-processed Frontal: Applying digital post-processing to a captured image can modify this image in a way that it is more suitable for automatic face recognition. The Post-processed Frontal Face Image Type is thought of as the interchange format for these kinds of facial images.• Basic 3D: The Basic 3D Image Type is the base Image Type of all 3D Face Image Types. All 3D Face Image Types obey normative requirements of this image type.• Full Frontal 3D: The Full Frontal 3D Image Type combines a Full Frontal 2D image with additional 3D information.• Token Frontal 3D: The Token Frontal 3D Image Type combines a Token Frontal 2D image with additional 3D information.Table 1 shows the relationships between Face Image Types using the notion of inheritance. For example, Frontal inherits properties from Basic, which means that all normative clauses that apply to Basic also apply to Frontal.Table 1
—
Inheritance of Face Image TypesFace Image TypeInherits fromNormative clausesInformative annexesBasicNone1, 2, 3, 4, 5, 6B.1FrontalBasic7B.2Full FrontalFrontal8B.3Token FrontalFrontal9B.4Post-processed FrontalFrontal10 Figure 1 gives a general overview of the scene, photographic, digitization, and format requirements for the face image types specified in this part of ISO/IEC 19794.Figure 1
—
The types of imaging requirements specified in this part of ISO/IEC 19794. The Basic Face Image Type has no scene, photographic, or digital requirementsRequirementsScenePhotographicDigitalFormat
Lighting

Image and Subject

Positioning


Camera Attributes

Digital Camera


Analogue to Digital


Image Scanning

Digital Specifications


Record Format and Organization
Clauses:Clauses:Clauses:Clauses:Basic FaceNoneBasic FaceNoneBasic FaceNoneBasic Face5       6.2       6.3       6.4Frontal Face7.2Frontal Face7.3Frontal Face7.4Frontal Face7.5Full Frontal Face8.2Full Frontal Face8.3Full Frontal Face8.4Full Frontal Face8.5    Token Face9.2Token Face9.3      Post-processed10.3      Frontal Face This is a revision of ISO/IEC 19794-5:2005. The structure of the data format is not compatible with the previous version.NOTE This part of ISO/IEC 19794 relies on other ISO International Standards.","1   ScopeThis part of ISO/IEC 19794— specifies a record format for storing, recording, and transmitting the information from one or more facial images or a short video stream of facial images,— specifies scene constraints of the facial images,— specifies photographic properties of the facial images,— specifies digital image attributes of the facial images,— provides best practices for the photography of faces.",ISO/IEC 19794-5:2011(en)
55,"Current airport screening protocols rely on identification cards such as drivers licenses to perform identity verification at TSA checkpoints.  In the future, ensuring identity could be even more important if a substantial fraction of the flying public goes through an expedited screening protocol based on information (e.g., travel patterns) associated with passenger identity.  In addition, more adaptive security protocols, in which checked or carry-on baggage screening protocols are closely linked to passenger risk, would require mechanisms for high-confidence association between passengers and their bags.In this task, AF-MIT/LL will conduct a study of existing and feasible near-term identity verification and correlation methods in order to assess advantages and projected vulnerabilities with respect to potential _screening at speed checkpoint protocols.  Components that will be considered include credential authentication technologies, standoff biometrics, scan-able unique identifiers, and the underlying software architecture and databases required to manage identity-related information.  AF-MIT/LL will perform limited implementation to support the analysis; for instance, evaluation of low-fidelity face matching to verify identity across multiple passenger visits to the same airport may require some testing with actual face images extracted from surveillance video at a range of resolutions.  The output of this task will be a set of recommendations for the use of identity verification and correlation mechanisms, and an identification of near-term development efforts required to fill capability gaps.",Information technology — Advanced image coding and evaluation — Part 1: Guidelines for image coding system evaluation,https://www.iso.org/obp/ui/#!iso:std:63637:en,"IntroductionThis document provides a framework and best practices to evaluate image compression algorithms. This document provides a selection of evaluation tools that allow testing multiple features, including objective metric image quality, subjective metric image quality and codec algorithmic complexity. Which features of codecs should be tested and pass-fail criteria is beyond the scope of this document.","1   ScopeThis document recommends best practices for coding system evaluation of images and image sequences. This document defines a common vocabulary of terms for coding system evaluation and divides evaluation methods into three broad categories:a) subjective assessment;b) objective assessment;c) computational assessment.In addition to these broad assessment categories, this document discusses special care that is given for coding unusual imagery, e.g. high dynamic range or high colour depth.A fourth assessment category, hardware complexity, is often important for real-time or computationally complex applications; however, it is outside the scope of this document.",ISO/IEC TR 29170-1:2017(en)
56,"DHS S&T is has a variety of projects supporting the development of next-generation aviation security capabilities as part of its APEX Screening-at-Speed program.   Several of those efforts will utilize an open architecture and allow integration with 3rd-party party capabilities.   DHS S&T is seeking the following from Sandia OTAP project to further its APEX program:1.   Further Development of OPSL to support AIT modularization to include third-party algorithms supporting threat detection (including ATR Reconstruction algorithms).2.   Development of a standardized data format to structure data from AIT scans.  The intent of the formatting effort is to ensure that third-parties have a standardized, non-proprietary way to access all levels of data from AIT scans as opposed to being limited to post-reconstruction, proprietary data.a.  (Optional) Accelerate the development of an _A-A data format for CT machines to enable greater access to pre-reconstruction data.  The technical lessons learned and progress from the CT effort already underway with the OTAP project will directly inform and facilitate the AIT data format effort. 3.   Support further development of the MRAD algorithm development environment to support rapid test and evaluation (T&E) of AIT ATR algorithms. a.       (Optional) Accelerate the development of third-party CT ATR and Reconstruction algorithms on the OTAP program to mature the ATR development and T&E of AIT algorithms. ",Information technology — Programming languages — Guidance to avoiding vulnerabilities in programming languages through language selection and use,https://www.iso.org/obp/ui/#!iso:std:61457:en,"IntroductionAll programming languages contain constructs that are incompletely specified, exhibit undefined behaviour, are implementation-dependent, or are difficult to use correctly. The use of those constructs may therefore give rise to vulnerabilities, as a result of which, software programs can execute differently than intended by the writer. In some cases, these vulnerabilities can compromise the safety of a system or be exploited by attackers to compromise the security or privacy of a system.This Technical Report is intended to provide guidance spanning multiple programming languages, so that application developers will be better able to avoid the programming constructs that lead to vulnerabilities in software written in their chosen language and their attendant consequences. This guidance can also be used by developers to select source code evaluation tools that can discover and eliminate some constructs that could lead to vulnerabilities in their software or to select a programming language that avoids anticipated problems.It should be noted that this Technical Report is inherently incomplete. It is not possible to provide a complete list of programming language vulnerabilities because new weaknesses are discovered continually. Any such report can only describe those that have been found, characterized, and determined to have sufficient probability and consequence.Furthermore, to focus its limited resources, the working group developing this report decided to defer comprehensive treatment of several subject areas until future editions of the report. These subject areas include:• Object-oriented language features (although some simple issues related to inheritance are described in 6.43 Inheritance [RIP])• Numerical analysis (although some simple items regarding the use of floating point are described in 6.5 Floating-point Arithmetic [PLF])• Inter-language operability","1.   ScopeThis Technical Report specifies software programming language vulnerabilities to be avoided in the development of systems where assured behaviour is required for security, safety, mission-critical and business-critical software. In general, this guidance is applicable to the software developed, reviewed, or maintained for any application.Vulnerabilities are described in a generic manner that is applicable to a broad range of programming languages.",ISO/IEC TR 24772:2013(en)
57,"DHS S&T is has a variety of projects supporting the development of next-generation aviation security capabilities as part of its APEX Screening-at-Speed program.   Several of those efforts will utilize an open architecture and allow integration with 3rd-party party capabilities.   DHS S&T is seeking the following from Sandia OTAP project to further its APEX program:1.   Further Development of OPSL to support AIT modularization to include third-party algorithms supporting threat detection (including ATR Reconstruction algorithms).2.   Development of a standardized data format to structure data from AIT scans.  The intent of the formatting effort is to ensure that third-parties have a standardized, non-proprietary way to access all levels of data from AIT scans as opposed to being limited to post-reconstruction, proprietary data.a.  (Optional) Accelerate the development of an _A-A data format for CT machines to enable greater access to pre-reconstruction data.  The technical lessons learned and progress from the CT effort already underway with the OTAP project will directly inform and facilitate the AIT data format effort. 3.   Support further development of the MRAD algorithm development environment to support rapid test and evaluation (T&E) of AIT ATR algorithms. a.       (Optional) Accelerate the development of third-party CT ATR and Reconstruction algorithms on the OTAP program to mature the ATR development and T&E of AIT algorithms. ","Information technology — Programming languages, their environments and system software interfaces — Technical Report on C++ Performance",https://www.iso.org/obp/ui/#!iso:std:43351:en,"Introduction“Performance” has many aspects - execution speed, code size, data size, and memory footprint at run-time, or time and space consumed by the edit/compile/link process. It could even refer to the time necessary to find and fix code defects. Most people are primarily concerned with execution speed, although program footprint and memory usage can be critical for small embedded systems where the program is stored in ROM, or where ROM and RAM are combined on a single chip.Efficiency has been a major design goal for C++ from the beginning, as has the principle of “zero overhead” for any feature that is not used in a program. It has been a guiding principle from the earliest days of C++ that “you don’t pay for what you don’t use”.Language features that are never used in a program should not have a cost in extra code size, memory size, or run-time. If there are places where C++ cannot guarantee zero overhead for unused features, this Technical Report will attempt to document them. It will also discuss ways in which compiler writers, library vendors, and programmers can minimize or eliminate performance penalties, and will discuss the trade-offs among different methods of implementation.Programming for resource-constrained environments is another focus of this Technical Report Typically, programs that run into resource limits of some kind are either very large or very small. Very large programs, such as database servers, may run into limits of disk space or virtual memory. At the other extreme, an embedded application may be constrained to run in the ROM and RAM space provided by a single chip, perhaps a total of 64K of memory, or even smaller.Apart from the issues of resource limits, some programs must interface with system hardware at a very low level. Historically the interfaces to hardware have been implemented as proprietary extensions to the compiler (often as macros). This has led to the situation that code has not been portable, even for programs written for a given environment, because each compiler for that environment has implemented different sets of extensions.","1   ScopeThe aim of this Technical Report is:• to give the reader a model of time and space overheads implied by use of various C++ language and library features,• to debunk widespread myths about performance problems,• to present techniques for use of C++ in applications where performance matters, and• to present techniques for implementing C++ Standard language and library facilities to yield efficient code.As far as run-time and space performance are concerned, if you can afford to use C for an application, you can afford to use C++ in a style that uses C++’s facilities appropriately for that application.This Technical Report first discusses areas where performance issues matter, such as various forms of embedded systems programming and high-performance numerical computation. After that, the main body of the Technical Report considers the basic cost of using language and library facilities, techniques for writing efficient code, and the special needs of embedded systems programming.Performance implications of object-oriented programming are presented. This discussion rests on measurements of key language facilities supporting OOP, such as classes, class member functions, class hierarchies, virtual functions, multiple inheritance, and run-time type information (RTTI). It is demonstrated that, with the exception of RTTI, current C++ implementations can match hand-written low-level code for equivalent tasks. Similarly, the performance implications of generic programming using templates are discussed. Here, however, the emphasis is on techniques for effective use. Error handling using exceptions is discussed based on another set of measurements. Both time and space overheads are discussed. In addition, the predictability of performance of a given operation is considered.The performance implications of IOStreams and Locales are examined in some detail and many generally useful techniques for time and space optimizations are discussed.The special needs of embedded systems programming are presented, including ROMability and predictability. A separate chapter presents general C and C++ interfaces to the basic hardware facilities of embedded systems.Additional research is continuing into techniques for producing efficient C++ libraries and programs. Please see the WG21 web site at www.open-std.org/jtc1/sc22/wg21 for example code from this Technical Report and pointers to other sites with relevant information.",ISO/IEC TR 18015:2006(en)
58,"DHS S&T is has a variety of projects supporting the development of next-generation aviation security capabilities as part of its APEX Screening-at-Speed program.   Several of those efforts will utilize an open architecture and allow integration with 3rd-party party capabilities.   DHS S&T is seeking the following from Sandia OTAP project to further its APEX program:1.   Further Development of OPSL to support AIT modularization to include third-party algorithms supporting threat detection (including ATR Reconstruction algorithms).2.   Development of a standardized data format to structure data from AIT scans.  The intent of the formatting effort is to ensure that third-parties have a standardized, non-proprietary way to access all levels of data from AIT scans as opposed to being limited to post-reconstruction, proprietary data.a.  (Optional) Accelerate the development of an _A-A data format for CT machines to enable greater access to pre-reconstruction data.  The technical lessons learned and progress from the CT effort already underway with the OTAP project will directly inform and facilitate the AIT data format effort. 3.   Support further development of the MRAD algorithm development environment to support rapid test and evaluation (T&E) of AIT ATR algorithms. a.       (Optional) Accelerate the development of third-party CT ATR and Reconstruction algorithms on the OTAP program to mature the ATR development and T&E of AIT algorithms. ",Software Engineering — Guide to the software engineering body of knowledge (SWEBOK),https://www.iso.org/obp/ui/#!iso:std:67604:en,"IntroductionThe purpose of the Guide to the Software Engineering Body of Knowledge is to provide a consensually validated characterization of the bounds of the software engineering discipline and to provide a topical access to the Body of Knowledge supporting that discipline. The Body of Knowledge is subdivided into fifteen software engineering Knowledge Areas (KA) providing an outline of topics. The descriptions in the KAs are designed to discriminate among the various important concepts, permitting readers to find their way quickly to subjects of interest. Upon finding a subject, readers are referred to key papers or books selected because they succinctly present the knowledge.Publication of the 2004 version of this Guide to the Software Engineering Body of Knowledge (SWEBOK 2004)—adopted as ISO/IEC TR 19759:2005—was a major milestone in establishing software engineering as a recognized engineering discipline. The goal in developing this update to SWEBOK is to improve the currency, readability, consistency, and usability of the Guide. All knowledge areas (KAs) have been updated to reflect changes in software engineering since publication of SWEBOK 2004. Four new foundation KAs and a Software Engineering Professional Practices KA have been added. The Software Engineering Tools and Methods KA has been revised as Software Engineering Models and Methods. Software engineering tools is now a topic in each of the KAs. Three appendices provide the specifications for the KA description, an annotated set of relevant standards for each KA, and a listing of the references cited in the Guide.An emphasis on engineering practice leads the Guide toward a strong relationship with the normative literature. Most of the computer science, information technology and software engineering literature provides information useful to software engineers, but a relatively small portion is normative. A normative document prescribes what an engineer should do in a specified situation rather than providing information that might be helpful. The normative literature is validated by consensus formed among practitioners and is concentrated in standards and related documents. From the beginning, the SWEBOK project was conceived as having a strong relationship to the normative literature of software engineering. The two major standards bodies for software engineering (IEEE Computer Society Software and Systems Engineering Standards Committee and ISO/IEC JTC1/SC7) cooperated in the project.The Guide is oriented toward a variety of audiences, all over the world. It aims to serve public and private organizations in need of a consistent view of software engineering for defining education and training requirements, classifying jobs, developing performance evaluation policies or specifying software development tasks. It also addresses practicing, or managing, software engineers and the officials responsible for making public policy regarding licensing and professional guidelines. In addition, professional societies and educators defining the certification rules, accreditation policies for university curricula, and guidelines for professional practice will benefit from the SWEBOK Guide, as well as the students learning the software engineering profession and educators and trainers engaged in defining curricula and course content.",1   ScopeThis Technical report characterizes the boundaries of the software engineering discipline and provides topical access to the literature supporting that discipline.,ISO/IEC TR 19759:2015(en)
59,"DHS S&T is has a variety of projects supporting the development of next-generation aviation security capabilities as part of its APEX Screening-at-Speed program.   Several of those efforts will utilize an open architecture and allow integration with 3rd-party party capabilities.   DHS S&T is seeking the following from Sandia OTAP project to further its APEX program:1.   Further Development of OPSL to support AIT modularization to include third-party algorithms supporting threat detection (including ATR Reconstruction algorithms).2.   Development of a standardized data format to structure data from AIT scans.  The intent of the formatting effort is to ensure that third-parties have a standardized, non-proprietary way to access all levels of data from AIT scans as opposed to being limited to post-reconstruction, proprietary data.a.  (Optional) Accelerate the development of an _A-A data format for CT machines to enable greater access to pre-reconstruction data.  The technical lessons learned and progress from the CT effort already underway with the OTAP project will directly inform and facilitate the AIT data format effort. 3.   Support further development of the MRAD algorithm development environment to support rapid test and evaluation (T&E) of AIT ATR algorithms. a.       (Optional) Accelerate the development of third-party CT ATR and Reconstruction algorithms on the OTAP program to mature the ATR development and T&E of AIT algorithms. ","Information and documentation - Qualitative conditions and basic statistics for library buildings — Space, function and design",https://www.iso.org/obp/ui/#!iso:std:50251:en,"IntroductionThis Technical Report provides guidance for the planning of library buildings by identifying requirements of space and technical equipment. It supports decision making for librarians, architects and financing institutions.This Technical Report considers planning, both for new buildings and for the reconstruction of existing library buildings or the conversion of other buildings for library use.In order to plan new or reconstructed buildings, libraries need reliable data that can help them to calculate their space requirements and to define the respective technical building equipment for housing the collections, as well as for offering adequate user areas and background areas for the internal library operations. Such data are not yet available in International Standards.This Technical Report includes data and specifications for all types of libraries, but especially for academic and public libraries. Its main topics are space requirements for:— user areas (user places, reference and information services, lending services, user training, recreation and communication areas, meeting and exhibition areas);— collection storage areas (including non-book materials);— library operations (media processing, bindery, computing and management).This Technical Report also covers technical aspects like security and safety systems, floor loading, transport systems, acoustic conditions, lighting systems and wiring and the issues of barrier-free construction and sustainability.Annex A gives an overview of issues to consider when planning to reconstruct or reorganize existing buildings for library purposes. Annex B provides a list of functional areas and rooms to be used when checking the completeness of the plans.Several clauses of this Technical Report are partly based on References [13] and [18].","1   ScopeThis Technical Report specifies data for the planning of library buildings. It also provides guidance on the selection of technical building equipment for the different functional areas of libraries.This Technical Report is applicable to all types of libraries in all countries, but especially to academic and public libraries.It does not include details on national, regional or local regulations that can affect the planning of library buildings.",ISO/TR 11219:2012(en)
60,"DHS S&T is has a variety of projects supporting the development of next-generation aviation security capabilities as part of its APEX Screening-at-Speed program.   Several of those efforts will utilize an open architecture and allow integration with 3rd-party party capabilities.   DHS S&T is seeking the following from Sandia OTAP project to further its APEX program:1.   Further Development of OPSL to support AIT modularization to include third-party algorithms supporting threat detection (including ATR Reconstruction algorithms).2.   Development of a standardized data format to structure data from AIT scans.  The intent of the formatting effort is to ensure that third-parties have a standardized, non-proprietary way to access all levels of data from AIT scans as opposed to being limited to post-reconstruction, proprietary data.a.  (Optional) Accelerate the development of an _A-A data format for CT machines to enable greater access to pre-reconstruction data.  The technical lessons learned and progress from the CT effort already underway with the OTAP project will directly inform and facilitate the AIT data format effort. 3.   Support further development of the MRAD algorithm development environment to support rapid test and evaluation (T&E) of AIT ATR algorithms. a.       (Optional) Accelerate the development of third-party CT ATR and Reconstruction algorithms on the OTAP program to mature the ATR development and T&E of AIT algorithms. ",Information technology — Common Language Infrastructure (CLI),https://www.iso.org/obp/ui/#!iso:std:58046:en,,"I.1   ScopeThis International Standard defines the Common Language Infrastructure (CLI) in which applications written in multiple high-level languages can be executed in different system environments without the need to rewrite those applications to take into consideration the unique characteristics of those environments. This International Standard consists of the following parts:● Partition I: Concepts and Architecture — Describes the overall architecture of the CLI, and provides the normative description of the Common Type System (CTS), the Virtual Execution System (VES), and the Common Language Specification (CLS). It also provides an informative description of the metadata.● Partition II: Metadata Definition and Semantics — Provides the normative description of the metadata: its physical layout (as a file format), its logical contents (as a set of tables and their relationships), and its semantics (as seen from a hypothetical assembler, ilasm).● Partition III: CIL Instruction Set — Describes the Common Intermediate Language (CIL) instruction set.● Partition IV: Profiles and Libraries — Provides an overview of the CLI Libraries, and a specification of their factoring into Profiles and Libraries. A companion file, CLILibrary.xml, considered to be part of this Partition, but distributed in XML format, provides details of each class, value type, and interface in the CLI Libraries.● Partition V: Debug Interchange Format — Describes a standard way to interchange debugging information between CLI producers and consumers.● Partition VI: Annexes — Contains some sample programs written in CIL Assembly Language (ILAsm), information about a particular implementation of an assembler, a machine-readable description of the CIL instruction set which can be used to derive parts of the grammar used by this assembler as well as other tools that manipulate CIL, a set of guidelines used in the design of the libraries of Partition IV. and portability considerations.",ISO/IEC 23271:2012(en)
61,"DHS S&T is has a variety of projects supporting the development of next-generation aviation security capabilities as part of its APEX Screening-at-Speed program.   Several of those efforts will utilize an open architecture and allow integration with 3rd-party party capabilities.   DHS S&T is seeking the following from Sandia OTAP project to further its APEX program:1.   Further Development of OPSL to support AIT modularization to include third-party algorithms supporting threat detection (including ATR Reconstruction algorithms).2.   Development of a standardized data format to structure data from AIT scans.  The intent of the formatting effort is to ensure that third-parties have a standardized, non-proprietary way to access all levels of data from AIT scans as opposed to being limited to post-reconstruction, proprietary data.a.  (Optional) Accelerate the development of an _A-A data format for CT machines to enable greater access to pre-reconstruction data.  The technical lessons learned and progress from the CT effort already underway with the OTAP project will directly inform and facilitate the AIT data format effort. 3.   Support further development of the MRAD algorithm development environment to support rapid test and evaluation (T&E) of AIT ATR algorithms. a.       (Optional) Accelerate the development of third-party CT ATR and Reconstruction algorithms on the OTAP program to mature the ATR development and T&E of AIT algorithms. ",Health informatics — Digital imaging and communication in medicine (DICOM) including workflow and data management,https://www.iso.org/obp/ui/#!iso:std:72941:en,"0	IntroductionDigital Imaging and Communications in Medicine (DICOM) is the standard for the communication and management of medical imaging information and related data.0.1	HistoryWith the introduction of computed tomography (CT) followed by other digital diagnostic imaging modalities in the 1970s, and the increasing use of computers in clinical applications, the American College of Radiology (ACR) and the National Electrical Manufacturers Association (NEMA) recognized the emerging need for a standard method for transferring images and associated information between devices manufactured by various vendors. These devices produce a variety of digital image formats.The American College of Radiology (ACR) and the National Electrical Manufacturers Association (NEMA) formed a joint committee in 1983 to develop a standard to:— promote communication of digital image information, regardless of device manufacturer;— facilitate the development and expansion of picture archiving and communication systems (PACS) that can also interface with other systems of hospital information;— allow the creation of diagnostic information databases that can be interrogated by a wide variety of devices distributed geographically.ACR-NEMA standards Publication No. 300-1985, published in 1985, was designated version 1.0. The standard was followed by two revisions: No. 1, dated October 1986 and No. 2, dated January 1988. These standards publications specified a hardware interface, a minimum set of software commands, and a consistent set of data formats.ACR-NEMA standards Publication No. 300-1988, published in 1988, was designated version 2.0. It included version 1.0, the published revisions, and additional revisions. It also included new material to provide command support for display devices, to introduce a new hierarchy scheme to identify an image, and to add data elements for increased specificity when describing an image.In 1993, ACR-NEMA/Standard 300 was substantially revised and replaced by this document, designated Digital Imaging and Communications in Medicine (DICOM). It embodies a number of major enhancements to previous versions of the ACR-NEMA standard, as listed below.— It is applicable to a networked environment. The ACR-NEMA standard was applicable in a point-to-point environment only; for operation in a networked environment, a Network Interface Unit (NIU) was required. DICOM supports operation in a networked environment using the industry standard networking protocol TCP/IP.— It is applicable to offline media exchange. The ACR-NEMA standard did not specify a file format or choice of physical media or logical filesystem. DICOM supports operation in an offline media environment using industry standard media such as CD-R, DVD-R and USB and common file systems.— It is a service-oriented protocol, specifying the semantics of commands and associated data, and how devices claiming conformance to the DICOM standard react to commands and data being exchanged. Specified services include support for management of the workflow of an imaging department. The ACR-NEMA standard was confined to the transfer of data with only implicit service requirements.— It specifies levels of conformance. The ACR-NEMA standard specified a minimum level of conformance. DICOM explicitly describes how an implementor must structure a Conformance Statement to select specific options.In 1995, with the addition of DICOM capabilities for cardiology imaging supported by the American College of Cardiology, the ACR-NEMA Joint Committee was reorganized as the DICOM Standards Committee, a broad collaboration of stakeholders across all medical imaging specialities.0.2	Principles0.2.1	Global applicability and localizationDICOM is a world-wide standard that can be used in every locale. It provides mechanisms to handle data that support cultural requirements, such as different writing systems, character sets, languages, and structures for addresses and person names. It supports the variety of workflows, processes and policies used for biomedical imaging in different geographic regions, medical specialities and local practices.Localization to meet the requirements of national or local health and workflow policies can be done without deviating from the DICOM standard. Such localization may include specifying code sets (e.g. procedure codes) or profiling data element usage (both specifying locally-allowed values, and making elements that are optional in the DICOM standard mandatory for local use).Localization and profiling can be specified in a number of mechanisms outside the purview of the DICOM standard. One such mechanism is Integration Profiles from the Integrating the Healthcare Enterprise (IHE) organization. It is important that Profiling adhere to the concept of non-contradiction. A Profile can add requirements but should not contradict DICOM requirements, as that would make it impossible to comply with both DICOM and the Profile.0.2.2	Continuous maintenanceThe DICOM standard is an evolving standard and it is maintained in accordance with the Procedures of the DICOM Standards Committee. Proposals for enhancements are welcome from all users of the DICOM standard and may be submitted to the Secretariat. Supplements and corrections to the DICOM standard are balloted and approved several times a year. When approved as Final Text, each change becomes official, is published separately, and goes into effect immediately. At intervals, all of the approved Final Text changes are consolidated and published in an updated edition of the DICOM standard. Once changes are consolidated into an updated edition of the DICOM standard, the individual change documents are not maintained; readers are directed to use the consolidated edition of the DICOM standard.A requirement in updating the DICOM standard is to maintain effective compatibility with previous editions.The maintenance process may involve retirement of sections of the DICOM standard.Retirement does not imply that these features cannot be used. However, the DICOM Standards Committee will not maintain the documentation of retired features. The reader is referred to earlier editions of the DICOM standard.The use of the retired features is discouraged for new implementations, in favour of those alternatives remaining in the DICOM standard.0.2.3	Information objects and unique object identificationMany DICOM services involve the exchange of persistent information objects, such as images. An instance of such an information object may be exchanged across many systems and many organizational contexts, and over time. While minor changes may be made to the attributes of an instance to facilitate its handling within a particular organization (e.g. by coercing a Patient ID to the value used in a local context), the semantic content of an instance does not change.Each instance is identified by a globally unique object identifier, which persists with the instance across all exchanges. Changes to the semantic content of an instance are defined to create a new instance, which is assigned a new globally unique object identifier.0.2.4	ConformanceConformance to the DICOM standard is stated in terms of Service-Object Pair (SOP) Classes, which represent Services (such as Storage using network, media, or web) operating on types of Information Objects (such as CT or MR images).SOP Class specifications in the DICOM standard are only changed in a manner that is intended to be forward and backward compatible for all editions of the DICOM standard. Conformance requirements and conformance claims are therefore referenced to the identifier of the SOP Class, and never referenced to an edition of the DICOM standard.Each implementation is required to provide a Conformance Statement, in accordance with a consistent pro forma structure, facilitating comparison of products for interoperability.0.2.5	Consistency of information modelA large number of information objects defined in the DICOM standard follow a common composite information model with information entities representing Patient, Study, Series, Equipment, Frame of Reference, and the specific instance data type. This information model is a simplification of the real world concepts and activities of medical imaging; for acquisition modalities, a Study is approximately equivalent to an ordered procedure, and a Series is approximately equivalent to a performed data acquisition protocol element. In other domains, such as Radiotherapy, the Study and Series are less clearly related to real world entities or activities, but are still required for consistency. This simplified model is sufficient for the pragmatic needs of managing imaging and related data collected in routine practice.New information objects defined in DICOM will typically conform to this existing common information model, allowing reuse of implementations with minimal changes to support the new objects.","1   ScopeThis document, within the field of health informatics, addresses the exchange of digital images and information related to the production and management of those images, between both medical imaging equipment and systems concerned with the management and communication of that information.This document facilitates interoperability of medical imaging equipment by specifying:— for network communications, a set of protocols to be followed by devices claiming conformance to this document;— the syntax and semantics of Commands and associated information which can be exchanged using these protocols;— for media communication, a set of media storage services to be followed by devices claiming conformance to this document, as well as a File Format and a medical directory structure to facilitate access to the images and related information stored on interchange media;— information that is to be supplied with an implementation for which conformance to this document is claimed.This document does not specify:— the implementation details of any features of the DICOM standard on a device claiming conformance;— the overall set of features and functions to be expected from a system implemented by integrating a group of devices each claiming conformance to this document;— a testing/validation procedure to assess an implementation's conformance to this document.This document pertains to the field of medical informatics. Within that field, it addresses the exchange of digital information between medical imaging equipment and other systems. Because such equipment may interoperate with other medical devices and information systems, the scope of this document needs to overlap with other areas of medical informatics. However, this document does not address the full breadth of this field.This document has been developed with an emphasis on diagnostic medical imaging as practiced in radiology, cardiology, pathology, dentistry, ophthalmology and related disciplines, and image-based therapies such as interventional radiology, radiotherapy and surgery. However, it is also applicable to a wide range of image and non-image related information exchanged in clinical, research, veterinary, and other medical environments.This document facilitates interoperability of systems claiming conformance in a multi-vendor environment, but does not, by itself, guarantee interoperability.",ISO 12052:2017(en)
62,"As a result, Cyber.gov is creating a robust, innovative and holistic cyber security architecture design that mitigates modern threats (Asset Management, Configuration Management, Phishing, DDoS, Ransomware, Mobile, Cloud issues and more) by leveraging best practices and implementable solutions with minimal impact to workforce efficiency. It will not only addresses some of the issues of perimeter based defense, but also brings new technologies such as Software Defined Perimeter (SDP) tailored to the .Gov domain, while guiding CIOs and CISOs in selecting and implementing current best practices for 34 cyber security components. Cyber.gov will significantly improve the security and resiliency across the D/As. ",Information technology — Security techniques — Guidelines for cybersecurity,https://www.iso.org/obp/ui/#!iso:std:44375:en,"IntroductionThe Cyberspace is a complex environment resulting from the interaction of people, software and services on the Internet, supported by worldwide distributed physical information and communications technology (ICT) devices and connected networks. However there are security issues that are not covered by current information security, Internet security, network security and ICT security best practices as there are gaps between these domains, as well as a lack of communication between organizations and providers in the Cyberspace. This is because the devices and connected networks that have supported the Cyberspace have multiple owners, each with their own business, operational and regulatory concerns. The different focus placed by each organization and provider in the Cyberspace on relevant security domains where little or no input is taken from another organization or provider has resulted in a fragmented state of security for the Cyberspace.As such, the first area of focus of this International Standard is to address Cyberspace security or Cybersecurity issues which concentrate on bridging the gaps between the different security domains in the Cyberspace. In particular this International Standard provides technical guidance for addressing common Cybersecurity risks, including:— social engineering attacks;— hacking;— the proliferation of malicious software (“malware”);— spyware; and— other potentially unwanted software.The technical guidance provides controls for addressing these risks, including controls for:— preparing for attacks by, for example, malware, individual miscreants, or criminal organizations on the Internet;— detecting and monitoring attacks; and— responding to attacks.The second area of focus of this International Standard is collaboration, as there is a need for efficient and effective information sharing, coordination and incident handling amongst stakeholders in the Cyberspace. This collaboration must be in a secure and reliable manner that also protects the privacy of the individuals concerned. Many of these stakeholders can reside in different geographical locations and time zones, and are likely to be governed by different regulatory requirements. Stakeholders include:— consumers, which can be various types of organizations or individuals; and— providers, which include service providers.Thus, this International Standard also provides a framework for— information sharing,— coordination, and— incident handling.The framework includes— key elements of considerations for establishing trust,— necessary processes for collaboration and information exchange and sharing, as well as— technical requirements for systems integration and interoperability between different stakeholders.Given the scope of this International Standard, the controls provided are necessarily at a high level. Detailed technical specification standards and guidelines applicable to each area are referenced within this International Standard for further guidance.","1   ScopeThis International Standard provides guidance for improving the state of Cybersecurity, drawing out the unique aspects of that activity and its dependencies on other security domains, in particular:— information security,— network security,— internet security, and— critical information infrastructure protection (CIIP).It covers the baseline security practices for stakeholders in the Cyberspace. This International Standard provides:— an overview of Cybersecurity,— an explanation of the relationship between Cybersecurity and other types of security,— a definition of stakeholders and a description of their roles in Cybersecurity,— guidance for addressing common Cybersecurity issues, and— a framework to enable stakeholders to collaborate on resolving Cybersecurity issues.",ISO/IEC 27032:2012(en)
63,"As a result, Cyber.gov is creating a robust, innovative and holistic cyber security architecture design that mitigates modern threats (Asset Management, Configuration Management, Phishing, DDoS, Ransomware, Mobile, Cloud issues and more) by leveraging best practices and implementable solutions with minimal impact to workforce efficiency. It will not only addresses some of the issues of perimeter based defense, but also brings new technologies such as Software Defined Perimeter (SDP) tailored to the .Gov domain, while guiding CIOs and CISOs in selecting and implementing current best practices for 34 cyber security components. Cyber.gov will significantly improve the security and resiliency across the D/As. ","Information technology — Security techniques — Selection, deployment and operations of intrusion detection systems (IDPS)",https://www.iso.org/obp/ui/#!iso:std:56889:en,"IntroductionOrganizations should not only know when, if, and how an intrusion of their network, system, or application occurs. They also should know what vulnerability was exploited and what safeguards or appropriate risk treatment options (i.e. risk modification, risk retention, risk avoidance, risk sharing) should be implemented to prevent similar intrusions in the future. Organizations should also recognize and deter cyber-based intrusions. This requires an analysis of host and network traffic and/or audit trails for attack signatures or specific patterns that usually indicate malicious or suspicious intent. In the mid-1990s, organizations began to use intrusion detection and prevention systems (IDPS) to fulfil these needs. The general use of IDPS continues to expand with a wider range of IDPS products being made available to satisfy an increasing level of organizational demands for advanced intrusion detection capability.In order for an organization to derive the maximum benefits from IDPS, the process of IDPS selection, deployment, and operations should be carefully planned and implemented by properly trained and experienced personnel. In the case where this process is achieved, then IDPS products can assist an organization in obtaining intrusion information and can serve as an important security device within the overall information and communications technology (ICT) infrastructure.This International Standard provides guidelines for effective IDPS selection, deployment, and operation, as well as fundamental knowledge about IDPS. It is also applicable to those organizations that are considering outsourcing their intrusion detection capabilities. Information about outsourcing service level agreements can be found in the IT service management (ITSM) processes based on ISO/IEC 20000 Series.This International Standard is intended to be helpful to: a) An organization in satisfying the following requirements of ISO/IEC 27001:— The organization shall implement procedures and other controls capable of enabling prompt detection of and response to security incidents;— The organization shall execute monitoring and review procedures and other controls to properly identify attempted and successful security breaches and incidents.b) An organization in implementing controls that meet the following security objectives of ISO/IEC 27002:— To detect unauthorized information processing activities;— Systems should be monitored and information security events should be recorded. Operator logs and fault logging should be used to ensure information system problems are identified;— An organization should comply with all relevant legal requirements applicable to its monitoring and logging activities;— System monitoring should be used to check the effectiveness of controls adopted and to verify conformity to an access policy model.An organization should recognize that deploying IDPS is not a sole and/or exhaustive solution to satisfy or meet the above-cited requirements. Furthermore, this International Standard is not intended as criteria for any kind of conformity assessments, e.g., information security management system (ISMS) certification, IDPS services or products certification.","1   ScopeThis International Standard provides guidelines to assist organizations in preparing to deploy intrusion detection and prevention systems (IDPS). In particular, it addresses the selection, deployment, and operations of IDPS. It also provides background information from which these guidelines are derived.",ISO/IEC 27039:2015(en)
64,"As a result, Cyber.gov is creating a robust, innovative and holistic cyber security architecture design that mitigates modern threats (Asset Management, Configuration Management, Phishing, DDoS, Ransomware, Mobile, Cloud issues and more) by leveraging best practices and implementable solutions with minimal impact to workforce efficiency. It will not only addresses some of the issues of perimeter based defense, but also brings new technologies such as Software Defined Perimeter (SDP) tailored to the .Gov domain, while guiding CIOs and CISOs in selecting and implementing current best practices for 34 cyber security components. Cyber.gov will significantly improve the security and resiliency across the D/As. ",Information technology — Security techniques — Information security risk management,https://www.iso.org/obp/ui/#!iso:std:56742:en,"IntroductionThis International Standard provides guidelines for information security risk management in an organization, supporting in particular the requirements of an information security management (ISMS) according to ISO/IEC 27001. However, this International Standard does not provide any specific method for information security risk management. It is up to the organization to define their approach to risk management, depending for example on the scope of the ISMS, context of risk management, or industry sector. A number of existing methodologies can be used under the framework described in this International Standard to implement the requirements of an ISMS.This International Standard is relevant to managers and staff concerned with information security risk management within an organization and, where appropriate, external parties supporting such activities.","1   ScopeThis International Standard provides guidelines for information security risk management.This International Standard supports the general concepts specified in ISO/IEC 27001 and is designed to assist the satisfactory implementation of information security based on a risk management approach.Knowledge of the concepts, models, processes and terminologies described in ISO/IEC 27001 and ISO/IEC 27002 is important for a complete understanding of this International Standard.This International Standard is applicable to all types of organizations (e.g. commercial enterprises, government agencies, non-profit organizations) which intend to manage risks that could compromise the organization’s information security.",ISO/IEC 27005:2011(en)
65,"As a result, Cyber.gov is creating a robust, innovative and holistic cyber security architecture design that mitigates modern threats (Asset Management, Configuration Management, Phishing, DDoS, Ransomware, Mobile, Cloud issues and more) by leveraging best practices and implementable solutions with minimal impact to workforce efficiency. It will not only addresses some of the issues of perimeter based defense, but also brings new technologies such as Software Defined Perimeter (SDP) tailored to the .Gov domain, while guiding CIOs and CISOs in selecting and implementing current best practices for 34 cyber security components. Cyber.gov will significantly improve the security and resiliency across the D/As. ",Information technology — Security techniques — Vulnerability disclosure,https://www.iso.org/obp/ui/#!iso:std:45170:en,"IntroductionA vulnerability is a weakness of software, hardware, or online service that can be exploited. An exploitation of vulnerabilities results in a disruption of the confidentiality, integrity, or availability of the ICT system or related information assets, which may cause a breach of data privacy, interruption of operation of mission critical systems, and so on.Vulnerabilities can be caused by both software or hardware design and programming flaws. Poor administrative processes and a lack of user awareness and education can also be a source of vulnerabilities, as can unforeseen changes in operating environments. Regardless of the cause, an exploitation of such vulnerabilities may result in real threats to mission-critical information systems. Individuals and organizations, including businesses and governments, rely heavily on hardware and software components used in operating systems, applications, networks, and critical national infrastructure. Vulnerabilities in these components increase risk to the information residing on them, thus increasing risks to users and owners of the information. In addition, the lack of awareness about these vulnerabilities also increases risk.Inappropriate disclosure of a vulnerability could not only delay the deployment of the vulnerability resolution but also give attackers hints to exploit it. That is why vulnerability disclosure should be carried out appropriately.Vulnerability disclosure is a process through which vendors and vulnerability finders may work cooperatively in finding solutions that reduce the risks associated with a vulnerability. It encompasses actions such as reporting, coordinating, and publishing information about a vulnerability and its resolution.The goals of vulnerability disclosure include the following:a) ensuring that identified vulnerabilities are addressed;b) minimizing the risk from vulnerabilities;c) providing users with sufficient information to evaluate risks from vulnerabilities to their systems; d) setting expectations to promote positive communication and coordination among involved parties.This International Standard provides guidelines for vendors to be included in their business processes when receiving information about potential vulnerabilities and distributing vulnerability resolution information.","1   ScopeThis International Standard gives guidelines for the disclosure of potential vulnerabilities in products and online services. This International Standard details the methods a vendor should use to address issues related to vulnerability disclosure. This International Standarda) provides guidelines for vendors on how to receive information about potential vulnerabilities in their products or online services,b) provides guidelines for vendors on how to disseminate resolution information about vulnerabilities in their products or online services,c) provides the information items that should be produced through the implementation of a vendor’s vulnerability disclosure process, andd) provides examples of content that should be included in the information items.This International Standard is applicable to vendors who respond to external reports of vulnerabilities in their products or online services.",ISO/IEC 29147:2014(en)
66,"As a result, Cyber.gov is creating a robust, innovative and holistic cyber security architecture design that mitigates modern threats (Asset Management, Configuration Management, Phishing, DDoS, Ransomware, Mobile, Cloud issues and more) by leveraging best practices and implementable solutions with minimal impact to workforce efficiency. It will not only addresses some of the issues of perimeter based defense, but also brings new technologies such as Software Defined Perimeter (SDP) tailored to the .Gov domain, while guiding CIOs and CISOs in selecting and implementing current best practices for 34 cyber security components. Cyber.gov will significantly improve the security and resiliency across the D/As. ",Information technology — Security techniques — Code of practice for information security controls,https://www.iso.org/obp/ui/#!iso:std:54533:en,"0	Introduction0.1	Background and contextThis International Standard is designed for organizations to use as a reference for selecting controls within the process of implementing an Information Security Management System (ISMS) based on ISO/IEC 27001[10] or as a guidance document for organizations implementing commonly accepted information security controls. This standard is also intended for use in developing industry- and organization-specific information security management guidelines, taking into consideration their specific information security risk environment(s).Organizations of all types and sizes (including public and private sector, commercial and non-profit) collect, process, store and transmit information in many forms including electronic, physical and verbal (e.g. conversations and presentations).The value of information goes beyond the written words, numbers and images: knowledge, concepts, ideas and brands are examples of intangible forms of information. In an interconnected world, information and related processes, systems, networks and personnel involved in their operation, handling and protection are assets that, like other important business assets, are valuable to an organization’s business and consequently deserve or require protection against various hazards.Assets are subject to both deliberate and accidental threats while the related processes, systems, networks and people have inherent vulnerabilities. Changes to business processes and systems or other external changes (such as new laws and regulations) may create new information security risks. Therefore, given the multitude of ways in which threats could take advantage of vulnerabilities to harm the organization, information security risks are always present. Effective information security reduces these risks by protecting the organization against threats and vulnerabilities, and then reduces impacts to its assets.Information security is achieved by implementing a suitable set of controls, including policies, processes, procedures, organizational structures and software and hardware functions. These controls need to be established, implemented, monitored, reviewed and improved, where necessary, to ensure that the specific security and business objectives of the organization are met. An ISMS such as that specified in ISO/IEC 27001[10] takes a holistic, coordinated view of the organization’s information security risks in order to implement a comprehensive suite of information security controls under the overall framework of a coherent management system.Many information systems have not been designed to be secure in the sense of ISO/IEC 27001[10] and this standard. The security that can be achieved through technical means is limited and should be supported by appropriate management and procedures. Identifying which controls should be in place requires careful planning and attention to detail. A successful ISMS requires support by all employees in the organization. It can also require participation from shareholders, suppliers or other external parties. Specialist advice from external parties can also be needed.In a more general sense, effective information security also assures management and other stakeholders that the organization’s assets are reasonably safe and protected against harm, thereby acting as a business enabler.0.2	Information security requirementsIt is essential that an organization identifies its security requirements. There are three main sources of security requirements:a) the assessment of risks to the organization, taking into account the organization’s overall business strategy and objectives. Through a risk assessment, threats to assets are identified, vulnerability to and likelihood of occurrence is evaluated and potential impact is estimated;b) the legal, statutory, regulatory and contractual requirements that an organization, its trading partners, contractors and service providers have to satisfy, and their socio-cultural environment;c) the set of principles, objectives and business requirements for information handling, processing, storing, communicating and archiving that an organization has developed to support its operations.Resources employed in implementing controls need to be balanced against the business harm likely to result from security issues in the absence of those controls. The results of a risk assessment will help guide and determine the appropriate management action and priorities for managing information security risks and for implementing controls selected to protect against these risks.ISO/IEC 27005[11] provides information security risk management guidance, including advice on risk assessment, risk treatment, risk acceptance, risk communication, risk monitoring and risk review.0.3	Selecting controlsControls can be selected from this standard or from other control sets, or new controls can be designed to meet specific needs as appropriate.The selection of controls is dependent upon organizational decisions based on the criteria for risk acceptance, risk treatment options and the general risk management approach applied to the organization, and should also be subject to all relevant national and international legislation and regulations. Control selection also depends on the manner in which controls interact to provide defence in depth.Some of the controls in this standard can be considered as guiding principles for information security management and applicable for most organizations. The controls are explained in more detail below along with implementation guidance. More information about selecting controls and other risk treatment options can be found in ISO/IEC 27005.[11]0.4	Developing your own guidelinesThis International Standard may be regarded as a starting point for developing organization-specific guidelines. Not all of the controls and guidance in this code of practice may be applicable. Furthermore, additional controls and guidelines not included in this standard may be required. When documents are developed containing additional guidelines or controls, it may be useful to include cross-references to clauses in this standard where applicable to facilitate compliance checking by auditors and business partners.0.5	Lifecycle considerationsInformation has a natural lifecycle, from creation and origination through storage, processing, use and transmission to its eventual destruction or decay. The value of, and risks to, assets may vary during their lifetime (e.g. unauthorized disclosure or theft of a company’s financial accounts is far less significant after they have been formally published) but information security remains important to some extent at all stages.Information systems have lifecycles within which they are conceived, specified, designed, developed, tested, implemented, used, maintained and eventually retired from service and disposed of. Information security should be taken into account at every stage. New system developments and changes to existing systems present opportunities for organizations to update and improve security controls, taking actual incidents and current and projected information security risks into account.0.6	Related standardsWhile this standard offers guidance on a broad range of information security controls that are commonly applied in many different organizations, the remaining standards in the ISO/IEC 27000 family provide complementary advice or requirements on other aspects of the overall process of managing information security.Refer to ISO/IEC 27000 for a general introduction to both ISMSs and the family of standards. ISO/IEC 27000 provides a glossary, formally defining most of the terms used throughout the ISO/IEC 27000 family of standards, and describes the scope and objectives for each member of the family.","1   ScopeThis International Standard gives guidelines for organizational information security standards and information security management practices including the selection, implementation and management of controls taking into consideration the organization’s information security risk environment(s).This International Standard is designed to be used by organizations that intend to:a) select controls within the process of implementing an Information Security Management System based on ISO/IEC 27001;[10]b) implement commonly accepted information security controls;c) develop their own information security management guidelines.",ISO/IEC 27002:2013(en)
67,"Recent core technology deliverables for FY 2012 include the SRI SIMON-based MDAS and the NRL Open Mongoose System.  Other new solutions include new multi-INT Persistent Wide Area Suveillance/Precision Geo-Location Capabilities.  This WAS (Wide Area Surveillance) capability is comprised of:_       algorithims to enable ship detection and generation of reports using NOAA HFR, DoD aerostat (air) radars, and shipboard commercial navigation radars._       Multi-INT buoy extension with power generation_       Mobile terminal to produce timely ship detection service using commercial SAR/EO satellites_       Develop buoy-based and sea-floor acoustic detection technology_       Develop a 3G/4G cellular based smart phone application and new Class E AIS standard.Existing technology and infrastructure was foraged for PCSI.",Ships and marine technology — Maritime standards list,https://www.iso.org/obp/ui/#!iso:std:36696:en,,"1   ScopeThis International Standard provides the list of existing maritime standards of ISO and eight (8) shipbuilding nations in the world (The People’s Republic of China, Germany, India, Japan, The Republic of Korea, Poland, Russian Federation and The United States of America) as of the year 2002 by classifying the maritime standards of the aforesaid organization and nations into the codes that correspond to the existing organization of subcommittees of ISO/TC 8 as specified hereunder.",ISO 15583:2005(en)
68,"Recent core technology deliverables for FY 2012 include the SRI SIMON-based MDAS and the NRL Open Mongoose System.  Other new solutions include new multi-INT Persistent Wide Area Suveillance/Precision Geo-Location Capabilities.  This WAS (Wide Area Surveillance) capability is comprised of:_       algorithims to enable ship detection and generation of reports using NOAA HFR, DoD aerostat (air) radars, and shipboard commercial navigation radars._       Multi-INT buoy extension with power generation_       Mobile terminal to produce timely ship detection service using commercial SAR/EO satellites_       Develop buoy-based and sea-floor acoustic detection technology_       Develop a 3G/4G cellular based smart phone application and new Class E AIS standard.Existing technology and infrastructure was foraged for PCSI.",Ships and marine technology — Maritime port facility security assessments and security plan development,https://www.iso.org/obp/ui/#!iso:std:46051:en,"IntroductionThis International Standard addresses the execution of marine port facility security assessments, marine port facility security plans (including countermeasures) and the skills and knowledge required of the personnel involved. This International Standard is designed to ensure that the completed work meets the requirements of the International Maritime Organization (IMO) International Ships and Port Facility Security Code (ISPS) and the appropriate maritime security practices that can be verified by an outside auditor. Since other ISO standards may address non-marine port facilities the word “marine” usually appears before port facilities in this standard. This standard is intended to address port facilities as defined in the ISPS.","1   Scope1.1   GeneralThis International Standard establishes a framework to assist marine port facilities in specifying the competence of personnel to conduct a marine port facility security assessment and to develop a security plan as required by the ISPS Code International Standard, conducting the marine port facility security assessment, and drafting/implementing a Port Facility Security Plan (PFSP).In addition, this International Standard establishes certain documentation requirements designed to ensure that the process used in performing the duties described above was recorded in a manner that would permit independent verification by a qualified and authorized agency (if the port facility has agreed to the review). It is not an objective of this International Standard to set requirements for a contracting government or designated authority in designating a Recognized Security Organization (RSO), or to impose the use of an outside service provider or other third parties to perform the marine port facility security assessment or security plan if the port facility personnel possess the expertise outlined in this specification. Ship operators may be informed that marine port facilities that use this document meet an industry-determined level of compliance with the ISPS Code.Port infrastructure that falls outside the security perimeter of a marine port facility might affect the security of the facility/ship interface. This International Standard does not address the requirements of the ISPS Code relative to such infrastructures. State governments have a duty to protect their populations and infrastructures from marine incidents occurring outside their marine port facilities. These duties are outside the scope of this International Standard.1.2   ConformanceWhile compliance with the ISPS Code is internationally mandated for all signatory countries, the use of this International Standard is voluntary. If a contracting government establishes requirements that preclude the use of this International Standard, local law takes precedence and compliance with this International Standard should not be claimed.",ISO 20858:2007(en)
69,"Recent core technology deliverables for FY 2012 include the SRI SIMON-based MDAS and the NRL Open Mongoose System.  Other new solutions include new multi-INT Persistent Wide Area Suveillance/Precision Geo-Location Capabilities.  This WAS (Wide Area Surveillance) capability is comprised of:_       algorithims to enable ship detection and generation of reports using NOAA HFR, DoD aerostat (air) radars, and shipboard commercial navigation radars._       Multi-INT buoy extension with power generation_       Mobile terminal to produce timely ship detection service using commercial SAR/EO satellites_       Develop buoy-based and sea-floor acoustic detection technology_       Develop a 3G/4G cellular based smart phone application and new Class E AIS standard.Existing technology and infrastructure was foraged for PCSI.",Ships and marine technology — Computer applications — General principles for the development and use of programmable electronic systems in marine applications,https://www.iso.org/obp/ui/#!iso:std:31619:en,"IntroductionSystems which include programmable electronic systems (PES) are not exact substitutes for the electromechanical systems and/or crew tasks which they replace. A new technology is involved, which can provide opportunities for integration of traditional system components (including crew tasks) and more complex behaviour. This allows increases in efficiency and safety through improved monitoring, better situational awareness on the bridge, etc. However, PES are complex products and, like all products, they can contain defects. These defects cannot be seen. Software does not respond to traditional engineering methods for the testing of soundness. The combination of complexity, replacement of a combination of mechanical and crew functions with computer hardware and software, and industry practice in developing and maintaining marine PES leads to a wide range of potential defects which cannot be guarded against by prescriptive standards.The use of a PES in the management, monitoring or control of a ship may have several effects:— potential to enhance the ability and efficiency of the crew;— changes in the organization of work through the automation of lower-level tasks;— integration of systems through use of several systems by one seafarer;— shift in the role of the crew towards the management of many linked, complex PES;— shift of the crew's perception of the ship to that presented by the interfaces of the PES;— layers of embedded and/or application software interposed between the crew and the ship;— physical interconnection of ship systems through the use of computer networks.The overall effect of the use of PES is that the ship becomes one total system of inter-linked PES and crew which work together to fulfil the operator's business goals for the ship. In order for this total system to be dependable, both the design of the PES and the management of its use have to support the safe and effective performance of the crew as a critical component of the total system. Such a human-centred approach has to be based on a thorough knowledge of the particular skills, working environment and tasks of the crew using the PES. The total system concept is described further in A.2.In the traditional approach to maritime safety, ship systems are built to and operated against precise, prescriptive standards. These standards were developed in response to feedback about incidents or risky behaviour of previous ship systems. This approach is appropriate for relatively simple systems in a time of slow technical innovation. However, suppliers and operators nowadays want to innovate with complex, new solutions. In addition, the base technologies for PES are evolving very quickly. The assurance of dependability in this case cannot rely on knowledge of previous systems. The solution is for the developer and operator to assess the risks from and to the particular ship, its systems, crew and its operating philosophy, and to address these specific risks in the design and operation of the PES. Components of the system can then either be re-designed or operated in such a way as to minimize these risks. The quality of construction, operation and maintenance of the system to be sure of the achievement of a required level of dependability of the PES is also defined.This International Standard is based on best practice in PES development as stated in existing marine, electrical and electronic, IT, ergonomics and safety standards. It is not intended to replace any of these standards. It presents a synoptic view of the requirements of these standards as a framework of principles for the development of dependable PES.","1   ScopeThis International Standard provides a set of mandatory principles, recommended criteria and associated guidance for the development and use of dependable marine programmable electronic systems for shipboard use. It applies to any shipboard equipment containing programmable elements which may affect the safe or efficient operation of the ship. It contains information for all parties involved in the specification, operation, maintenance and assessment of such systems. The principles and guidance in the document are largely based on requirements in national and International Standards. The source standards and their contribution to this International Standard are presented in the bibliography.NOTE This International Standard does not directly address performance, test or test results requirements associated with specific types of equipment or functions. In such instances existing application or component standards may be applied, e.g. IEC 60945, in respect of navigation and radio-communications equipment. The responsible body (e.g. National Administration, Classification Society or other contracted party) will determine the applicability of this International Standard, and its specific requirements where any potential conflict arises.",ISO 17894:2005(en)
70,"The SWAMP offers a no-cost, cloud-based, high throughput computing platform at mir-swamp.org that is capable of analyzing over 275 million lines of code each day. With this infrastructure, users can conduct a variety of tests on applications of any size in a timely manner. The SWAMP staff maintains and updates all tools and platforms available in the SWAMP, ensuring that users can focus on testing and not worry about the infrastructure necessary to support testing activities.The SWAMPs shared, continuous assurance facility simplifies integrating security into the classroom and software development life cycle by offering a marketplace of open-source and commercial software analysis tools to perform comprehensive security testing on your own applications. A library of public applications with known vulnerabilities is also provided for download, testing, and assessment. Research has shown that it is necessary to use more than one tool when testing a software package to ensure that secure coding practices were followed and that there are no vulnerabilities in the application. In the SWAMP, results from multiple testing tools are compiled into a single integrated viewer that presents identified weaknesses in a way that helps users prioritize and fix each error in the code. As weaknesses are addressed, software applications become more secure but should be reassessed to ensure that no new weaknesses were introduced during the remediation process or throughout the software development life cycle.New science and technology generated from this program is listed below:_       New analysis techniques for static code analysis through the STAMP project_       Improved testing and evaluation techniques for static analysis tools_       Unified Threat Management (UTM) system. This provides the capability for security professional to monitor and manage a wide range of security related applications and infrastructure through a single management environment.The key R&D center for this SWAMP is the Morgridge Institute of Research, where the testing tools and platforms are located.  There are multiple projects and multiple contracts associated with this program, so there is no aggregate TRL level calculated for the SWA program as a whole.  Projects may starts at different TRL levels, especially if they are leveraging previous work. ",Information technology — Programming languages — Guidance to avoiding vulnerabilities in programming languages through language selection and use,https://www.iso.org/obp/ui/#!iso:std:61457:en,"IntroductionAll programming languages contain constructs that are incompletely specified, exhibit undefined behaviour, are implementation-dependent, or are difficult to use correctly. The use of those constructs may therefore give rise to vulnerabilities, as a result of which, software programs can execute differently than intended by the writer. In some cases, these vulnerabilities can compromise the safety of a system or be exploited by attackers to compromise the security or privacy of a system.This Technical Report is intended to provide guidance spanning multiple programming languages, so that application developers will be better able to avoid the programming constructs that lead to vulnerabilities in software written in their chosen language and their attendant consequences. This guidance can also be used by developers to select source code evaluation tools that can discover and eliminate some constructs that could lead to vulnerabilities in their software or to select a programming language that avoids anticipated problems.It should be noted that this Technical Report is inherently incomplete. It is not possible to provide a complete list of programming language vulnerabilities because new weaknesses are discovered continually. Any such report can only describe those that have been found, characterized, and determined to have sufficient probability and consequence.Furthermore, to focus its limited resources, the working group developing this report decided to defer comprehensive treatment of several subject areas until future editions of the report. These subject areas include:• Object-oriented language features (although some simple issues related to inheritance are described in 6.43 Inheritance [RIP])• Numerical analysis (although some simple items regarding the use of floating point are described in 6.5 Floating-point Arithmetic [PLF])• Inter-language operability","1.   ScopeThis Technical Report specifies software programming language vulnerabilities to be avoided in the development of systems where assured behaviour is required for security, safety, mission-critical and business-critical software. In general, this guidance is applicable to the software developed, reviewed, or maintained for any application.Vulnerabilities are described in a generic manner that is applicable to a broad range of programming languages.",ISO/IEC TR 24772:2013(en)
71,"The SWAMP offers a no-cost, cloud-based, high throughput computing platform at mir-swamp.org that is capable of analyzing over 275 million lines of code each day. With this infrastructure, users can conduct a variety of tests on applications of any size in a timely manner. The SWAMP staff maintains and updates all tools and platforms available in the SWAMP, ensuring that users can focus on testing and not worry about the infrastructure necessary to support testing activities.The SWAMPs shared, continuous assurance facility simplifies integrating security into the classroom and software development life cycle by offering a marketplace of open-source and commercial software analysis tools to perform comprehensive security testing on your own applications. A library of public applications with known vulnerabilities is also provided for download, testing, and assessment. Research has shown that it is necessary to use more than one tool when testing a software package to ensure that secure coding practices were followed and that there are no vulnerabilities in the application. In the SWAMP, results from multiple testing tools are compiled into a single integrated viewer that presents identified weaknesses in a way that helps users prioritize and fix each error in the code. As weaknesses are addressed, software applications become more secure but should be reassessed to ensure that no new weaknesses were introduced during the remediation process or throughout the software development life cycle.New science and technology generated from this program is listed below:_       New analysis techniques for static code analysis through the STAMP project_       Improved testing and evaluation techniques for static analysis tools_       Unified Threat Management (UTM) system. This provides the capability for security professional to monitor and manage a wide range of security related applications and infrastructure through a single management environment.The key R&D center for this SWAMP is the Morgridge Institute of Research, where the testing tools and platforms are located.  There are multiple projects and multiple contracts associated with this program, so there is no aggregate TRL level calculated for the SWA program as a whole.  Projects may starts at different TRL levels, especially if they are leveraging previous work. ",Software Engineering — Guide to the software engineering body of knowledge (SWEBOK),https://www.iso.org/obp/ui/#!iso:std:67604:en,"IntroductionThe purpose of the Guide to the Software Engineering Body of Knowledge is to provide a consensually validated characterization of the bounds of the software engineering discipline and to provide a topical access to the Body of Knowledge supporting that discipline. The Body of Knowledge is subdivided into fifteen software engineering Knowledge Areas (KA) providing an outline of topics. The descriptions in the KAs are designed to discriminate among the various important concepts, permitting readers to find their way quickly to subjects of interest. Upon finding a subject, readers are referred to key papers or books selected because they succinctly present the knowledge.Publication of the 2004 version of this Guide to the Software Engineering Body of Knowledge (SWEBOK 2004)—adopted as ISO/IEC TR 19759:2005—was a major milestone in establishing software engineering as a recognized engineering discipline. The goal in developing this update to SWEBOK is to improve the currency, readability, consistency, and usability of the Guide. All knowledge areas (KAs) have been updated to reflect changes in software engineering since publication of SWEBOK 2004. Four new foundation KAs and a Software Engineering Professional Practices KA have been added. The Software Engineering Tools and Methods KA has been revised as Software Engineering Models and Methods. Software engineering tools is now a topic in each of the KAs. Three appendices provide the specifications for the KA description, an annotated set of relevant standards for each KA, and a listing of the references cited in the Guide.An emphasis on engineering practice leads the Guide toward a strong relationship with the normative literature. Most of the computer science, information technology and software engineering literature provides information useful to software engineers, but a relatively small portion is normative. A normative document prescribes what an engineer should do in a specified situation rather than providing information that might be helpful. The normative literature is validated by consensus formed among practitioners and is concentrated in standards and related documents. From the beginning, the SWEBOK project was conceived as having a strong relationship to the normative literature of software engineering. The two major standards bodies for software engineering (IEEE Computer Society Software and Systems Engineering Standards Committee and ISO/IEC JTC1/SC7) cooperated in the project.The Guide is oriented toward a variety of audiences, all over the world. It aims to serve public and private organizations in need of a consistent view of software engineering for defining education and training requirements, classifying jobs, developing performance evaluation policies or specifying software development tasks. It also addresses practicing, or managing, software engineers and the officials responsible for making public policy regarding licensing and professional guidelines. In addition, professional societies and educators defining the certification rules, accreditation policies for university curricula, and guidelines for professional practice will benefit from the SWEBOK Guide, as well as the students learning the software engineering profession and educators and trainers engaged in defining curricula and course content.",1   ScopeThis Technical report characterizes the boundaries of the software engineering discipline and provides topical access to the literature supporting that discipline.,ISO/IEC TR 19759:2015(en)
72,"The SWAMP offers a no-cost, cloud-based, high throughput computing platform at mir-swamp.org that is capable of analyzing over 275 million lines of code each day. With this infrastructure, users can conduct a variety of tests on applications of any size in a timely manner. The SWAMP staff maintains and updates all tools and platforms available in the SWAMP, ensuring that users can focus on testing and not worry about the infrastructure necessary to support testing activities.The SWAMPs shared, continuous assurance facility simplifies integrating security into the classroom and software development life cycle by offering a marketplace of open-source and commercial software analysis tools to perform comprehensive security testing on your own applications. A library of public applications with known vulnerabilities is also provided for download, testing, and assessment. Research has shown that it is necessary to use more than one tool when testing a software package to ensure that secure coding practices were followed and that there are no vulnerabilities in the application. In the SWAMP, results from multiple testing tools are compiled into a single integrated viewer that presents identified weaknesses in a way that helps users prioritize and fix each error in the code. As weaknesses are addressed, software applications become more secure but should be reassessed to ensure that no new weaknesses were introduced during the remediation process or throughout the software development life cycle.New science and technology generated from this program is listed below:_       New analysis techniques for static code analysis through the STAMP project_       Improved testing and evaluation techniques for static analysis tools_       Unified Threat Management (UTM) system. This provides the capability for security professional to monitor and manage a wide range of security related applications and infrastructure through a single management environment.The key R&D center for this SWAMP is the Morgridge Institute of Research, where the testing tools and platforms are located.  There are multiple projects and multiple contracts associated with this program, so there is no aggregate TRL level calculated for the SWA program as a whole.  Projects may starts at different TRL levels, especially if they are leveraging previous work. ",Information technology — Security techniques — Methodology for IT security evaluation,https://www.iso.org/obp/ui/#!iso:std:46412:en,"IntroductionThe target audience for this International Standard is primarily evaluators applying ISO/IEC 15408 and certifiers confirming evaluator actions; evaluation sponsors, developers, PP/ST authors and other parties interested in IT security are a secondary audience.This International Standard recognises that not all questions concerning IT security evaluation will be answered herein and that further interpretations will be needed. Individual schemes will determine how to handle such interpretations, although these can be subject to mutual recognition agreements. A list of methodology-related activities that can be handled by individual schemes can be found in Annex A.","1   ScopeThis International Standard is a companion document to the evaluation criteria for IT security defined in ISO/IEC 15408. It defines the minimum actions to be performed by an evaluator in order to conduct an ISO/IEC 15408 evaluation, using the criteria and evaluation evidence defined in ISO/IEC 15408.This International Standard does not define evaluator actions for certain high assurance ISO/IEC 15408 components, where there is as yet no generally agreed guidance.",ISO/IEC 18045:2008(en)
73,"The SWAMP offers a no-cost, cloud-based, high throughput computing platform at mir-swamp.org that is capable of analyzing over 275 million lines of code each day. With this infrastructure, users can conduct a variety of tests on applications of any size in a timely manner. The SWAMP staff maintains and updates all tools and platforms available in the SWAMP, ensuring that users can focus on testing and not worry about the infrastructure necessary to support testing activities.The SWAMPs shared, continuous assurance facility simplifies integrating security into the classroom and software development life cycle by offering a marketplace of open-source and commercial software analysis tools to perform comprehensive security testing on your own applications. A library of public applications with known vulnerabilities is also provided for download, testing, and assessment. Research has shown that it is necessary to use more than one tool when testing a software package to ensure that secure coding practices were followed and that there are no vulnerabilities in the application. In the SWAMP, results from multiple testing tools are compiled into a single integrated viewer that presents identified weaknesses in a way that helps users prioritize and fix each error in the code. As weaknesses are addressed, software applications become more secure but should be reassessed to ensure that no new weaknesses were introduced during the remediation process or throughout the software development life cycle.New science and technology generated from this program is listed below:_       New analysis techniques for static code analysis through the STAMP project_       Improved testing and evaluation techniques for static analysis tools_       Unified Threat Management (UTM) system. This provides the capability for security professional to monitor and manage a wide range of security related applications and infrastructure through a single management environment.The key R&D center for this SWAMP is the Morgridge Institute of Research, where the testing tools and platforms are located.  There are multiple projects and multiple contracts associated with this program, so there is no aggregate TRL level calculated for the SWA program as a whole.  Projects may starts at different TRL levels, especially if they are leveraging previous work. ",Information technology — Security techniques — Evaluation criteria for IT security — Part 3: Security assurance components,https://www.iso.org/obp/ui/#!iso:std:46413:en,"IntroductionSecurity assurance components, as defined in this part of ISO/IEC 15408, are the basis for the security assurance requirements expressed in a Protection Profile (PP) or a Security Target (ST).These requirements establish a standard way of expressing the assurance requirements for TOEs. This part of ISO/IEC 15408 catalogues the set of assurance components, families and classes. This part of ISO/IEC 15408 also defines evaluation criteria for PPs and STs and presents evaluation assurance levels that define the predefined ISO/IEC 15408 scale for rating assurance for Targets of Evaluation (TOEs), which is called the Evaluation Assurance Levels (EALs).The audience for this part of ISO/IEC 15408 includes consumers, developers, and evaluators of secure IT products. ISO/IEC 15408-1:2009, Clause 5 provides additional information on the target audience of ISO/IEC 15408, and on the use of ISO/IEC 15408 by the groups that comprise the target audience. These groups may use this part of ISO/IEC 15408 as follows:a) Consumers, who use this part of ISO/IEC 15408 when selecting components to express assurance requirements to satisfy the security objectives expressed in a PP or ST, determining required levels of security assurance of the TOE.b) Developers, who respond to actual or perceived consumer security requirements in constructing a TOE, reference this part of ISO/IEC 15408 when interpreting statements of assurance requirements and determining assurance approaches of TOEs.c) Evaluators, who use the assurance requirements defined in this part of ISO/IEC 15408 as a mandatory statement of evaluation criteria when determining the assurance of TOEs and when evaluating PPs and STs.","1   ScopeThis part of ISO/IEC 15408 defines the assurance requirements of ISO/IEC 15408. It includes the evaluation assurance levels (EALs) that define a scale for measuring assurance for component Targets of Evaluation (TOEs), the composed assurance packages (CAPs) that define a scale for measuring assurance for composed TOEs, the individual assurance components from which the assurance levels and packages are composed, and the criteria for evaluation of Protection Profiles (PPs) and Security Targets (STs).",ISO/IEC 15408-3:2008(en)
